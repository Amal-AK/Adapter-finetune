/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
INFO:name:device: cuda:0, n_gpu: 1
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/unixcoder-base/resolve/main/config.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/unixcoder-base/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

INFO:name:[{'insert_modules': ('attention.self', 'intermediate', 'output'), 'bottleneck_dim': (16, 64, 128), 'non_linearity': 'gelu', 'dropout_rate': 0.2, 'normalization': 'layer_norm', 'skip_connection': True}, 0, 0, {'insert_modules': ('intermediate', 'attention.self'), 'bottleneck_dim': (64, 32), 'non_linearity': 'swish', 'dropout_rate': 0.3, 'normalization': 'layer_norm', 'skip_connection': True}, 0, 0, {'insert_modules': ('attention.self', 'output'), 'bottleneck_dim': (32, 128), 'non_linearity': 'silu', 'dropout_rate': 0.25, 'normalization': 'layer_norm', 'skip_connection': True}, 0, 0, 0, 0, 0]
[INFO|(OpenDelta)basemodel:700]2025-01-23 18:17:01,227 >> Trainable Ratio: 1313904/127243632=1.032589%
[INFO|(OpenDelta)basemodel:702]2025-01-23 18:17:01,227 >> Delta Parameter Ratio: 1313904/127243632=1.032589%
[INFO|(OpenDelta)basemodel:704]2025-01-23 18:17:01,227 >> Static Memory 0.00 GB, Max Memory 0.00 GB
INFO:name:1.52
/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 10
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 78700
INFO:name:epoch 0 step 100 loss 0.29478
INFO:name:epoch 0 step 200 loss 0.20994
INFO:name:epoch 0 step 300 loss 0.18487
INFO:name:epoch 0 step 400 loss 0.19835
INFO:name:epoch 0 step 500 loss 0.18613
INFO:name:epoch 0 step 600 loss 0.15624
INFO:name:epoch 0 step 700 loss 0.15501
INFO:name:epoch 0 step 800 loss 0.16828
INFO:name:epoch 0 step 900 loss 0.15513
INFO:name:epoch 0 step 1000 loss 0.15432
INFO:name:epoch 0 step 1100 loss 0.1435
INFO:name:epoch 0 step 1200 loss 0.15934
INFO:name:epoch 0 step 1300 loss 0.14281
INFO:name:epoch 0 step 1400 loss 0.15494
INFO:name:epoch 0 step 1500 loss 0.12873
INFO:name:epoch 0 step 1600 loss 0.13264
INFO:name:epoch 0 step 1700 loss 0.12545
INFO:name:epoch 0 step 1800 loss 0.14029
INFO:name:epoch 0 step 1900 loss 0.12662
INFO:name:epoch 0 step 2000 loss 0.11133
INFO:name:epoch 0 step 2100 loss 0.13237
INFO:name:epoch 0 step 2200 loss 0.1121
INFO:name:epoch 0 step 2300 loss 0.11771
INFO:name:epoch 0 step 2400 loss 0.12225
INFO:name:epoch 0 step 2500 loss 0.12472
INFO:name:epoch 0 step 2600 loss 0.12739
INFO:name:epoch 0 step 2700 loss 0.12348
INFO:name:epoch 0 step 2800 loss 0.13557
INFO:name:epoch 0 step 2900 loss 0.11819
INFO:name:epoch 0 step 3000 loss 0.11813
INFO:name:epoch 0 step 3100 loss 0.12513
INFO:name:epoch 0 step 3200 loss 0.12106
INFO:name:epoch 0 step 3300 loss 0.10993
INFO:name:epoch 0 step 3400 loss 0.12234
INFO:name:epoch 0 step 3500 loss 0.12133
INFO:name:epoch 0 step 3600 loss 0.10589
INFO:name:epoch 0 step 3700 loss 0.11935
INFO:name:epoch 0 step 3800 loss 0.12306
INFO:name:epoch 0 step 3900 loss 0.1184
INFO:name:epoch 0 step 4000 loss 0.11364
INFO:name:epoch 0 step 4100 loss 0.11672
INFO:name:epoch 0 step 4200 loss 0.12778
INFO:name:epoch 0 step 4300 loss 0.10592
INFO:name:epoch 0 step 4400 loss 0.10842
INFO:name:epoch 0 step 4500 loss 0.11547
INFO:name:epoch 0 step 4600 loss 0.1222
INFO:name:epoch 0 step 4700 loss 0.10545
INFO:name:epoch 0 step 4800 loss 0.10208
INFO:name:epoch 0 step 4900 loss 0.10008
INFO:name:epoch 0 step 5000 loss 0.10497
INFO:name:epoch 0 step 5100 loss 0.10543
INFO:name:epoch 0 step 5200 loss 0.11048
INFO:name:epoch 0 step 5300 loss 0.10867
INFO:name:epoch 0 step 5400 loss 0.10131
INFO:name:epoch 0 step 5500 loss 0.10864
INFO:name:epoch 0 step 5600 loss 0.10911
INFO:name:epoch 0 step 5700 loss 0.09931
INFO:name:epoch 0 step 5800 loss 0.11516
INFO:name:epoch 0 step 5900 loss 0.11203
INFO:name:epoch 0 step 6000 loss 0.11622
INFO:name:epoch 0 step 6100 loss 0.10985
INFO:name:epoch 0 step 6200 loss 0.11589
INFO:name:epoch 0 step 6300 loss 0.11296
INFO:name:epoch 0 step 6400 loss 0.1176
INFO:name:epoch 0 step 6500 loss 0.10052
INFO:name:epoch 0 step 6600 loss 0.10151
INFO:name:epoch 0 step 6700 loss 0.10876
INFO:name:epoch 0 step 6800 loss 0.10761
INFO:name:epoch 0 step 6900 loss 0.10128
INFO:name:epoch 0 step 7000 loss 0.09884
INFO:name:epoch 0 step 7100 loss 0.10034
INFO:name:epoch 0 step 7200 loss 0.09766
INFO:name:epoch 0 step 7300 loss 0.11163
INFO:name:epoch 0 step 7400 loss 0.09993
INFO:name:epoch 0 step 7500 loss 0.0981
INFO:name:epoch 0 step 7600 loss 0.10492
INFO:name:epoch 0 step 7700 loss 0.10241
INFO:name:epoch 0 step 7800 loss 0.12646
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4013
INFO:name:  ********************
INFO:name:  Best eval mrr:0.4013
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3363
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 0.0698
INFO:name:epoch 1 step 200 loss 0.05663
INFO:name:epoch 1 step 300 loss 0.0525
INFO:name:epoch 1 step 400 loss 0.05511
INFO:name:epoch 1 step 500 loss 0.05001
INFO:name:epoch 1 step 600 loss 0.05856
INFO:name:epoch 1 step 700 loss 0.0544
INFO:name:epoch 1 step 800 loss 0.0512
INFO:name:epoch 1 step 900 loss 0.05574
INFO:name:epoch 1 step 1000 loss 0.053
INFO:name:epoch 1 step 1100 loss 0.06319
INFO:name:epoch 1 step 1200 loss 0.05037
INFO:name:epoch 1 step 1300 loss 0.04363
INFO:name:epoch 1 step 1400 loss 0.05801
INFO:name:epoch 1 step 1500 loss 0.04879
INFO:name:epoch 1 step 1600 loss 0.0498
INFO:name:epoch 1 step 1700 loss 0.05161
INFO:name:epoch 1 step 1800 loss 0.05318
INFO:name:epoch 1 step 1900 loss 0.05672
INFO:name:epoch 1 step 2000 loss 0.05067
INFO:name:epoch 1 step 2100 loss 0.05386
INFO:name:epoch 1 step 2200 loss 0.05271
INFO:name:epoch 1 step 2300 loss 0.04474
INFO:name:epoch 1 step 2400 loss 0.04034
INFO:name:epoch 1 step 2500 loss 0.05108
INFO:name:epoch 1 step 2600 loss 0.05164
INFO:name:epoch 1 step 2700 loss 0.05844
INFO:name:epoch 1 step 2800 loss 0.05883
INFO:name:epoch 1 step 2900 loss 0.05119
INFO:name:epoch 1 step 3000 loss 0.05577
INFO:name:epoch 1 step 3100 loss 0.04742
INFO:name:epoch 1 step 3200 loss 0.04403
INFO:name:epoch 1 step 3300 loss 0.05554
INFO:name:epoch 1 step 3400 loss 0.06277
INFO:name:epoch 1 step 3500 loss 0.05306
INFO:name:epoch 1 step 3600 loss 0.05315
INFO:name:epoch 1 step 3700 loss 0.05738
INFO:name:epoch 1 step 3800 loss 0.06282
INFO:name:epoch 1 step 3900 loss 0.04711
INFO:name:epoch 1 step 4000 loss 0.05652
INFO:name:epoch 1 step 4100 loss 0.05988
INFO:name:epoch 1 step 4200 loss 0.05672
INFO:name:epoch 1 step 4300 loss 0.04194
INFO:name:epoch 1 step 4400 loss 0.04094
INFO:name:epoch 1 step 4500 loss 0.05165
INFO:name:epoch 1 step 4600 loss 0.05533
INFO:name:epoch 1 step 4700 loss 0.04512
INFO:name:epoch 1 step 4800 loss 0.05013
INFO:name:epoch 1 step 4900 loss 0.04404
INFO:name:epoch 1 step 5000 loss 0.05832
INFO:name:epoch 1 step 5100 loss 0.04683
INFO:name:epoch 1 step 5200 loss 0.05488
INFO:name:epoch 1 step 5300 loss 0.05707
INFO:name:epoch 1 step 5400 loss 0.05438
INFO:name:epoch 1 step 5500 loss 0.05564
INFO:name:epoch 1 step 5600 loss 0.05075
INFO:name:epoch 1 step 5700 loss 0.0562
INFO:name:epoch 1 step 5800 loss 0.05559
INFO:name:epoch 1 step 5900 loss 0.07163
INFO:name:epoch 1 step 6000 loss 0.04986
INFO:name:epoch 1 step 6100 loss 0.05012
INFO:name:epoch 1 step 6200 loss 0.05284
INFO:name:epoch 1 step 6300 loss 0.04864
INFO:name:epoch 1 step 6400 loss 0.04669
INFO:name:epoch 1 step 6500 loss 0.05061
INFO:name:epoch 1 step 6600 loss 0.04996
INFO:name:epoch 1 step 6700 loss 0.05049
INFO:name:epoch 1 step 6800 loss 0.04859
INFO:name:epoch 1 step 6900 loss 0.05538
INFO:name:epoch 1 step 7000 loss 0.0572
INFO:name:epoch 1 step 7100 loss 0.05993
INFO:name:epoch 1 step 7200 loss 0.05291
INFO:name:epoch 1 step 7300 loss 0.05433
INFO:name:epoch 1 step 7400 loss 0.05573
INFO:name:epoch 1 step 7500 loss 0.05356
INFO:name:epoch 1 step 7600 loss 0.05521
INFO:name:epoch 1 step 7700 loss 0.05223
INFO:name:epoch 1 step 7800 loss 0.04319
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4154
INFO:name:  ********************
INFO:name:  Best eval mrr:0.4154
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3483
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 2 step 100 loss 0.04529
INFO:name:epoch 2 step 200 loss 0.03404
INFO:name:epoch 2 step 300 loss 0.03818
INFO:name:epoch 2 step 400 loss 0.04115
INFO:name:epoch 2 step 500 loss 0.02981
INFO:name:epoch 2 step 600 loss 0.03712
INFO:name:epoch 2 step 700 loss 0.03421
INFO:name:epoch 2 step 800 loss 0.03866
INFO:name:epoch 2 step 900 loss 0.03254
INFO:name:epoch 2 step 1000 loss 0.04344
INFO:name:epoch 2 step 1100 loss 0.03712
INFO:name:epoch 2 step 1200 loss 0.03576
INFO:name:epoch 2 step 1300 loss 0.04296
INFO:name:epoch 2 step 1400 loss 0.03461
INFO:name:epoch 2 step 1500 loss 0.03923
INFO:name:epoch 2 step 1600 loss 0.03843
INFO:name:epoch 2 step 1700 loss 0.03144
INFO:name:epoch 2 step 1800 loss 0.03976
INFO:name:epoch 2 step 1900 loss 0.04403
INFO:name:epoch 2 step 2000 loss 0.0433
INFO:name:epoch 2 step 2100 loss 0.03824
INFO:name:epoch 2 step 2200 loss 0.04081
INFO:name:epoch 2 step 2300 loss 0.04222
INFO:name:epoch 2 step 2400 loss 0.03983
INFO:name:epoch 2 step 2500 loss 0.04536
INFO:name:epoch 2 step 2600 loss 0.03982
INFO:name:epoch 2 step 2700 loss 0.03563
INFO:name:epoch 2 step 2800 loss 0.03712
INFO:name:epoch 2 step 2900 loss 0.04061
INFO:name:epoch 2 step 3000 loss 0.05211
INFO:name:epoch 2 step 3100 loss 0.04666
INFO:name:epoch 2 step 3200 loss 0.04933
INFO:name:epoch 2 step 3300 loss 0.03618
INFO:name:epoch 2 step 3400 loss 0.03629
INFO:name:epoch 2 step 3500 loss 0.04269
INFO:name:epoch 2 step 3600 loss 0.03773
INFO:name:epoch 2 step 3700 loss 0.026
INFO:name:epoch 2 step 3800 loss 0.0352
INFO:name:epoch 2 step 3900 loss 0.03929
INFO:name:epoch 2 step 4000 loss 0.0382
INFO:name:epoch 2 step 4100 loss 0.03077
INFO:name:epoch 2 step 4200 loss 0.04545
INFO:name:epoch 2 step 4300 loss 0.03652
INFO:name:epoch 2 step 4400 loss 0.04025
INFO:name:epoch 2 step 4500 loss 0.04054
INFO:name:epoch 2 step 4600 loss 0.03239
INFO:name:epoch 2 step 4700 loss 0.03674
INFO:name:epoch 2 step 4800 loss 0.03151
INFO:name:epoch 2 step 4900 loss 0.03267
INFO:name:epoch 2 step 5000 loss 0.03918
INFO:name:epoch 2 step 5100 loss 0.036
INFO:name:epoch 2 step 5200 loss 0.03772
INFO:name:epoch 2 step 5300 loss 0.04603
INFO:name:epoch 2 step 5400 loss 0.04111
INFO:name:epoch 2 step 5500 loss 0.03163
INFO:name:epoch 2 step 5600 loss 0.03263
INFO:name:epoch 2 step 5700 loss 0.04302
INFO:name:epoch 2 step 5800 loss 0.03586
INFO:name:epoch 2 step 5900 loss 0.03417
INFO:name:epoch 2 step 6000 loss 0.03868
INFO:name:epoch 2 step 6100 loss 0.04257
INFO:name:epoch 2 step 6200 loss 0.03312
INFO:name:epoch 2 step 6300 loss 0.03765
INFO:name:epoch 2 step 6400 loss 0.04157
INFO:name:epoch 2 step 6500 loss 0.03283
INFO:name:epoch 2 step 6600 loss 0.03741
INFO:name:epoch 2 step 6700 loss 0.04065
INFO:name:epoch 2 step 6800 loss 0.03386
INFO:name:epoch 2 step 6900 loss 0.04202
INFO:name:epoch 2 step 7000 loss 0.04044
INFO:name:epoch 2 step 7100 loss 0.03827
INFO:name:epoch 2 step 7200 loss 0.03585
INFO:name:epoch 2 step 7300 loss 0.04147
INFO:name:epoch 2 step 7400 loss 0.03553
INFO:name:epoch 2 step 7500 loss 0.03895
INFO:name:epoch 2 step 7600 loss 0.04625
INFO:name:epoch 2 step 7700 loss 0.03761
INFO:name:epoch 2 step 7800 loss 0.03923
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4252
INFO:name:  ********************
INFO:name:  Best eval mrr:0.4252
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3575
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.03467
INFO:name:epoch 3 step 200 loss 0.02483
INFO:name:epoch 3 step 300 loss 0.02826
INFO:name:epoch 3 step 400 loss 0.02769
INFO:name:epoch 3 step 500 loss 0.02513
INFO:name:epoch 3 step 600 loss 0.02569
INFO:name:epoch 3 step 700 loss 0.03019
INFO:name:epoch 3 step 800 loss 0.02172
INFO:name:epoch 3 step 900 loss 0.02633
INFO:name:epoch 3 step 1000 loss 0.02267
INFO:name:epoch 3 step 1100 loss 0.0289
INFO:name:epoch 3 step 1200 loss 0.02928
INFO:name:epoch 3 step 1300 loss 0.02505
INFO:name:epoch 3 step 1400 loss 0.03294
INFO:name:epoch 3 step 1500 loss 0.02667
INFO:name:epoch 3 step 1600 loss 0.02511
INFO:name:epoch 3 step 1700 loss 0.02653
INFO:name:epoch 3 step 1800 loss 0.02379
INFO:name:epoch 3 step 1900 loss 0.02761
INFO:name:epoch 3 step 2000 loss 0.02415
INFO:name:epoch 3 step 2100 loss 0.02847
INFO:name:epoch 3 step 2200 loss 0.02844
INFO:name:epoch 3 step 2300 loss 0.02889
INFO:name:epoch 3 step 2400 loss 0.02483
INFO:name:epoch 3 step 2500 loss 0.02558
INFO:name:epoch 3 step 2600 loss 0.03523
INFO:name:epoch 3 step 2700 loss 0.03236
INFO:name:epoch 3 step 2800 loss 0.03103
INFO:name:epoch 3 step 2900 loss 0.03291
INFO:name:epoch 3 step 3000 loss 0.02788
INFO:name:epoch 3 step 3100 loss 0.02915
INFO:name:epoch 3 step 3200 loss 0.0293
INFO:name:epoch 3 step 3300 loss 0.03287
INFO:name:epoch 3 step 3400 loss 0.02988
INFO:name:epoch 3 step 3500 loss 0.0266
INFO:name:epoch 3 step 3600 loss 0.03409
INFO:name:epoch 3 step 3700 loss 0.02366
INFO:name:epoch 3 step 3800 loss 0.02347
INFO:name:epoch 3 step 3900 loss 0.03504
INFO:name:epoch 3 step 4000 loss 0.02785
INFO:name:epoch 3 step 4100 loss 0.02752
INFO:name:epoch 3 step 4200 loss 0.02454
INFO:name:epoch 3 step 4300 loss 0.02821
INFO:name:epoch 3 step 4400 loss 0.02928
INFO:name:epoch 3 step 4500 loss 0.03084
INFO:name:epoch 3 step 4600 loss 0.02674
INFO:name:epoch 3 step 4700 loss 0.03006
INFO:name:epoch 3 step 4800 loss 0.03249
INFO:name:epoch 3 step 4900 loss 0.03013
INFO:name:epoch 3 step 5000 loss 0.0327
INFO:name:epoch 3 step 5100 loss 0.02882
INFO:name:epoch 3 step 5200 loss 0.03221
INFO:name:epoch 3 step 5300 loss 0.02735
INFO:name:epoch 3 step 5400 loss 0.03058
INFO:name:epoch 3 step 5500 loss 0.0298
INFO:name:epoch 3 step 5600 loss 0.031
INFO:name:epoch 3 step 5700 loss 0.02852
INFO:name:epoch 3 step 5800 loss 0.02891
INFO:name:epoch 3 step 5900 loss 0.03042
INFO:name:epoch 3 step 6000 loss 0.026
INFO:name:epoch 3 step 6100 loss 0.03309
INFO:name:epoch 3 step 6200 loss 0.03455
INFO:name:epoch 3 step 6300 loss 0.02842
INFO:name:epoch 3 step 6400 loss 0.02924
INFO:name:epoch 3 step 6500 loss 0.03014
INFO:name:epoch 3 step 6600 loss 0.03418
INFO:name:epoch 3 step 6700 loss 0.02979
INFO:name:epoch 3 step 6800 loss 0.03172
INFO:name:epoch 3 step 6900 loss 0.02549
INFO:name:epoch 3 step 7000 loss 0.02622
INFO:name:epoch 3 step 7100 loss 0.02364
INFO:name:epoch 3 step 7200 loss 0.02704
INFO:name:epoch 3 step 7300 loss 0.03204
INFO:name:epoch 3 step 7400 loss 0.02744
INFO:name:epoch 3 step 7500 loss 0.02575
INFO:name:epoch 3 step 7600 loss 0.02842
INFO:name:epoch 3 step 7700 loss 0.02841
INFO:name:epoch 3 step 7800 loss 0.02705
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4186
INFO:name:epoch 4 step 100 loss 0.02291
INFO:name:epoch 4 step 200 loss 0.02227
INFO:name:epoch 4 step 300 loss 0.026
INFO:name:epoch 4 step 400 loss 0.01936
INFO:name:epoch 4 step 500 loss 0.01793
INFO:name:epoch 4 step 600 loss 0.01987
INFO:name:epoch 4 step 700 loss 0.01685
INFO:name:epoch 4 step 800 loss 0.02597
INFO:name:epoch 4 step 900 loss 0.01965
INFO:name:epoch 4 step 1000 loss 0.02177
INFO:name:epoch 4 step 1100 loss 0.01853
INFO:name:epoch 4 step 1200 loss 0.02161
INFO:name:epoch 4 step 1300 loss 0.02084
INFO:name:epoch 4 step 1400 loss 0.01885
INFO:name:epoch 4 step 1500 loss 0.01693
INFO:name:epoch 4 step 1600 loss 0.02049
INFO:name:epoch 4 step 1700 loss 0.01984
INFO:name:epoch 4 step 1800 loss 0.0215
INFO:name:epoch 4 step 1900 loss 0.01719
INFO:name:epoch 4 step 2000 loss 0.02373
INFO:name:epoch 4 step 2100 loss 0.01933
INFO:name:epoch 4 step 2200 loss 0.01707
INFO:name:epoch 4 step 2300 loss 0.01769
INFO:name:epoch 4 step 2400 loss 0.02498
INFO:name:epoch 4 step 2500 loss 0.02259
INFO:name:epoch 4 step 2600 loss 0.02456
INFO:name:epoch 4 step 2700 loss 0.02205
INFO:name:epoch 4 step 2800 loss 0.02017
INFO:name:epoch 4 step 2900 loss 0.02059
INFO:name:epoch 4 step 3000 loss 0.02137
INFO:name:epoch 4 step 3100 loss 0.02146
INFO:name:epoch 4 step 3200 loss 0.02368
INFO:name:epoch 4 step 3300 loss 0.01793
INFO:name:epoch 4 step 3400 loss 0.01836
INFO:name:epoch 4 step 3500 loss 0.02066
INFO:name:epoch 4 step 3600 loss 0.02324
INFO:name:epoch 4 step 3700 loss 0.0197
INFO:name:epoch 4 step 3800 loss 0.02048
INFO:name:epoch 4 step 3900 loss 0.0195
INFO:name:epoch 4 step 4000 loss 0.02365
INFO:name:epoch 4 step 4100 loss 0.01738
INFO:name:epoch 4 step 4200 loss 0.02333
INFO:name:epoch 4 step 4300 loss 0.02479
INFO:name:epoch 4 step 4400 loss 0.02742
INFO:name:epoch 4 step 4500 loss 0.02366
INFO:name:epoch 4 step 4600 loss 0.0229
INFO:name:epoch 4 step 4700 loss 0.02438
INFO:name:epoch 4 step 4800 loss 0.02478
INFO:name:epoch 4 step 4900 loss 0.02018
INFO:name:epoch 4 step 5000 loss 0.02407
INFO:name:epoch 4 step 5100 loss 0.0242
INFO:name:epoch 4 step 5200 loss 0.02198
INFO:name:epoch 4 step 5300 loss 0.02073
INFO:name:epoch 4 step 5400 loss 0.0183
INFO:name:epoch 4 step 5500 loss 0.02074
INFO:name:epoch 4 step 5600 loss 0.02058
INFO:name:epoch 4 step 5700 loss 0.01876
INFO:name:epoch 4 step 5800 loss 0.02393
INFO:name:epoch 4 step 5900 loss 0.02001
INFO:name:epoch 4 step 6000 loss 0.02205
INFO:name:epoch 4 step 6100 loss 0.01953
INFO:name:epoch 4 step 6200 loss 0.02078
INFO:name:epoch 4 step 6300 loss 0.0228
INFO:name:epoch 4 step 6400 loss 0.02214
INFO:name:epoch 4 step 6500 loss 0.02536
INFO:name:epoch 4 step 6600 loss 0.02046
INFO:name:epoch 4 step 6700 loss 0.02875
INFO:name:epoch 4 step 6800 loss 0.02004
INFO:name:epoch 4 step 6900 loss 0.01883
INFO:name:epoch 4 step 7000 loss 0.01679
INFO:name:epoch 4 step 7100 loss 0.02222
INFO:name:epoch 4 step 7200 loss 0.02372
INFO:name:epoch 4 step 7300 loss 0.0262
INFO:name:epoch 4 step 7400 loss 0.02258
INFO:name:epoch 4 step 7500 loss 0.02146
INFO:name:epoch 4 step 7600 loss 0.02001
INFO:name:epoch 4 step 7700 loss 0.02541
INFO:name:epoch 4 step 7800 loss 0.02263
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4228
INFO:name:epoch 5 step 100 loss 0.01808
INFO:name:epoch 5 step 200 loss 0.01442
INFO:name:epoch 5 step 300 loss 0.02118
INFO:name:epoch 5 step 400 loss 0.01894
INFO:name:epoch 5 step 500 loss 0.01626
INFO:name:epoch 5 step 600 loss 0.01511
INFO:name:epoch 5 step 700 loss 0.01599
INFO:name:epoch 5 step 800 loss 0.01411
INFO:name:epoch 5 step 900 loss 0.01721
INFO:name:epoch 5 step 1000 loss 0.01916
INFO:name:epoch 5 step 1100 loss 0.01806
INFO:name:epoch 5 step 1200 loss 0.01707
INFO:name:epoch 5 step 1300 loss 0.01589
INFO:name:epoch 5 step 1400 loss 0.0168
INFO:name:epoch 5 step 1500 loss 0.0164
INFO:name:epoch 5 step 1600 loss 0.0204
INFO:name:epoch 5 step 1700 loss 0.01377
INFO:name:epoch 5 step 1800 loss 0.01677
INFO:name:epoch 5 step 1900 loss 0.01595
INFO:name:epoch 5 step 2000 loss 0.01805
INFO:name:epoch 5 step 2100 loss 0.01352
INFO:name:epoch 5 step 2200 loss 0.01709
INFO:name:epoch 5 step 2300 loss 0.02029
INFO:name:epoch 5 step 2400 loss 0.01656
INFO:name:epoch 5 step 2500 loss 0.01647
INFO:name:epoch 5 step 2600 loss 0.01826
INFO:name:epoch 5 step 2700 loss 0.01638
INFO:name:epoch 5 step 2800 loss 0.0191
INFO:name:epoch 5 step 2900 loss 0.01979
INFO:name:epoch 5 step 3000 loss 0.0174
INFO:name:epoch 5 step 3100 loss 0.01931
INFO:name:epoch 5 step 3200 loss 0.0152
INFO:name:epoch 5 step 3300 loss 0.01808
INFO:name:epoch 5 step 3400 loss 0.01671
INFO:name:epoch 5 step 3500 loss 0.01995
INFO:name:epoch 5 step 3600 loss 0.01633
INFO:name:epoch 5 step 3700 loss 0.01471
INFO:name:epoch 5 step 3800 loss 0.0199
INFO:name:epoch 5 step 3900 loss 0.01443
INFO:name:epoch 5 step 4000 loss 0.01942
INFO:name:epoch 5 step 4100 loss 0.02022
INFO:name:epoch 5 step 4200 loss 0.01734
INFO:name:epoch 5 step 4300 loss 0.01857
INFO:name:epoch 5 step 4400 loss 0.01631
INFO:name:epoch 5 step 4500 loss 0.01842
INFO:name:epoch 5 step 4600 loss 0.01782
INFO:name:epoch 5 step 4700 loss 0.01995
INFO:name:epoch 5 step 4800 loss 0.0189
INFO:name:epoch 5 step 4900 loss 0.01913
INFO:name:epoch 5 step 5000 loss 0.01849
INFO:name:epoch 5 step 5100 loss 0.01524
INFO:name:epoch 5 step 5200 loss 0.01874
INFO:name:epoch 5 step 5300 loss 0.01982
INFO:name:epoch 5 step 5400 loss 0.01859
INFO:name:epoch 5 step 5500 loss 0.01591
INFO:name:epoch 5 step 5600 loss 0.01414
INFO:name:epoch 5 step 5700 loss 0.01625
INFO:name:epoch 5 step 5800 loss 0.01718
INFO:name:epoch 5 step 5900 loss 0.01387
INFO:name:epoch 5 step 6000 loss 0.01483
INFO:name:epoch 5 step 6100 loss 0.01962
INFO:name:epoch 5 step 6200 loss 0.01671
INFO:name:epoch 5 step 6300 loss 0.01682
INFO:name:epoch 5 step 6400 loss 0.01435
INFO:name:epoch 5 step 6500 loss 0.01618
INFO:name:epoch 5 step 6600 loss 0.01887
INFO:name:epoch 5 step 6700 loss 0.01926
INFO:name:epoch 5 step 6800 loss 0.01768
INFO:name:epoch 5 step 6900 loss 0.01835
INFO:name:epoch 5 step 7000 loss 0.02316
INFO:name:epoch 5 step 7100 loss 0.0189
INFO:name:epoch 5 step 7200 loss 0.017
INFO:name:epoch 5 step 7300 loss 0.01434
INFO:name:epoch 5 step 7400 loss 0.02005
INFO:name:epoch 5 step 7500 loss 0.01731
INFO:name:epoch 5 step 7600 loss 0.02155
INFO:name:epoch 5 step 7700 loss 0.01905
INFO:name:epoch 5 step 7800 loss 0.01888
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.418
INFO:name:epoch 6 step 100 loss 0.01333
INFO:name:epoch 6 step 200 loss 0.01369
INFO:name:epoch 6 step 300 loss 0.01215
INFO:name:epoch 6 step 400 loss 0.01376
INFO:name:epoch 6 step 500 loss 0.01427
INFO:name:epoch 6 step 600 loss 0.01231
INFO:name:epoch 6 step 700 loss 0.01539
INFO:name:epoch 6 step 800 loss 0.01511
INFO:name:epoch 6 step 900 loss 0.01276
INFO:name:epoch 6 step 1000 loss 0.01472
INFO:name:epoch 6 step 1100 loss 0.01098
INFO:name:epoch 6 step 1200 loss 0.01521
INFO:name:epoch 6 step 1300 loss 0.01438
INFO:name:epoch 6 step 1400 loss 0.01635
INFO:name:epoch 6 step 1500 loss 0.01941
INFO:name:epoch 6 step 1600 loss 0.01619
INFO:name:epoch 6 step 1700 loss 0.0138
INFO:name:epoch 6 step 1800 loss 0.01153
INFO:name:epoch 6 step 1900 loss 0.01439
INFO:name:epoch 6 step 2000 loss 0.01635
INFO:name:epoch 6 step 2100 loss 0.01459
INFO:name:epoch 6 step 2200 loss 0.01566
INFO:name:epoch 6 step 2300 loss 0.0143
INFO:name:epoch 6 step 2400 loss 0.01339
INFO:name:epoch 6 step 2500 loss 0.01668
INFO:name:epoch 6 step 2600 loss 0.01479
INFO:name:epoch 6 step 2700 loss 0.01497
INFO:name:epoch 6 step 2800 loss 0.01363
INFO:name:epoch 6 step 2900 loss 0.01336
INFO:name:epoch 6 step 3000 loss 0.01628
INFO:name:epoch 6 step 3100 loss 0.01669
INFO:name:epoch 6 step 3200 loss 0.01508
INFO:name:epoch 6 step 3300 loss 0.01243
INFO:name:epoch 6 step 3400 loss 0.01678
INFO:name:epoch 6 step 3500 loss 0.01347
INFO:name:epoch 6 step 3600 loss 0.01332
INFO:name:epoch 6 step 3700 loss 0.01336
INFO:name:epoch 6 step 3800 loss 0.01602
INFO:name:epoch 6 step 3900 loss 0.01496
INFO:name:epoch 6 step 4000 loss 0.01335
INFO:name:epoch 6 step 4100 loss 0.0157
INFO:name:epoch 6 step 4200 loss 0.01452
INFO:name:epoch 6 step 4300 loss 0.01345
INFO:name:epoch 6 step 4400 loss 0.01449
INFO:name:epoch 6 step 4500 loss 0.01348
INFO:name:epoch 6 step 4600 loss 0.01423
INFO:name:epoch 6 step 4700 loss 0.01458
INFO:name:epoch 6 step 4800 loss 0.01309
INFO:name:epoch 6 step 4900 loss 0.01176
INFO:name:epoch 6 step 5000 loss 0.01399
INFO:name:epoch 6 step 5100 loss 0.01281
INFO:name:epoch 6 step 5200 loss 0.01773
INFO:name:epoch 6 step 5300 loss 0.0133
INFO:name:epoch 6 step 5400 loss 0.01092
INFO:name:epoch 6 step 5500 loss 0.01425
INFO:name:epoch 6 step 5600 loss 0.01145
INFO:name:epoch 6 step 5700 loss 0.02039
INFO:name:epoch 6 step 5800 loss 0.01299
INFO:name:epoch 6 step 5900 loss 0.0147
INFO:name:epoch 6 step 6000 loss 0.01405
INFO:name:epoch 6 step 6100 loss 0.01369
INFO:name:epoch 6 step 6200 loss 0.0168
INFO:name:epoch 6 step 6300 loss 0.01448
INFO:name:epoch 6 step 6400 loss 0.01379
INFO:name:epoch 6 step 6500 loss 0.01599
INFO:name:epoch 6 step 6600 loss 0.01354
INFO:name:epoch 6 step 6700 loss 0.01531
INFO:name:epoch 6 step 6800 loss 0.01674
INFO:name:epoch 6 step 6900 loss 0.01753
INFO:name:epoch 6 step 7000 loss 0.01556
INFO:name:epoch 6 step 7100 loss 0.01661
INFO:name:epoch 6 step 7200 loss 0.01296
INFO:name:epoch 6 step 7300 loss 0.01355
INFO:name:epoch 6 step 7400 loss 0.01492
INFO:name:epoch 6 step 7500 loss 0.01458
INFO:name:epoch 6 step 7600 loss 0.0159
INFO:name:epoch 6 step 7700 loss 0.01706
INFO:name:epoch 6 step 7800 loss 0.01572
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4062
INFO:name:epoch 7 step 100 loss 0.01432
INFO:name:epoch 7 step 200 loss 0.01037
INFO:name:epoch 7 step 300 loss 0.01091
INFO:name:epoch 7 step 400 loss 0.01081
INFO:name:epoch 7 step 500 loss 0.01076
INFO:name:epoch 7 step 600 loss 0.01083
INFO:name:epoch 7 step 700 loss 0.01236
INFO:name:epoch 7 step 800 loss 0.01135
INFO:name:epoch 7 step 900 loss 0.01242
INFO:name:epoch 7 step 1000 loss 0.01011
INFO:name:epoch 7 step 1100 loss 0.01092
INFO:name:epoch 7 step 1200 loss 0.01084
INFO:name:epoch 7 step 1300 loss 0.01235
INFO:name:epoch 7 step 1400 loss 0.0099
INFO:name:epoch 7 step 1500 loss 0.01581
INFO:name:epoch 7 step 1600 loss 0.01145
INFO:name:epoch 7 step 1700 loss 0.01098
INFO:name:epoch 7 step 1800 loss 0.01298
INFO:name:epoch 7 step 1900 loss 0.01076
INFO:name:epoch 7 step 2000 loss 0.01097
INFO:name:epoch 7 step 2100 loss 0.01037
INFO:name:epoch 7 step 2200 loss 0.01201
INFO:name:epoch 7 step 2300 loss 0.01395
INFO:name:epoch 7 step 2400 loss 0.01107
INFO:name:epoch 7 step 2500 loss 0.01234
INFO:name:epoch 7 step 2600 loss 0.01283
INFO:name:epoch 7 step 2700 loss 0.01448
INFO:name:epoch 7 step 2800 loss 0.01112
INFO:name:epoch 7 step 2900 loss 0.01331
INFO:name:epoch 7 step 3000 loss 0.01091
INFO:name:epoch 7 step 3100 loss 0.0127
INFO:name:epoch 7 step 3200 loss 0.01097
INFO:name:epoch 7 step 3300 loss 0.01172
INFO:name:epoch 7 step 3400 loss 0.01186
INFO:name:epoch 7 step 3500 loss 0.01331
INFO:name:epoch 7 step 3600 loss 0.01232
INFO:name:epoch 7 step 3700 loss 0.0133
INFO:name:epoch 7 step 3800 loss 0.01222
INFO:name:epoch 7 step 3900 loss 0.01075
INFO:name:epoch 7 step 4000 loss 0.01704
INFO:name:epoch 7 step 4100 loss 0.01251
INFO:name:epoch 7 step 4200 loss 0.01326
INFO:name:epoch 7 step 4300 loss 0.01163
INFO:name:epoch 7 step 4400 loss 0.0103
INFO:name:epoch 7 step 4500 loss 0.01062
INFO:name:epoch 7 step 4600 loss 0.01132
INFO:name:epoch 7 step 4700 loss 0.01336
INFO:name:epoch 7 step 4800 loss 0.01179
INFO:name:epoch 7 step 4900 loss 0.0115
INFO:name:epoch 7 step 5000 loss 0.0104
INFO:name:epoch 7 step 5100 loss 0.0106
INFO:name:epoch 7 step 5200 loss 0.01182
INFO:name:epoch 7 step 5300 loss 0.01412
INFO:name:epoch 7 step 5400 loss 0.01308
INFO:name:epoch 7 step 5500 loss 0.0133
INFO:name:epoch 7 step 5600 loss 0.01207
INFO:name:epoch 7 step 5700 loss 0.01292
INFO:name:epoch 7 step 5800 loss 0.01413
INFO:name:epoch 7 step 5900 loss 0.0099
INFO:name:epoch 7 step 6000 loss 0.01156
INFO:name:epoch 7 step 6100 loss 0.01298
INFO:name:epoch 7 step 6200 loss 0.01123
INFO:name:epoch 7 step 6300 loss 0.01396
INFO:name:epoch 7 step 6400 loss 0.01081
INFO:name:epoch 7 step 6500 loss 0.01123
INFO:name:epoch 7 step 6600 loss 0.01057
INFO:name:epoch 7 step 6700 loss 0.01055
INFO:name:epoch 7 step 6800 loss 0.0107
INFO:name:epoch 7 step 6900 loss 0.01262
INFO:name:epoch 7 step 7000 loss 0.01049
INFO:name:epoch 7 step 7100 loss 0.01115
INFO:name:epoch 7 step 7200 loss 0.01085
INFO:name:epoch 7 step 7300 loss 0.01374
INFO:name:epoch 7 step 7400 loss 0.01322
INFO:name:epoch 7 step 7500 loss 0.01182
INFO:name:epoch 7 step 7600 loss 0.01164
INFO:name:epoch 7 step 7700 loss 0.01159
INFO:name:epoch 7 step 7800 loss 0.01166
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3946
INFO:name:epoch 8 step 100 loss 0.01139
INFO:name:epoch 8 step 200 loss 0.01295
INFO:name:epoch 8 step 300 loss 0.01169
INFO:name:epoch 8 step 400 loss 0.01301
INFO:name:epoch 8 step 500 loss 0.01011
INFO:name:epoch 8 step 600 loss 0.00976
INFO:name:epoch 8 step 700 loss 0.01054
INFO:name:epoch 8 step 800 loss 0.01067
INFO:name:epoch 8 step 900 loss 0.0082
INFO:name:epoch 8 step 1000 loss 0.01214
INFO:name:epoch 8 step 1100 loss 0.01144
INFO:name:epoch 8 step 1200 loss 0.01018
INFO:name:epoch 8 step 1300 loss 0.01248
INFO:name:epoch 8 step 1400 loss 0.01021
INFO:name:epoch 8 step 1500 loss 0.00999
INFO:name:epoch 8 step 1600 loss 0.00981
INFO:name:epoch 8 step 1700 loss 0.01077
INFO:name:epoch 8 step 1800 loss 0.01066
INFO:name:epoch 8 step 1900 loss 0.00856
INFO:name:epoch 8 step 2000 loss 0.01106
INFO:name:epoch 8 step 2100 loss 0.01076
INFO:name:epoch 8 step 2200 loss 0.01133
INFO:name:epoch 8 step 2300 loss 0.01129
INFO:name:epoch 8 step 2400 loss 0.00989
INFO:name:epoch 8 step 2500 loss 0.01028
INFO:name:epoch 8 step 2600 loss 0.00806
INFO:name:epoch 8 step 2700 loss 0.01031
INFO:name:epoch 8 step 2800 loss 0.01091
INFO:name:epoch 8 step 2900 loss 0.00806
INFO:name:epoch 8 step 3000 loss 0.01072
INFO:name:epoch 8 step 3100 loss 0.01118
INFO:name:epoch 8 step 3200 loss 0.00979
INFO:name:epoch 8 step 3300 loss 0.01078
INFO:name:epoch 8 step 3400 loss 0.01021
INFO:name:epoch 8 step 3500 loss 0.01018
INFO:name:epoch 8 step 3600 loss 0.01069
INFO:name:epoch 8 step 3700 loss 0.01126
INFO:name:epoch 8 step 3800 loss 0.01249
INFO:name:epoch 8 step 3900 loss 0.00961
INFO:name:epoch 8 step 4000 loss 0.01446
INFO:name:epoch 8 step 4100 loss 0.01251
INFO:name:epoch 8 step 4200 loss 0.01162
INFO:name:epoch 8 step 4300 loss 0.01327
INFO:name:epoch 8 step 4400 loss 0.01197
INFO:name:epoch 8 step 4500 loss 0.0095
INFO:name:epoch 8 step 4600 loss 0.00872
INFO:name:epoch 8 step 4700 loss 0.01346
INFO:name:epoch 8 step 4800 loss 0.01117
INFO:name:epoch 8 step 4900 loss 0.0121
INFO:name:epoch 8 step 5000 loss 0.01026
INFO:name:epoch 8 step 5100 loss 0.0101
INFO:name:epoch 8 step 5200 loss 0.00991
INFO:name:epoch 8 step 5300 loss 0.01038
INFO:name:epoch 8 step 5400 loss 0.00969
INFO:name:epoch 8 step 5500 loss 0.01116
INFO:name:epoch 8 step 5600 loss 0.00821
INFO:name:epoch 8 step 5700 loss 0.00841
INFO:name:epoch 8 step 5800 loss 0.01052
INFO:name:epoch 8 step 5900 loss 0.0091
INFO:name:epoch 8 step 6000 loss 0.01139
INFO:name:epoch 8 step 6100 loss 0.00844
INFO:name:epoch 8 step 6200 loss 0.01188
INFO:name:epoch 8 step 6300 loss 0.00835
INFO:name:epoch 8 step 6400 loss 0.01254
INFO:name:epoch 8 step 6500 loss 0.01049
INFO:name:epoch 8 step 6600 loss 0.00984
INFO:name:epoch 8 step 6700 loss 0.01027
INFO:name:epoch 8 step 6800 loss 0.0108
INFO:name:epoch 8 step 6900 loss 0.01073
INFO:name:epoch 8 step 7000 loss 0.01062
INFO:name:epoch 8 step 7100 loss 0.01004
INFO:name:epoch 8 step 7200 loss 0.01427
INFO:name:epoch 8 step 7300 loss 0.00872
INFO:name:epoch 8 step 7400 loss 0.01136
INFO:name:epoch 8 step 7500 loss 0.00984
INFO:name:epoch 8 step 7600 loss 0.01209
INFO:name:epoch 8 step 7700 loss 0.00925
INFO:name:epoch 8 step 7800 loss 0.00932
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4041
INFO:name:epoch 9 step 100 loss 0.01096
INFO:name:epoch 9 step 200 loss 0.01073
INFO:name:epoch 9 step 300 loss 0.00903
INFO:name:epoch 9 step 400 loss 0.00801
INFO:name:epoch 9 step 500 loss 0.00767
INFO:name:epoch 9 step 600 loss 0.00829
INFO:name:epoch 9 step 700 loss 0.0099
INFO:name:epoch 9 step 800 loss 0.00915
INFO:name:epoch 9 step 900 loss 0.01068
INFO:name:epoch 9 step 1000 loss 0.01155
INFO:name:epoch 9 step 1100 loss 0.01233
INFO:name:epoch 9 step 1200 loss 0.00776
INFO:name:epoch 9 step 1300 loss 0.00971
INFO:name:epoch 9 step 1400 loss 0.01166
INFO:name:epoch 9 step 1500 loss 0.01011
INFO:name:epoch 9 step 1600 loss 0.00752
INFO:name:epoch 9 step 1700 loss 0.00959
INFO:name:epoch 9 step 1800 loss 0.00915
INFO:name:epoch 9 step 1900 loss 0.01049
INFO:name:epoch 9 step 2000 loss 0.00778
INFO:name:epoch 9 step 2100 loss 0.00894
INFO:name:epoch 9 step 2200 loss 0.01191
INFO:name:epoch 9 step 2300 loss 0.00995
INFO:name:epoch 9 step 2400 loss 0.01128
INFO:name:epoch 9 step 2500 loss 0.00859
INFO:name:epoch 9 step 2600 loss 0.0083
INFO:name:epoch 9 step 2700 loss 0.00971
INFO:name:epoch 9 step 2800 loss 0.01278
INFO:name:epoch 9 step 2900 loss 0.01021
INFO:name:epoch 9 step 3000 loss 0.01021
INFO:name:epoch 9 step 3100 loss 0.00854
INFO:name:epoch 9 step 3200 loss 0.00968
INFO:name:epoch 9 step 3300 loss 0.01028
INFO:name:epoch 9 step 3400 loss 0.00989
INFO:name:epoch 9 step 3500 loss 0.00867
INFO:name:epoch 9 step 3600 loss 0.01007
INFO:name:epoch 9 step 3700 loss 0.00738
INFO:name:epoch 9 step 3800 loss 0.01281
INFO:name:epoch 9 step 3900 loss 0.00957
INFO:name:epoch 9 step 4000 loss 0.00906
INFO:name:epoch 9 step 4100 loss 0.01082
INFO:name:epoch 9 step 4200 loss 0.01123
INFO:name:epoch 9 step 4300 loss 0.0081
INFO:name:epoch 9 step 4400 loss 0.01197
INFO:name:epoch 9 step 4500 loss 0.00856
INFO:name:epoch 9 step 4600 loss 0.00982
INFO:name:epoch 9 step 4700 loss 0.00734
INFO:name:epoch 9 step 4800 loss 0.00903
INFO:name:epoch 9 step 4900 loss 0.01054
INFO:name:epoch 9 step 5000 loss 0.0115
INFO:name:epoch 9 step 5100 loss 0.01014
INFO:name:epoch 9 step 5200 loss 0.01082
INFO:name:epoch 9 step 5300 loss 0.00716
INFO:name:epoch 9 step 5400 loss 0.00967
INFO:name:epoch 9 step 5500 loss 0.01145
INFO:name:epoch 9 step 5600 loss 0.00959
INFO:name:epoch 9 step 5700 loss 0.00965
INFO:name:epoch 9 step 5800 loss 0.00767
INFO:name:epoch 9 step 5900 loss 0.00765
INFO:name:epoch 9 step 6000 loss 0.00924
INFO:name:epoch 9 step 6100 loss 0.01041
INFO:name:epoch 9 step 6200 loss 0.0094
INFO:name:epoch 9 step 6300 loss 0.01248
INFO:name:epoch 9 step 6400 loss 0.01173
INFO:name:epoch 9 step 6500 loss 0.00806
INFO:name:epoch 9 step 6600 loss 0.01103
INFO:name:epoch 9 step 6700 loss 0.00884
INFO:name:epoch 9 step 6800 loss 0.00989
INFO:name:epoch 9 step 6900 loss 0.00909
INFO:name:epoch 9 step 7000 loss 0.00959
INFO:name:epoch 9 step 7100 loss 0.00938
INFO:name:epoch 9 step 7200 loss 0.00875
INFO:name:epoch 9 step 7300 loss 0.00902
INFO:name:epoch 9 step 7400 loss 0.00851
INFO:name:epoch 9 step 7500 loss 0.00864
INFO:name:epoch 9 step 7600 loss 0.00789
INFO:name:epoch 9 step 7700 loss 0.00973
INFO:name:epoch 9 step 7800 loss 0.0093
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4063
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:[{'insert_modules': ('intermediate',), 'bottleneck_dim': (64,), 'non_linearity': 'silu', 'dropout_rate': 0.1, 'normalization': None, 'skip_connection': True}, 0, {'insert_modules': ('attention.self',), 'bottleneck_dim': (32,), 'non_linearity': 'gelu_new', 'dropout_rate': 0.25, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('attention.self', 'attention.output', 'output'), 'bottleneck_dim': (32, 32, 128), 'non_linearity': 'tanh', 'dropout_rate': 0.3, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('output', 'attention.output', 'intermediate'), 'bottleneck_dim': (128, 32, 64), 'non_linearity': 'tanh', 'dropout_rate': 0.1, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('output',), 'bottleneck_dim': (256,), 'non_linearity': 'leakyrelu', 'dropout_rate': 0.2, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('attention.self', 'intermediate'), 'bottleneck_dim': (16, 64), 'non_linearity': 'leakyrelu', 'dropout_rate': 0.3, 'normalization': None, 'skip_connection': True}, 0, {'insert_modules': ('attention.output', 'attention.self', 'intermediate'), 'bottleneck_dim': (64, 32, 128), 'non_linearity': 'gelu_new', 'dropout_rate': 0.3, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('intermediate', 'attention.output', 'attention.self'), 'bottleneck_dim': (128, 64, 16), 'non_linearity': 'tanh', 'dropout_rate': 0.0, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('attention.self',), 'bottleneck_dim': (32,), 'non_linearity': 'gelu_new', 'dropout_rate': 0.0, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('attention.output', 'attention.self', 'intermediate'), 'bottleneck_dim': (32, 16, 64), 'non_linearity': 'swish', 'dropout_rate': 0.3, 'normalization': 'layer_norm', 'skip_connection': True}]
[INFO|(OpenDelta)basemodel:700]2025-01-23 22:59:08,968 >> Trainable Ratio: 4578672/130508400=3.508335%
[INFO|(OpenDelta)basemodel:702]2025-01-23 22:59:08,968 >> Delta Parameter Ratio: 4578672/130508400=3.508335%
[INFO|(OpenDelta)basemodel:704]2025-01-23 22:59:08,968 >> Static Memory 0.49 GB, Max Memory 7.94 GB
INFO:name:5.11
/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 10
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 78700
train results ([0.1248682726830779, 0.05263754935756745, 0.038466490348407545, 0.028584672151463434, 0.021363155069829616, 0.017496002198885086, 0.014559267699611083, 0.011931348624774563, 0.010646079924046932, 0.009650209869896098], [0.4013123202301177, 0.4153899768066633, 0.42516709288288557, 0.4186241792542853, 0.42277206232871084, 0.4180179174458557, 0.40615359753106106, 0.39455365450276786, 0.4040623920579654, 0.4062881224816677])
INFO:name:epoch 0 step 100 loss 0.27747
INFO:name:epoch 0 step 200 loss 0.15173
INFO:name:epoch 0 step 300 loss 0.15958
INFO:name:epoch 0 step 400 loss 0.13995
INFO:name:epoch 0 step 500 loss 0.13504
INFO:name:epoch 0 step 600 loss 0.12734
INFO:name:epoch 0 step 700 loss 0.13177
INFO:name:epoch 0 step 800 loss 0.11974
INFO:name:epoch 0 step 900 loss 0.12927
INFO:name:epoch 0 step 1000 loss 0.10477
INFO:name:epoch 0 step 1100 loss 0.10464
INFO:name:epoch 0 step 1200 loss 0.09543
INFO:name:epoch 0 step 1300 loss 0.10064
INFO:name:epoch 0 step 1400 loss 0.09634
INFO:name:epoch 0 step 1500 loss 0.09351
INFO:name:epoch 0 step 1600 loss 0.10108
INFO:name:epoch 0 step 1700 loss 0.10676
INFO:name:epoch 0 step 1800 loss 0.09337
INFO:name:epoch 0 step 1900 loss 0.08508
INFO:name:epoch 0 step 2000 loss 0.09064
INFO:name:epoch 0 step 2100 loss 0.09631
INFO:name:epoch 0 step 2200 loss 0.09116
INFO:name:epoch 0 step 2300 loss 0.08426
INFO:name:epoch 0 step 2400 loss 0.0804
INFO:name:epoch 0 step 2500 loss 0.09026
INFO:name:epoch 0 step 2600 loss 0.08913
INFO:name:epoch 0 step 2700 loss 0.08742
INFO:name:epoch 0 step 2800 loss 0.08462
INFO:name:epoch 0 step 2900 loss 0.08954
INFO:name:epoch 0 step 3000 loss 0.07541
INFO:name:epoch 0 step 3100 loss 0.09347
INFO:name:epoch 0 step 3200 loss 0.08705
INFO:name:epoch 0 step 3300 loss 0.07836
INFO:name:epoch 0 step 3400 loss 0.08844
INFO:name:epoch 0 step 3500 loss 0.09504
INFO:name:epoch 0 step 3600 loss 0.07467
INFO:name:epoch 0 step 3700 loss 0.08071
INFO:name:epoch 0 step 3800 loss 0.07937
INFO:name:epoch 0 step 3900 loss 0.07336
INFO:name:epoch 0 step 4000 loss 0.06415
INFO:name:epoch 0 step 4100 loss 0.07891
INFO:name:epoch 0 step 4200 loss 0.07653
INFO:name:epoch 0 step 4300 loss 0.08474
INFO:name:epoch 0 step 4400 loss 0.07218
INFO:name:epoch 0 step 4500 loss 0.07966
INFO:name:epoch 0 step 4600 loss 0.08546
INFO:name:epoch 0 step 4700 loss 0.07927
INFO:name:epoch 0 step 4800 loss 0.08783
INFO:name:epoch 0 step 4900 loss 0.0852
INFO:name:epoch 0 step 5000 loss 0.07412
INFO:name:epoch 0 step 5100 loss 0.08038
INFO:name:epoch 0 step 5200 loss 0.08177
INFO:name:epoch 0 step 5300 loss 0.07463
INFO:name:epoch 0 step 5400 loss 0.08203
INFO:name:epoch 0 step 5500 loss 0.07378
INFO:name:epoch 0 step 5600 loss 0.07316
INFO:name:epoch 0 step 5700 loss 0.06708
INFO:name:epoch 0 step 5800 loss 0.07973
INFO:name:epoch 0 step 5900 loss 0.06998
INFO:name:epoch 0 step 6000 loss 0.079
INFO:name:epoch 0 step 6100 loss 0.06977
INFO:name:epoch 0 step 6200 loss 0.0856
INFO:name:epoch 0 step 6300 loss 0.06829
INFO:name:epoch 0 step 6400 loss 0.07361
INFO:name:epoch 0 step 6500 loss 0.06174
INFO:name:epoch 0 step 6600 loss 0.07102
INFO:name:epoch 0 step 6700 loss 0.07866
INFO:name:epoch 0 step 6800 loss 0.0674
INFO:name:epoch 0 step 6900 loss 0.06641
INFO:name:epoch 0 step 7000 loss 0.06912
INFO:name:epoch 0 step 7100 loss 0.0744
INFO:name:epoch 0 step 7200 loss 0.06755
INFO:name:epoch 0 step 7300 loss 0.0717
INFO:name:epoch 0 step 7400 loss 0.06719
INFO:name:epoch 0 step 7500 loss 0.06575
INFO:name:epoch 0 step 7600 loss 0.07068
INFO:name:epoch 0 step 7700 loss 0.07797
INFO:name:epoch 0 step 7800 loss 0.06422
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4357
INFO:name:  ********************
INFO:name:  Best eval mrr:0.4357
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3714
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 0.05629
INFO:name:epoch 1 step 200 loss 0.04465
INFO:name:epoch 1 step 300 loss 0.04825
INFO:name:epoch 1 step 400 loss 0.03745
INFO:name:epoch 1 step 500 loss 0.04299
INFO:name:epoch 1 step 600 loss 0.04477
INFO:name:epoch 1 step 700 loss 0.0434
INFO:name:epoch 1 step 800 loss 0.04246
INFO:name:epoch 1 step 900 loss 0.04364
INFO:name:epoch 1 step 1000 loss 0.03947
INFO:name:epoch 1 step 1100 loss 0.03765
INFO:name:epoch 1 step 1200 loss 0.04631
INFO:name:epoch 1 step 1300 loss 0.03897
INFO:name:epoch 1 step 1400 loss 0.03793
INFO:name:epoch 1 step 1500 loss 0.03894
INFO:name:epoch 1 step 1600 loss 0.03788
INFO:name:epoch 1 step 1700 loss 0.03863
INFO:name:epoch 1 step 1800 loss 0.0532
INFO:name:epoch 1 step 1900 loss 0.04368
INFO:name:epoch 1 step 2000 loss 0.05043
INFO:name:epoch 1 step 2100 loss 0.03245
INFO:name:epoch 1 step 2200 loss 0.03685
INFO:name:epoch 1 step 2300 loss 0.05011
INFO:name:epoch 1 step 2400 loss 0.05079
INFO:name:epoch 1 step 2500 loss 0.04089
INFO:name:epoch 1 step 2600 loss 0.04089
INFO:name:epoch 1 step 2700 loss 0.0405
INFO:name:epoch 1 step 2800 loss 0.04171
INFO:name:epoch 1 step 2900 loss 0.04389
INFO:name:epoch 1 step 3000 loss 0.05637
INFO:name:epoch 1 step 3100 loss 0.05093
INFO:name:epoch 1 step 3200 loss 0.03461
INFO:name:epoch 1 step 3300 loss 0.04165
INFO:name:epoch 1 step 3400 loss 0.02848
INFO:name:epoch 1 step 3500 loss 0.04123
INFO:name:epoch 1 step 3600 loss 0.04643
INFO:name:epoch 1 step 3700 loss 0.0417
INFO:name:epoch 1 step 3800 loss 0.04009
INFO:name:epoch 1 step 3900 loss 0.04781
INFO:name:epoch 1 step 4000 loss 0.05291
INFO:name:epoch 1 step 4100 loss 0.04456
INFO:name:epoch 1 step 4200 loss 0.03785
INFO:name:epoch 1 step 4300 loss 0.0397
INFO:name:epoch 1 step 4400 loss 0.0386
INFO:name:epoch 1 step 4500 loss 0.04663
INFO:name:epoch 1 step 4600 loss 0.0295
INFO:name:epoch 1 step 4700 loss 0.04057
INFO:name:epoch 1 step 4800 loss 0.04243
INFO:name:epoch 1 step 4900 loss 0.04905
INFO:name:epoch 1 step 5000 loss 0.03317
INFO:name:epoch 1 step 5100 loss 0.04663
INFO:name:epoch 1 step 5200 loss 0.04314
INFO:name:epoch 1 step 5300 loss 0.04492
INFO:name:epoch 1 step 5400 loss 0.04141
INFO:name:epoch 1 step 5500 loss 0.03501
INFO:name:epoch 1 step 5600 loss 0.04286
INFO:name:epoch 1 step 5700 loss 0.0469
INFO:name:epoch 1 step 5800 loss 0.04422
INFO:name:epoch 1 step 5900 loss 0.03977
INFO:name:epoch 1 step 6000 loss 0.04997
INFO:name:epoch 1 step 6100 loss 0.04453
INFO:name:epoch 1 step 6200 loss 0.04015
INFO:name:epoch 1 step 6300 loss 0.04598
INFO:name:epoch 1 step 6400 loss 0.03576
INFO:name:epoch 1 step 6500 loss 0.04897
INFO:name:epoch 1 step 6600 loss 0.04091
INFO:name:epoch 1 step 6700 loss 0.04032
INFO:name:epoch 1 step 6800 loss 0.0397
INFO:name:epoch 1 step 6900 loss 0.04488
INFO:name:epoch 1 step 7000 loss 0.03753
INFO:name:epoch 1 step 7100 loss 0.04369
INFO:name:epoch 1 step 7200 loss 0.04603
INFO:name:epoch 1 step 7300 loss 0.04185
INFO:name:epoch 1 step 7400 loss 0.03756
INFO:name:epoch 1 step 7500 loss 0.04759
INFO:name:epoch 1 step 7600 loss 0.0397
INFO:name:epoch 1 step 7700 loss 0.04288
INFO:name:epoch 1 step 7800 loss 0.03616
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4266
INFO:name:epoch 2 step 100 loss 0.03482
INFO:name:epoch 2 step 200 loss 0.03175
INFO:name:epoch 2 step 300 loss 0.02975
INFO:name:epoch 2 step 400 loss 0.02336
INFO:name:epoch 2 step 500 loss 0.03089
INFO:name:epoch 2 step 600 loss 0.02665
INFO:name:epoch 2 step 700 loss 0.02289
INFO:name:epoch 2 step 800 loss 0.02213
INFO:name:epoch 2 step 900 loss 0.03095
INFO:name:epoch 2 step 1000 loss 0.01899
INFO:name:epoch 2 step 1100 loss 0.02144
INFO:name:epoch 2 step 1200 loss 0.02697
INFO:name:epoch 2 step 1300 loss 0.02991
INFO:name:epoch 2 step 1400 loss 0.02333
INFO:name:epoch 2 step 1500 loss 0.02979
INFO:name:epoch 2 step 1600 loss 0.02946
INFO:name:epoch 2 step 1700 loss 0.02837
INFO:name:epoch 2 step 1800 loss 0.03366
INFO:name:epoch 2 step 1900 loss 0.03651
INFO:name:epoch 2 step 2000 loss 0.02326
INFO:name:epoch 2 step 2100 loss 0.02919
INFO:name:epoch 2 step 2200 loss 0.03192
INFO:name:epoch 2 step 2300 loss 0.02706
INFO:name:epoch 2 step 2400 loss 0.02854
INFO:name:epoch 2 step 2500 loss 0.02831
INFO:name:epoch 2 step 2600 loss 0.03345
INFO:name:epoch 2 step 2700 loss 0.02598
INFO:name:epoch 2 step 2800 loss 0.03002
INFO:name:epoch 2 step 2900 loss 0.02648
INFO:name:epoch 2 step 3000 loss 0.03882
INFO:name:epoch 2 step 3100 loss 0.0327
INFO:name:epoch 2 step 3200 loss 0.02896
INFO:name:epoch 2 step 3300 loss 0.02808
INFO:name:epoch 2 step 3400 loss 0.0343
INFO:name:epoch 2 step 3500 loss 0.02764
INFO:name:epoch 2 step 3600 loss 0.03092
INFO:name:epoch 2 step 3700 loss 0.03491
INFO:name:epoch 2 step 3800 loss 0.02868
INFO:name:epoch 2 step 3900 loss 0.02627
INFO:name:epoch 2 step 4000 loss 0.0313
INFO:name:epoch 2 step 4100 loss 0.02786
INFO:name:epoch 2 step 4200 loss 0.02531
INFO:name:epoch 2 step 4300 loss 0.03289
INFO:name:epoch 2 step 4400 loss 0.03046
INFO:name:epoch 2 step 4500 loss 0.03084
INFO:name:epoch 2 step 4600 loss 0.02034
INFO:name:epoch 2 step 4700 loss 0.02988
INFO:name:epoch 2 step 4800 loss 0.03145
INFO:name:epoch 2 step 4900 loss 0.02662
INFO:name:epoch 2 step 5000 loss 0.03051
INFO:name:epoch 2 step 5100 loss 0.02828
INFO:name:epoch 2 step 5200 loss 0.02642
INFO:name:epoch 2 step 5300 loss 0.02686
INFO:name:epoch 2 step 5400 loss 0.02791
INFO:name:epoch 2 step 5500 loss 0.02879
INFO:name:epoch 2 step 5600 loss 0.03098
INFO:name:epoch 2 step 5700 loss 0.03135
INFO:name:epoch 2 step 5800 loss 0.0322
INFO:name:epoch 2 step 5900 loss 0.03631
INFO:name:epoch 2 step 6000 loss 0.03104
INFO:name:epoch 2 step 6100 loss 0.02869
INFO:name:epoch 2 step 6200 loss 0.03422
INFO:name:epoch 2 step 6300 loss 0.0298
INFO:name:epoch 2 step 6400 loss 0.03199
INFO:name:epoch 2 step 6500 loss 0.02922
INFO:name:epoch 2 step 6600 loss 0.02845
INFO:name:epoch 2 step 6700 loss 0.03036
INFO:name:epoch 2 step 6800 loss 0.02517
INFO:name:epoch 2 step 6900 loss 0.02795
INFO:name:epoch 2 step 7000 loss 0.02534
INFO:name:epoch 2 step 7100 loss 0.03287
INFO:name:epoch 2 step 7200 loss 0.02552
INFO:name:epoch 2 step 7300 loss 0.03291
INFO:name:epoch 2 step 7400 loss 0.0329
INFO:name:epoch 2 step 7500 loss 0.02606
INFO:name:epoch 2 step 7600 loss 0.03036
INFO:name:epoch 2 step 7700 loss 0.02984
INFO:name:epoch 2 step 7800 loss 0.03441
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4525
INFO:name:  ********************
INFO:name:  Best eval mrr:0.4525
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3846
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.02164
INFO:name:epoch 3 step 200 loss 0.0174
INFO:name:epoch 3 step 300 loss 0.017
INFO:name:epoch 3 step 400 loss 0.01925
INFO:name:epoch 3 step 500 loss 0.01737
INFO:name:epoch 3 step 600 loss 0.01751
INFO:name:epoch 3 step 700 loss 0.01771
INFO:name:epoch 3 step 800 loss 0.01717
INFO:name:epoch 3 step 900 loss 0.01659
INFO:name:epoch 3 step 1000 loss 0.02116
INFO:name:epoch 3 step 1100 loss 0.01882
INFO:name:epoch 3 step 1200 loss 0.02098
INFO:name:epoch 3 step 1300 loss 0.01629
INFO:name:epoch 3 step 1400 loss 0.0202
INFO:name:epoch 3 step 1500 loss 0.01249
INFO:name:epoch 3 step 1600 loss 0.01937
INFO:name:epoch 3 step 1700 loss 0.02667
INFO:name:epoch 3 step 1800 loss 0.0214
INFO:name:epoch 3 step 1900 loss 0.01919
INFO:name:epoch 3 step 2000 loss 0.02128
INFO:name:epoch 3 step 2100 loss 0.0186
INFO:name:epoch 3 step 2200 loss 0.02149
INFO:name:epoch 3 step 2300 loss 0.02162
INFO:name:epoch 3 step 2400 loss 0.02138
INFO:name:epoch 3 step 2500 loss 0.01838
INFO:name:epoch 3 step 2600 loss 0.0179
INFO:name:epoch 3 step 2700 loss 0.01931
INFO:name:epoch 3 step 2800 loss 0.01984
INFO:name:epoch 3 step 2900 loss 0.01968
INFO:name:epoch 3 step 3000 loss 0.01885
INFO:name:epoch 3 step 3100 loss 0.01993
INFO:name:epoch 3 step 3200 loss 0.02271
INFO:name:epoch 3 step 3300 loss 0.01979
INFO:name:epoch 3 step 3400 loss 0.0204
INFO:name:epoch 3 step 3500 loss 0.01621
INFO:name:epoch 3 step 3600 loss 0.0228
INFO:name:epoch 3 step 3700 loss 0.0201
INFO:name:epoch 3 step 3800 loss 0.01856
INFO:name:epoch 3 step 3900 loss 0.01665
INFO:name:epoch 3 step 4000 loss 0.02026
INFO:name:epoch 3 step 4100 loss 0.01968
INFO:name:epoch 3 step 4200 loss 0.01846
INFO:name:epoch 3 step 4300 loss 0.02059
INFO:name:epoch 3 step 4400 loss 0.02265
INFO:name:epoch 3 step 4500 loss 0.01952
INFO:name:epoch 3 step 4600 loss 0.01732
INFO:name:epoch 3 step 4700 loss 0.01898
INFO:name:epoch 3 step 4800 loss 0.01927
INFO:name:epoch 3 step 4900 loss 0.02176
INFO:name:epoch 3 step 5000 loss 0.01824
INFO:name:epoch 3 step 5100 loss 0.01718
INFO:name:epoch 3 step 5200 loss 0.01989
INFO:name:epoch 3 step 5300 loss 0.026
INFO:name:epoch 3 step 5400 loss 0.0229
INFO:name:epoch 3 step 5500 loss 0.02842
INFO:name:epoch 3 step 5600 loss 0.02472
INFO:name:epoch 3 step 5700 loss 0.02173
INFO:name:epoch 3 step 5800 loss 0.02121
INFO:name:epoch 3 step 5900 loss 0.02155
INFO:name:epoch 3 step 6000 loss 0.02215
INFO:name:epoch 3 step 6100 loss 0.01843
INFO:name:epoch 3 step 6200 loss 0.01933
INFO:name:epoch 3 step 6300 loss 0.02099
INFO:name:epoch 3 step 6400 loss 0.02301
INFO:name:epoch 3 step 6500 loss 0.02608
INFO:name:epoch 3 step 6600 loss 0.02378
INFO:name:epoch 3 step 6700 loss 0.022
INFO:name:epoch 3 step 6800 loss 0.0181
INFO:name:epoch 3 step 6900 loss 0.02216
INFO:name:epoch 3 step 7000 loss 0.0257
INFO:name:epoch 3 step 7100 loss 0.01791
INFO:name:epoch 3 step 7200 loss 0.01902
INFO:name:epoch 3 step 7300 loss 0.02392
INFO:name:epoch 3 step 7400 loss 0.02157
INFO:name:epoch 3 step 7500 loss 0.01957
INFO:name:epoch 3 step 7600 loss 0.01555
INFO:name:epoch 3 step 7700 loss 0.02379
INFO:name:epoch 3 step 7800 loss 0.02422
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4609
INFO:name:  ********************
INFO:name:  Best eval mrr:0.4609
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3947
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 4 step 100 loss 0.01429
INFO:name:epoch 4 step 200 loss 0.01304
INFO:name:epoch 4 step 300 loss 0.01218
INFO:name:epoch 4 step 400 loss 0.0111
INFO:name:epoch 4 step 500 loss 0.01513
INFO:name:epoch 4 step 600 loss 0.01422
INFO:name:epoch 4 step 700 loss 0.01333
INFO:name:epoch 4 step 800 loss 0.01633
INFO:name:epoch 4 step 900 loss 0.01145
INFO:name:epoch 4 step 1000 loss 0.0135
INFO:name:epoch 4 step 1100 loss 0.01506
INFO:name:epoch 4 step 1200 loss 0.01393
INFO:name:epoch 4 step 1300 loss 0.00993
INFO:name:epoch 4 step 1400 loss 0.01289
INFO:name:epoch 4 step 1500 loss 0.01403
INFO:name:epoch 4 step 1600 loss 0.01284
INFO:name:epoch 4 step 1700 loss 0.01296
INFO:name:epoch 4 step 1800 loss 0.01424
INFO:name:epoch 4 step 1900 loss 0.013
INFO:name:epoch 4 step 2000 loss 0.01782
INFO:name:epoch 4 step 2100 loss 0.01685
INFO:name:epoch 4 step 2200 loss 0.0179
INFO:name:epoch 4 step 2300 loss 0.01446
INFO:name:epoch 4 step 2400 loss 0.01428
INFO:name:epoch 4 step 2500 loss 0.01424
INFO:name:epoch 4 step 2600 loss 0.01335
INFO:name:epoch 4 step 2700 loss 0.01638
INFO:name:epoch 4 step 2800 loss 0.01529
INFO:name:epoch 4 step 2900 loss 0.01232
INFO:name:epoch 4 step 3000 loss 0.01927
INFO:name:epoch 4 step 3100 loss 0.01615
INFO:name:epoch 4 step 3200 loss 0.01508
INFO:name:epoch 4 step 3300 loss 0.01044
INFO:name:epoch 4 step 3400 loss 0.01607
INFO:name:epoch 4 step 3500 loss 0.01415
INFO:name:epoch 4 step 3600 loss 0.01603
INFO:name:epoch 4 step 3700 loss 0.01526
INFO:name:epoch 4 step 3800 loss 0.01492
INFO:name:epoch 4 step 3900 loss 0.01191
INFO:name:epoch 4 step 4000 loss 0.01257
INFO:name:epoch 4 step 4100 loss 0.01761
INFO:name:epoch 4 step 4200 loss 0.01748
INFO:name:epoch 4 step 4300 loss 0.01611
INFO:name:epoch 4 step 4400 loss 0.01677
INFO:name:epoch 4 step 4500 loss 0.01553
INFO:name:epoch 4 step 4600 loss 0.01454
INFO:name:epoch 4 step 4700 loss 0.01519
INFO:name:epoch 4 step 4800 loss 0.01338
INFO:name:epoch 4 step 4900 loss 0.01463
INFO:name:epoch 4 step 5000 loss 0.01098
INFO:name:epoch 4 step 5100 loss 0.0131
INFO:name:epoch 4 step 5200 loss 0.01774
INFO:name:epoch 4 step 5300 loss 0.01501
INFO:name:epoch 4 step 5400 loss 0.01387
INFO:name:epoch 4 step 5500 loss 0.01565
INFO:name:epoch 4 step 5600 loss 0.01544
INFO:name:epoch 4 step 5700 loss 0.01299
INFO:name:epoch 4 step 5800 loss 0.01848
INFO:name:epoch 4 step 5900 loss 0.01366
INFO:name:epoch 4 step 6000 loss 0.01221
INFO:name:epoch 4 step 6100 loss 0.01411
INFO:name:epoch 4 step 6200 loss 0.01413
INFO:name:epoch 4 step 6300 loss 0.0159
INFO:name:epoch 4 step 6400 loss 0.01324
INFO:name:epoch 4 step 6500 loss 0.01659
INFO:name:epoch 4 step 6600 loss 0.01792
INFO:name:epoch 4 step 6700 loss 0.01315
INFO:name:epoch 4 step 6800 loss 0.01485
INFO:name:epoch 4 step 6900 loss 0.01686
INFO:name:epoch 4 step 7000 loss 0.01699
INFO:name:epoch 4 step 7100 loss 0.01687
INFO:name:epoch 4 step 7200 loss 0.01244
INFO:name:epoch 4 step 7300 loss 0.01471
INFO:name:epoch 4 step 7400 loss 0.01512
INFO:name:epoch 4 step 7500 loss 0.0175
INFO:name:epoch 4 step 7600 loss 0.01055
INFO:name:epoch 4 step 7700 loss 0.01757
INFO:name:epoch 4 step 7800 loss 0.01582
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.456
INFO:name:epoch 5 step 100 loss 0.01534
INFO:name:epoch 5 step 200 loss 0.01201
INFO:name:epoch 5 step 300 loss 0.01202
INFO:name:epoch 5 step 400 loss 0.01088
INFO:name:epoch 5 step 500 loss 0.01061
INFO:name:epoch 5 step 600 loss 0.00924
INFO:name:epoch 5 step 700 loss 0.0099
INFO:name:epoch 5 step 800 loss 0.01478
INFO:name:epoch 5 step 900 loss 0.00982
INFO:name:epoch 5 step 1000 loss 0.01197
INFO:name:epoch 5 step 1100 loss 0.01089
INFO:name:epoch 5 step 1200 loss 0.01226
INFO:name:epoch 5 step 1300 loss 0.01094
INFO:name:epoch 5 step 1400 loss 0.01006
INFO:name:epoch 5 step 1500 loss 0.01026
INFO:name:epoch 5 step 1600 loss 0.00972
INFO:name:epoch 5 step 1700 loss 0.01136
INFO:name:epoch 5 step 1800 loss 0.00929
INFO:name:epoch 5 step 1900 loss 0.01536
INFO:name:epoch 5 step 2000 loss 0.01282
INFO:name:epoch 5 step 2100 loss 0.01402
INFO:name:epoch 5 step 2200 loss 0.0089
INFO:name:epoch 5 step 2300 loss 0.01181
INFO:name:epoch 5 step 2400 loss 0.01095
INFO:name:epoch 5 step 2500 loss 0.01267
INFO:name:epoch 5 step 2600 loss 0.00965
INFO:name:epoch 5 step 2700 loss 0.00983
INFO:name:epoch 5 step 2800 loss 0.01345
INFO:name:epoch 5 step 2900 loss 0.01264
INFO:name:epoch 5 step 3000 loss 0.0095
INFO:name:epoch 5 step 3100 loss 0.01233
INFO:name:epoch 5 step 3200 loss 0.01027
INFO:name:epoch 5 step 3300 loss 0.01095
INFO:name:epoch 5 step 3400 loss 0.01101
INFO:name:epoch 5 step 3500 loss 0.01139
INFO:name:epoch 5 step 3600 loss 0.01168
INFO:name:epoch 5 step 3700 loss 0.01051
INFO:name:epoch 5 step 3800 loss 0.00863
INFO:name:epoch 5 step 3900 loss 0.01372
INFO:name:epoch 5 step 4000 loss 0.01147
INFO:name:epoch 5 step 4100 loss 0.01168
INFO:name:epoch 5 step 4200 loss 0.00982
INFO:name:epoch 5 step 4300 loss 0.01216
INFO:name:epoch 5 step 4400 loss 0.01235
INFO:name:epoch 5 step 4500 loss 0.01316
INFO:name:epoch 5 step 4600 loss 0.01242
INFO:name:epoch 5 step 4700 loss 0.01463
INFO:name:epoch 5 step 4800 loss 0.01791
INFO:name:epoch 5 step 4900 loss 0.01269
INFO:name:epoch 5 step 5000 loss 0.01287
INFO:name:epoch 5 step 5100 loss 0.01307
INFO:name:epoch 5 step 5200 loss 0.00954
INFO:name:epoch 5 step 5300 loss 0.01253
INFO:name:epoch 5 step 5400 loss 0.01069
INFO:name:epoch 5 step 5500 loss 0.00882
INFO:name:epoch 5 step 5600 loss 0.01229
INFO:name:epoch 5 step 5700 loss 0.01063
INFO:name:epoch 5 step 5800 loss 0.0121
INFO:name:epoch 5 step 5900 loss 0.00982
INFO:name:epoch 5 step 6000 loss 0.01286
INFO:name:epoch 5 step 6100 loss 0.01232
INFO:name:epoch 5 step 6200 loss 0.01001
INFO:name:epoch 5 step 6300 loss 0.01206
INFO:name:epoch 5 step 6400 loss 0.01034
INFO:name:epoch 5 step 6500 loss 0.00967
INFO:name:epoch 5 step 6600 loss 0.01161
INFO:name:epoch 5 step 6700 loss 0.01334
INFO:name:epoch 5 step 6800 loss 0.01307
INFO:name:epoch 5 step 6900 loss 0.01058
INFO:name:epoch 5 step 7000 loss 0.01192
INFO:name:epoch 5 step 7100 loss 0.01419
INFO:name:epoch 5 step 7200 loss 0.01478
INFO:name:epoch 5 step 7300 loss 0.01067
INFO:name:epoch 5 step 7400 loss 0.01368
INFO:name:epoch 5 step 7500 loss 0.01263
INFO:name:epoch 5 step 7600 loss 0.01248
INFO:name:epoch 5 step 7700 loss 0.01477
INFO:name:epoch 5 step 7800 loss 0.00809
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.464
INFO:name:  ********************
INFO:name:  Best eval mrr:0.464
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3979
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 6 step 100 loss 0.01185
INFO:name:epoch 6 step 200 loss 0.0092
INFO:name:epoch 6 step 300 loss 0.00993
INFO:name:epoch 6 step 400 loss 0.01123
INFO:name:epoch 6 step 500 loss 0.01254
INFO:name:epoch 6 step 600 loss 0.01135
INFO:name:epoch 6 step 700 loss 0.0101
INFO:name:epoch 6 step 800 loss 0.0092
INFO:name:epoch 6 step 900 loss 0.00974
INFO:name:epoch 6 step 1000 loss 0.00811
INFO:name:epoch 6 step 1100 loss 0.00984
INFO:name:epoch 6 step 1200 loss 0.01032
INFO:name:epoch 6 step 1300 loss 0.00936
INFO:name:epoch 6 step 1400 loss 0.01009
INFO:name:epoch 6 step 1500 loss 0.00831
INFO:name:epoch 6 step 1600 loss 0.00997
INFO:name:epoch 6 step 1700 loss 0.01009
INFO:name:epoch 6 step 1800 loss 0.01303
INFO:name:epoch 6 step 1900 loss 0.00867
INFO:name:epoch 6 step 2000 loss 0.01345
INFO:name:epoch 6 step 2100 loss 0.01168
INFO:name:epoch 6 step 2200 loss 0.00924
INFO:name:epoch 6 step 2300 loss 0.01095
INFO:name:epoch 6 step 2400 loss 0.00835
INFO:name:epoch 6 step 2500 loss 0.00904
INFO:name:epoch 6 step 2600 loss 0.00862
INFO:name:epoch 6 step 2700 loss 0.00652
INFO:name:epoch 6 step 2800 loss 0.01067
INFO:name:epoch 6 step 2900 loss 0.00908
INFO:name:epoch 6 step 3000 loss 0.01172
INFO:name:epoch 6 step 3100 loss 0.00879
INFO:name:epoch 6 step 3200 loss 0.01285
INFO:name:epoch 6 step 3300 loss 0.01202
INFO:name:epoch 6 step 3400 loss 0.01006
INFO:name:epoch 6 step 3500 loss 0.01181
INFO:name:epoch 6 step 3600 loss 0.01037
INFO:name:epoch 6 step 3700 loss 0.00713
INFO:name:epoch 6 step 3800 loss 0.00831
INFO:name:epoch 6 step 3900 loss 0.00817
INFO:name:epoch 6 step 4000 loss 0.01109
INFO:name:epoch 6 step 4100 loss 0.0096
INFO:name:epoch 6 step 4200 loss 0.00726
INFO:name:epoch 6 step 4300 loss 0.0113
INFO:name:epoch 6 step 4400 loss 0.00945
INFO:name:epoch 6 step 4500 loss 0.01116
INFO:name:epoch 6 step 4600 loss 0.01058
INFO:name:epoch 6 step 4700 loss 0.01088
INFO:name:epoch 6 step 4800 loss 0.01079
INFO:name:epoch 6 step 4900 loss 0.01018
INFO:name:epoch 6 step 5000 loss 0.00977
INFO:name:epoch 6 step 5100 loss 0.00984
INFO:name:epoch 6 step 5200 loss 0.01027
INFO:name:epoch 6 step 5300 loss 0.00928
INFO:name:epoch 6 step 5400 loss 0.00988
INFO:name:epoch 6 step 5500 loss 0.00882
INFO:name:epoch 6 step 5600 loss 0.00852
INFO:name:epoch 6 step 5700 loss 0.0109
INFO:name:epoch 6 step 5800 loss 0.00899
INFO:name:epoch 6 step 5900 loss 0.01049
INFO:name:epoch 6 step 6000 loss 0.0118
INFO:name:epoch 6 step 6100 loss 0.01017
INFO:name:epoch 6 step 6200 loss 0.00857
INFO:name:epoch 6 step 6300 loss 0.01088
INFO:name:epoch 6 step 6400 loss 0.01063
INFO:name:epoch 6 step 6500 loss 0.00898
INFO:name:epoch 6 step 6600 loss 0.00661
INFO:name:epoch 6 step 6700 loss 0.01
INFO:name:epoch 6 step 6800 loss 0.00987
INFO:name:epoch 6 step 6900 loss 0.00801
INFO:name:epoch 6 step 7000 loss 0.00728
INFO:name:epoch 6 step 7100 loss 0.00788
INFO:name:epoch 6 step 7200 loss 0.01558
INFO:name:epoch 6 step 7300 loss 0.01017
INFO:name:epoch 6 step 7400 loss 0.00898
INFO:name:epoch 6 step 7500 loss 0.0081
INFO:name:epoch 6 step 7600 loss 0.0085
INFO:name:epoch 6 step 7700 loss 0.00966
INFO:name:epoch 6 step 7800 loss 0.01166
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4628
INFO:name:epoch 7 step 100 loss 0.00915
INFO:name:epoch 7 step 200 loss 0.00745
INFO:name:epoch 7 step 300 loss 0.00962
INFO:name:epoch 7 step 400 loss 0.00911
INFO:name:epoch 7 step 500 loss 0.01011
INFO:name:epoch 7 step 600 loss 0.00794
INFO:name:epoch 7 step 700 loss 0.00838
INFO:name:epoch 7 step 800 loss 0.00966
INFO:name:epoch 7 step 900 loss 0.00884
INFO:name:epoch 7 step 1000 loss 0.00743
INFO:name:epoch 7 step 1100 loss 0.00689
INFO:name:epoch 7 step 1200 loss 0.00894
INFO:name:epoch 7 step 1300 loss 0.00803
INFO:name:epoch 7 step 1400 loss 0.00913
INFO:name:epoch 7 step 1500 loss 0.00981
INFO:name:epoch 7 step 1600 loss 0.01142
INFO:name:epoch 7 step 1700 loss 0.00812
INFO:name:epoch 7 step 1800 loss 0.00946
INFO:name:epoch 7 step 1900 loss 0.00844
INFO:name:epoch 7 step 2000 loss 0.00699
INFO:name:epoch 7 step 2100 loss 0.00918
INFO:name:epoch 7 step 2200 loss 0.01063
INFO:name:epoch 7 step 2300 loss 0.00777
INFO:name:epoch 7 step 2400 loss 0.00665
INFO:name:epoch 7 step 2500 loss 0.00934
INFO:name:epoch 7 step 2600 loss 0.00845
INFO:name:epoch 7 step 2700 loss 0.00732
INFO:name:epoch 7 step 2800 loss 0.00741
INFO:name:epoch 7 step 2900 loss 0.01008
INFO:name:epoch 7 step 3000 loss 0.00929
INFO:name:epoch 7 step 3100 loss 0.00899
INFO:name:epoch 7 step 3200 loss 0.00916
INFO:name:epoch 7 step 3300 loss 0.00915
INFO:name:epoch 7 step 3400 loss 0.00829
INFO:name:epoch 7 step 3500 loss 0.00757
INFO:name:epoch 7 step 3600 loss 0.00854
INFO:name:epoch 7 step 3700 loss 0.00776
INFO:name:epoch 7 step 3800 loss 0.00847
INFO:name:epoch 7 step 3900 loss 0.00797
INFO:name:epoch 7 step 4000 loss 0.00881
INFO:name:epoch 7 step 4100 loss 0.00958
INFO:name:epoch 7 step 4200 loss 0.00678
INFO:name:epoch 7 step 4300 loss 0.00849
INFO:name:epoch 7 step 4400 loss 0.00633
INFO:name:epoch 7 step 4500 loss 0.00783
INFO:name:epoch 7 step 4600 loss 0.00913
INFO:name:epoch 7 step 4700 loss 0.01002
INFO:name:epoch 7 step 4800 loss 0.00745
INFO:name:epoch 7 step 4900 loss 0.00821
INFO:name:epoch 7 step 5000 loss 0.00671
INFO:name:epoch 7 step 5100 loss 0.00688
INFO:name:epoch 7 step 5200 loss 0.0084
INFO:name:epoch 7 step 5300 loss 0.007
INFO:name:epoch 7 step 5400 loss 0.00946
INFO:name:epoch 7 step 5500 loss 0.0065
INFO:name:epoch 7 step 5600 loss 0.0072
INFO:name:epoch 7 step 5700 loss 0.00757
INFO:name:epoch 7 step 5800 loss 0.01006
INFO:name:epoch 7 step 5900 loss 0.00722
INFO:name:epoch 7 step 6000 loss 0.01041
INFO:name:epoch 7 step 6100 loss 0.00916
INFO:name:epoch 7 step 6200 loss 0.00805
INFO:name:epoch 7 step 6300 loss 0.00688
INFO:name:epoch 7 step 6400 loss 0.00771
INFO:name:epoch 7 step 6500 loss 0.01103
INFO:name:epoch 7 step 6600 loss 0.00761
INFO:name:epoch 7 step 6700 loss 0.0068
INFO:name:epoch 7 step 6800 loss 0.00945
INFO:name:epoch 7 step 6900 loss 0.00747
INFO:name:epoch 7 step 7000 loss 0.00792
INFO:name:epoch 7 step 7100 loss 0.0066
INFO:name:epoch 7 step 7200 loss 0.00778
INFO:name:epoch 7 step 7300 loss 0.00849
INFO:name:epoch 7 step 7400 loss 0.00711
INFO:name:epoch 7 step 7500 loss 0.00831
INFO:name:epoch 7 step 7600 loss 0.00742
INFO:name:epoch 7 step 7700 loss 0.00827
INFO:name:epoch 7 step 7800 loss 0.00857
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4517
INFO:name:epoch 8 step 100 loss 0.00706
INFO:name:epoch 8 step 200 loss 0.00719
INFO:name:epoch 8 step 300 loss 0.00651
INFO:name:epoch 8 step 400 loss 0.00591
INFO:name:epoch 8 step 500 loss 0.00721
INFO:name:epoch 8 step 600 loss 0.0074
INFO:name:epoch 8 step 700 loss 0.00658
INFO:name:epoch 8 step 800 loss 0.00667
INFO:name:epoch 8 step 900 loss 0.00638
INFO:name:epoch 8 step 1000 loss 0.00721
INFO:name:epoch 8 step 1100 loss 0.0065
INFO:name:epoch 8 step 1200 loss 0.00756
INFO:name:epoch 8 step 1300 loss 0.00711
INFO:name:epoch 8 step 1400 loss 0.0083
INFO:name:epoch 8 step 1500 loss 0.00692
INFO:name:epoch 8 step 1600 loss 0.00722
INFO:name:epoch 8 step 1700 loss 0.00635
INFO:name:epoch 8 step 1800 loss 0.00667
INFO:name:epoch 8 step 1900 loss 0.01115
INFO:name:epoch 8 step 2000 loss 0.00925
INFO:name:epoch 8 step 2100 loss 0.00678
INFO:name:epoch 8 step 2200 loss 0.0079
INFO:name:epoch 8 step 2300 loss 0.00816
INFO:name:epoch 8 step 2400 loss 0.00667
INFO:name:epoch 8 step 2500 loss 0.00605
INFO:name:epoch 8 step 2600 loss 0.0072
INFO:name:epoch 8 step 2700 loss 0.01057
INFO:name:epoch 8 step 2800 loss 0.00672
INFO:name:epoch 8 step 2900 loss 0.00589
INFO:name:epoch 8 step 3000 loss 0.00757
INFO:name:epoch 8 step 3100 loss 0.00604
INFO:name:epoch 8 step 3200 loss 0.00699
INFO:name:epoch 8 step 3300 loss 0.00998
INFO:name:epoch 8 step 3400 loss 0.00707
INFO:name:epoch 8 step 3500 loss 0.00539
INFO:name:epoch 8 step 3600 loss 0.00731
INFO:name:epoch 8 step 3700 loss 0.00604
INFO:name:epoch 8 step 3800 loss 0.00606
INFO:name:epoch 8 step 3900 loss 0.00768
INFO:name:epoch 8 step 4000 loss 0.00641
INFO:name:epoch 8 step 4100 loss 0.0077
INFO:name:epoch 8 step 4200 loss 0.00662
INFO:name:epoch 8 step 4300 loss 0.00886
INFO:name:epoch 8 step 4400 loss 0.00866
INFO:name:epoch 8 step 4500 loss 0.0074
INFO:name:epoch 8 step 4600 loss 0.00669
INFO:name:epoch 8 step 4700 loss 0.00874
INFO:name:epoch 8 step 4800 loss 0.00521
INFO:name:epoch 8 step 4900 loss 0.00981
INFO:name:epoch 8 step 5000 loss 0.00609
INFO:name:epoch 8 step 5100 loss 0.00653
INFO:name:epoch 8 step 5200 loss 0.00685
INFO:name:epoch 8 step 5300 loss 0.00778
INFO:name:epoch 8 step 5400 loss 0.00635
INFO:name:epoch 8 step 5500 loss 0.00886
INFO:name:epoch 8 step 5600 loss 0.00559
INFO:name:epoch 8 step 5700 loss 0.00631
INFO:name:epoch 8 step 5800 loss 0.0091
INFO:name:epoch 8 step 5900 loss 0.00631
INFO:name:epoch 8 step 6000 loss 0.00633
INFO:name:epoch 8 step 6100 loss 0.00924
INFO:name:epoch 8 step 6200 loss 0.00874
INFO:name:epoch 8 step 6300 loss 0.00569
INFO:name:epoch 8 step 6400 loss 0.00576
INFO:name:epoch 8 step 6500 loss 0.00724
INFO:name:epoch 8 step 6600 loss 0.00895
INFO:name:epoch 8 step 6700 loss 0.00621
INFO:name:epoch 8 step 6800 loss 0.00593
INFO:name:epoch 8 step 6900 loss 0.00774
INFO:name:epoch 8 step 7000 loss 0.00733
INFO:name:epoch 8 step 7100 loss 0.00586
INFO:name:epoch 8 step 7200 loss 0.00705
INFO:name:epoch 8 step 7300 loss 0.00571
INFO:name:epoch 8 step 7400 loss 0.00605
INFO:name:epoch 8 step 7500 loss 0.00808
INFO:name:epoch 8 step 7600 loss 0.00543
INFO:name:epoch 8 step 7700 loss 0.00729
INFO:name:epoch 8 step 7800 loss 0.00591
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4515
INFO:name:epoch 9 step 100 loss 0.0068
INFO:name:epoch 9 step 200 loss 0.00847
INFO:name:epoch 9 step 300 loss 0.0088
INFO:name:epoch 9 step 400 loss 0.00551
INFO:name:epoch 9 step 500 loss 0.00686
INFO:name:epoch 9 step 600 loss 0.00734
INFO:name:epoch 9 step 700 loss 0.00651
INFO:name:epoch 9 step 800 loss 0.00665
INFO:name:epoch 9 step 900 loss 0.00642
INFO:name:epoch 9 step 1000 loss 0.00567
INFO:name:epoch 9 step 1100 loss 0.00712
INFO:name:epoch 9 step 1200 loss 0.00754
INFO:name:epoch 9 step 1300 loss 0.00589
INFO:name:epoch 9 step 1400 loss 0.00483
INFO:name:epoch 9 step 1500 loss 0.00663
INFO:name:epoch 9 step 1600 loss 0.00479
INFO:name:epoch 9 step 1700 loss 0.00676
INFO:name:epoch 9 step 1800 loss 0.0063
INFO:name:epoch 9 step 1900 loss 0.00745
INFO:name:epoch 9 step 2000 loss 0.00569
INFO:name:epoch 9 step 2100 loss 0.00736
INFO:name:epoch 9 step 2200 loss 0.00711
INFO:name:epoch 9 step 2300 loss 0.00625
INFO:name:epoch 9 step 2400 loss 0.00531
INFO:name:epoch 9 step 2500 loss 0.00671
INFO:name:epoch 9 step 2600 loss 0.00641
INFO:name:epoch 9 step 2700 loss 0.00817
INFO:name:epoch 9 step 2800 loss 0.00622
INFO:name:epoch 9 step 2900 loss 0.00652
INFO:name:epoch 9 step 3000 loss 0.00549
INFO:name:epoch 9 step 3100 loss 0.00746
INFO:name:epoch 9 step 3200 loss 0.00579
INFO:name:epoch 9 step 3300 loss 0.00747
INFO:name:epoch 9 step 3400 loss 0.00502
INFO:name:epoch 9 step 3500 loss 0.0072
INFO:name:epoch 9 step 3600 loss 0.00544
INFO:name:epoch 9 step 3700 loss 0.00614
INFO:name:epoch 9 step 3800 loss 0.00603
INFO:name:epoch 9 step 3900 loss 0.00599
INFO:name:epoch 9 step 4000 loss 0.00499
INFO:name:epoch 9 step 4100 loss 0.00643
INFO:name:epoch 9 step 4200 loss 0.00687
INFO:name:epoch 9 step 4300 loss 0.0048
INFO:name:epoch 9 step 4400 loss 0.00559
INFO:name:epoch 9 step 4500 loss 0.00668
INFO:name:epoch 9 step 4600 loss 0.00728
INFO:name:epoch 9 step 4700 loss 0.00774
INFO:name:epoch 9 step 4800 loss 0.00626
INFO:name:epoch 9 step 4900 loss 0.00571
INFO:name:epoch 9 step 5000 loss 0.00531
INFO:name:epoch 9 step 5100 loss 0.00593
INFO:name:epoch 9 step 5200 loss 0.00675
INFO:name:epoch 9 step 5300 loss 0.00742
INFO:name:epoch 9 step 5400 loss 0.00641
INFO:name:epoch 9 step 5500 loss 0.0058
INFO:name:epoch 9 step 5600 loss 0.00603
INFO:name:epoch 9 step 5700 loss 0.00875
INFO:name:epoch 9 step 5800 loss 0.00583
INFO:name:epoch 9 step 5900 loss 0.00835
INFO:name:epoch 9 step 6000 loss 0.00768
INFO:name:epoch 9 step 6100 loss 0.00629
INFO:name:epoch 9 step 6200 loss 0.00929
INFO:name:epoch 9 step 6300 loss 0.0068
INFO:name:epoch 9 step 6400 loss 0.007
INFO:name:epoch 9 step 6500 loss 0.00895
INFO:name:epoch 9 step 6600 loss 0.00434
INFO:name:epoch 9 step 6700 loss 0.00663
INFO:name:epoch 9 step 6800 loss 0.00565
INFO:name:epoch 9 step 6900 loss 0.00673
INFO:name:epoch 9 step 7000 loss 0.00576
INFO:name:epoch 9 step 7100 loss 0.00556
INFO:name:epoch 9 step 7200 loss 0.00663
INFO:name:epoch 9 step 7300 loss 0.00797
INFO:name:epoch 9 step 7400 loss 0.00673
INFO:name:epoch 9 step 7500 loss 0.00515
INFO:name:epoch 9 step 7600 loss 0.006
INFO:name:epoch 9 step 7700 loss 0.00523
INFO:name:epoch 9 step 7800 loss 0.00489
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4537
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:[{'insert_modules': ('attention.self',), 'bottleneck_dim': (32,), 'non_linearity': 'gelu_new', 'dropout_rate': 0.25, 'normalization': 'layer_norm', 'skip_connection': True}, 0, {'insert_modules': ('intermediate',), 'bottleneck_dim': (64,), 'non_linearity': 'silu', 'dropout_rate': 0.1, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('attention.self', 'attention.output', 'output'), 'bottleneck_dim': (32, 32, 128), 'non_linearity': 'tanh', 'dropout_rate': 0.3, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('output', 'attention.output', 'intermediate'), 'bottleneck_dim': (128, 32, 64), 'non_linearity': 'tanh', 'dropout_rate': 0.1, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('output',), 'bottleneck_dim': (256,), 'non_linearity': 'leakyrelu', 'dropout_rate': 0.2, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('attention.self', 'intermediate'), 'bottleneck_dim': (16, 64), 'non_linearity': 'leakyrelu', 'dropout_rate': 0.3, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('intermediate', 'attention.output', 'attention.self'), 'bottleneck_dim': (128, 64, 16), 'non_linearity': 'tanh', 'dropout_rate': 0.0, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('attention.output', 'attention.self', 'intermediate'), 'bottleneck_dim': (64, 32, 128), 'non_linearity': 'gelu_new', 'dropout_rate': 0.3, 'normalization': None, 'skip_connection': True}, 0, {'insert_modules': ('attention.self',), 'bottleneck_dim': (32,), 'non_linearity': 'gelu_new', 'dropout_rate': 0.0, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('attention.output', 'attention.self', 'intermediate'), 'bottleneck_dim': (32, 16, 64), 'non_linearity': 'swish', 'dropout_rate': 0.3, 'normalization': 'layer_norm', 'skip_connection': True}]
[INFO|(OpenDelta)basemodel:700]2025-01-24 04:07:34,162 >> Trainable Ratio: 4578672/130508400=3.508335%
[INFO|(OpenDelta)basemodel:702]2025-01-24 04:07:34,162 >> Delta Parameter Ratio: 4578672/130508400=3.508335%
[INFO|(OpenDelta)basemodel:704]2025-01-24 04:07:34,162 >> Static Memory 1.00 GB, Max Memory 9.36 GB
INFO:name:5.11
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 10
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 78700
train results ([0.08914751963085805, 0.04242198220207125, 0.029110707189716763, 0.020206177294876466, 0.014674467706115439, 0.011669100112957525, 0.00991965014653234, 0.008318893204099531, 0.007154192635338349, 0.006505326265716594], [0.4357259311520822, 0.42656729774667274, 0.45249097711197434, 0.4608844351800978, 0.45595000419689996, 0.4639956028189799, 0.46280592454600766, 0.451690555148329, 0.45153953631957344, 0.4536731727545352])
INFO:name:epoch 0 step 100 loss 0.28447
INFO:name:epoch 0 step 200 loss 0.17013
INFO:name:epoch 0 step 300 loss 0.16168
INFO:name:epoch 0 step 400 loss 0.13352
INFO:name:epoch 0 step 500 loss 0.141
INFO:name:epoch 0 step 600 loss 0.11685
INFO:name:epoch 0 step 700 loss 0.10075
INFO:name:epoch 0 step 800 loss 0.10669
INFO:name:epoch 0 step 900 loss 0.11666
INFO:name:epoch 0 step 1000 loss 0.11793
INFO:name:epoch 0 step 1100 loss 0.09355
INFO:name:epoch 0 step 1200 loss 0.11057
INFO:name:epoch 0 step 1300 loss 0.10831
INFO:name:epoch 0 step 1400 loss 0.08956
INFO:name:epoch 0 step 1500 loss 0.08229
INFO:name:epoch 0 step 1600 loss 0.10819
INFO:name:epoch 0 step 1700 loss 0.09516
INFO:name:epoch 0 step 1800 loss 0.09274
INFO:name:epoch 0 step 1900 loss 0.07752
INFO:name:epoch 0 step 2000 loss 0.10764
INFO:name:epoch 0 step 2100 loss 0.08222
INFO:name:epoch 0 step 2200 loss 0.11044
INFO:name:epoch 0 step 2300 loss 0.09824
INFO:name:epoch 0 step 2400 loss 0.08798
INFO:name:epoch 0 step 2500 loss 0.09983
INFO:name:epoch 0 step 2600 loss 0.0983
INFO:name:epoch 0 step 2700 loss 0.08883
INFO:name:epoch 0 step 2800 loss 0.08812
INFO:name:epoch 0 step 2900 loss 0.09294
INFO:name:epoch 0 step 3000 loss 0.07499
INFO:name:epoch 0 step 3100 loss 0.08913
INFO:name:epoch 0 step 3200 loss 0.0774
INFO:name:epoch 0 step 3300 loss 0.09115
INFO:name:epoch 0 step 3400 loss 0.087
INFO:name:epoch 0 step 3500 loss 0.07387
INFO:name:epoch 0 step 3600 loss 0.09929
INFO:name:epoch 0 step 3700 loss 0.06807
INFO:name:epoch 0 step 3800 loss 0.08318
INFO:name:epoch 0 step 3900 loss 0.08436
INFO:name:epoch 0 step 4000 loss 0.07764
INFO:name:epoch 0 step 4100 loss 0.07322
INFO:name:epoch 0 step 4200 loss 0.06883
INFO:name:epoch 0 step 4300 loss 0.08668
INFO:name:epoch 0 step 4400 loss 0.07626
INFO:name:epoch 0 step 4500 loss 0.0837
INFO:name:epoch 0 step 4600 loss 0.07752
INFO:name:epoch 0 step 4700 loss 0.08333
INFO:name:epoch 0 step 4800 loss 0.06485
INFO:name:epoch 0 step 4900 loss 0.08106
INFO:name:epoch 0 step 5000 loss 0.07692
INFO:name:epoch 0 step 5100 loss 0.07472
INFO:name:epoch 0 step 5200 loss 0.07746
INFO:name:epoch 0 step 5300 loss 0.08393
INFO:name:epoch 0 step 5400 loss 0.08826
INFO:name:epoch 0 step 5500 loss 0.08256
INFO:name:epoch 0 step 5600 loss 0.07382
INFO:name:epoch 0 step 5700 loss 0.06798
INFO:name:epoch 0 step 5800 loss 0.07091
INFO:name:epoch 0 step 5900 loss 0.06683
INFO:name:epoch 0 step 6000 loss 0.07537
INFO:name:epoch 0 step 6100 loss 0.06108
INFO:name:epoch 0 step 6200 loss 0.08268
INFO:name:epoch 0 step 6300 loss 0.08482
INFO:name:epoch 0 step 6400 loss 0.08597
INFO:name:epoch 0 step 6500 loss 0.08154
INFO:name:epoch 0 step 6600 loss 0.07082
INFO:name:epoch 0 step 6700 loss 0.06583
INFO:name:epoch 0 step 6800 loss 0.08551
INFO:name:epoch 0 step 6900 loss 0.07789
INFO:name:epoch 0 step 7000 loss 0.08266
INFO:name:epoch 0 step 7100 loss 0.09118
INFO:name:epoch 0 step 7200 loss 0.09135
INFO:name:epoch 0 step 7300 loss 0.07032
INFO:name:epoch 0 step 7400 loss 0.06912
INFO:name:epoch 0 step 7500 loss 0.0694
INFO:name:epoch 0 step 7600 loss 0.07061
INFO:name:epoch 0 step 7700 loss 0.07097
INFO:name:epoch 0 step 7800 loss 0.07241
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4461
INFO:name:  ********************
INFO:name:  Best eval mrr:0.4461
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3814
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 0.05074
INFO:name:epoch 1 step 200 loss 0.04082
INFO:name:epoch 1 step 300 loss 0.04136
INFO:name:epoch 1 step 400 loss 0.04729
INFO:name:epoch 1 step 500 loss 0.05299
INFO:name:epoch 1 step 600 loss 0.04386
INFO:name:epoch 1 step 700 loss 0.04075
INFO:name:epoch 1 step 800 loss 0.05185
INFO:name:epoch 1 step 900 loss 0.04131
INFO:name:epoch 1 step 1000 loss 0.03809
INFO:name:epoch 1 step 1100 loss 0.04262
INFO:name:epoch 1 step 1200 loss 0.05197
INFO:name:epoch 1 step 1300 loss 0.0424
INFO:name:epoch 1 step 1400 loss 0.04493
INFO:name:epoch 1 step 1500 loss 0.04404
INFO:name:epoch 1 step 1600 loss 0.03923
INFO:name:epoch 1 step 1700 loss 0.04488
INFO:name:epoch 1 step 1800 loss 0.04997
INFO:name:epoch 1 step 1900 loss 0.03707
INFO:name:epoch 1 step 2000 loss 0.04676
INFO:name:epoch 1 step 2100 loss 0.04802
INFO:name:epoch 1 step 2200 loss 0.04457
INFO:name:epoch 1 step 2300 loss 0.04272
INFO:name:epoch 1 step 2400 loss 0.04593
INFO:name:epoch 1 step 2500 loss 0.0527
INFO:name:epoch 1 step 2600 loss 0.0471
INFO:name:epoch 1 step 2700 loss 0.04384
INFO:name:epoch 1 step 2800 loss 0.04636
INFO:name:epoch 1 step 2900 loss 0.04452
INFO:name:epoch 1 step 3000 loss 0.04152
INFO:name:epoch 1 step 3100 loss 0.04613
INFO:name:epoch 1 step 3200 loss 0.05541
INFO:name:epoch 1 step 3300 loss 0.04476
INFO:name:epoch 1 step 3400 loss 0.04863
INFO:name:epoch 1 step 3500 loss 0.04089
INFO:name:epoch 1 step 3600 loss 0.04484
INFO:name:epoch 1 step 3700 loss 0.04916
INFO:name:epoch 1 step 3800 loss 0.04631
INFO:name:epoch 1 step 3900 loss 0.04272
INFO:name:epoch 1 step 4000 loss 0.04762
INFO:name:epoch 1 step 4100 loss 0.03278
INFO:name:epoch 1 step 4200 loss 0.04346
INFO:name:epoch 1 step 4300 loss 0.04526
INFO:name:epoch 1 step 4400 loss 0.04637
INFO:name:epoch 1 step 4500 loss 0.03967
INFO:name:epoch 1 step 4600 loss 0.03736
INFO:name:epoch 1 step 4700 loss 0.04619
INFO:name:epoch 1 step 4800 loss 0.04947
INFO:name:epoch 1 step 4900 loss 0.04381
INFO:name:epoch 1 step 5000 loss 0.03797
INFO:name:epoch 1 step 5100 loss 0.03932
INFO:name:epoch 1 step 5200 loss 0.05038
INFO:name:epoch 1 step 5300 loss 0.04241
INFO:name:epoch 1 step 5400 loss 0.03235
INFO:name:epoch 1 step 5500 loss 0.03199
INFO:name:epoch 1 step 5600 loss 0.03373
INFO:name:epoch 1 step 5700 loss 0.04048
INFO:name:epoch 1 step 5800 loss 0.05453
INFO:name:epoch 1 step 5900 loss 0.04047
INFO:name:epoch 1 step 6000 loss 0.04183
INFO:name:epoch 1 step 6100 loss 0.04051
INFO:name:epoch 1 step 6200 loss 0.03949
INFO:name:epoch 1 step 6300 loss 0.03999
INFO:name:epoch 1 step 6400 loss 0.04748
INFO:name:epoch 1 step 6500 loss 0.03556
INFO:name:epoch 1 step 6600 loss 0.04314
INFO:name:epoch 1 step 6700 loss 0.03938
INFO:name:epoch 1 step 6800 loss 0.05215
INFO:name:epoch 1 step 6900 loss 0.04651
INFO:name:epoch 1 step 7000 loss 0.04309
INFO:name:epoch 1 step 7100 loss 0.04245
INFO:name:epoch 1 step 7200 loss 0.03605
INFO:name:epoch 1 step 7300 loss 0.04308
INFO:name:epoch 1 step 7400 loss 0.04172
INFO:name:epoch 1 step 7500 loss 0.03772
INFO:name:epoch 1 step 7600 loss 0.04008
INFO:name:epoch 1 step 7700 loss 0.04661
INFO:name:epoch 1 step 7800 loss 0.0442
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4466
INFO:name:  ********************
INFO:name:  Best eval mrr:0.4466
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3801
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 2 step 100 loss 0.03823
INFO:name:epoch 2 step 200 loss 0.02953
INFO:name:epoch 2 step 300 loss 0.02862
INFO:name:epoch 2 step 400 loss 0.02265
INFO:name:epoch 2 step 500 loss 0.03364
INFO:name:epoch 2 step 600 loss 0.02365
INFO:name:epoch 2 step 700 loss 0.02937
INFO:name:epoch 2 step 800 loss 0.02886
INFO:name:epoch 2 step 900 loss 0.0329
INFO:name:epoch 2 step 1000 loss 0.01758
INFO:name:epoch 2 step 1100 loss 0.02836
INFO:name:epoch 2 step 1200 loss 0.02989
INFO:name:epoch 2 step 1300 loss 0.02668
INFO:name:epoch 2 step 1400 loss 0.03747
INFO:name:epoch 2 step 1500 loss 0.02775
INFO:name:epoch 2 step 1600 loss 0.02747
INFO:name:epoch 2 step 1700 loss 0.0241
INFO:name:epoch 2 step 1800 loss 0.03041
INFO:name:epoch 2 step 1900 loss 0.02324
INFO:name:epoch 2 step 2000 loss 0.02261
INFO:name:epoch 2 step 2100 loss 0.0307
INFO:name:epoch 2 step 2200 loss 0.0222
INFO:name:epoch 2 step 2300 loss 0.02536
INFO:name:epoch 2 step 2400 loss 0.02912
INFO:name:epoch 2 step 2500 loss 0.02862
INFO:name:epoch 2 step 2600 loss 0.02824
INFO:name:epoch 2 step 2700 loss 0.02605
INFO:name:epoch 2 step 2800 loss 0.02887
INFO:name:epoch 2 step 2900 loss 0.0263
INFO:name:epoch 2 step 3000 loss 0.04183
INFO:name:epoch 2 step 3100 loss 0.02799
INFO:name:epoch 2 step 3200 loss 0.03579
INFO:name:epoch 2 step 3300 loss 0.03056
INFO:name:epoch 2 step 3400 loss 0.02745
INFO:name:epoch 2 step 3500 loss 0.03093
INFO:name:epoch 2 step 3600 loss 0.02994
INFO:name:epoch 2 step 3700 loss 0.03333
INFO:name:epoch 2 step 3800 loss 0.03112
INFO:name:epoch 2 step 3900 loss 0.02997
INFO:name:epoch 2 step 4000 loss 0.03129
INFO:name:epoch 2 step 4100 loss 0.02757
INFO:name:epoch 2 step 4200 loss 0.02864
INFO:name:epoch 2 step 4300 loss 0.02855
INFO:name:epoch 2 step 4400 loss 0.02692
INFO:name:epoch 2 step 4500 loss 0.0431
INFO:name:epoch 2 step 4600 loss 0.03025
INFO:name:epoch 2 step 4700 loss 0.03074
INFO:name:epoch 2 step 4800 loss 0.03499
INFO:name:epoch 2 step 4900 loss 0.03534
INFO:name:epoch 2 step 5000 loss 0.02921
INFO:name:epoch 2 step 5100 loss 0.03003
INFO:name:epoch 2 step 5200 loss 0.03089
INFO:name:epoch 2 step 5300 loss 0.03269
INFO:name:epoch 2 step 5400 loss 0.0342
INFO:name:epoch 2 step 5500 loss 0.02858
INFO:name:epoch 2 step 5600 loss 0.02989
INFO:name:epoch 2 step 5700 loss 0.02697
INFO:name:epoch 2 step 5800 loss 0.03042
INFO:name:epoch 2 step 5900 loss 0.0341
INFO:name:epoch 2 step 6000 loss 0.03347
INFO:name:epoch 2 step 6100 loss 0.03185
INFO:name:epoch 2 step 6200 loss 0.04066
INFO:name:epoch 2 step 6300 loss 0.0279
INFO:name:epoch 2 step 6400 loss 0.03577
INFO:name:epoch 2 step 6500 loss 0.02234
INFO:name:epoch 2 step 6600 loss 0.02577
INFO:name:epoch 2 step 6700 loss 0.0285
INFO:name:epoch 2 step 6800 loss 0.03156
INFO:name:epoch 2 step 6900 loss 0.03453
INFO:name:epoch 2 step 7000 loss 0.03556
INFO:name:epoch 2 step 7100 loss 0.03475
INFO:name:epoch 2 step 7200 loss 0.02835
INFO:name:epoch 2 step 7300 loss 0.03771
INFO:name:epoch 2 step 7400 loss 0.03176
INFO:name:epoch 2 step 7500 loss 0.02651
INFO:name:epoch 2 step 7600 loss 0.02455
INFO:name:epoch 2 step 7700 loss 0.02906
INFO:name:epoch 2 step 7800 loss 0.0275
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4502
INFO:name:  ********************
INFO:name:  Best eval mrr:0.4502
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3828
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.02359
INFO:name:epoch 3 step 200 loss 0.01657
INFO:name:epoch 3 step 300 loss 0.01913
INFO:name:epoch 3 step 400 loss 0.0191
INFO:name:epoch 3 step 500 loss 0.01871
INFO:name:epoch 3 step 600 loss 0.02054
INFO:name:epoch 3 step 700 loss 0.02116
INFO:name:epoch 3 step 800 loss 0.01873
INFO:name:epoch 3 step 900 loss 0.01732
INFO:name:epoch 3 step 1000 loss 0.02046
INFO:name:epoch 3 step 1100 loss 0.0278
INFO:name:epoch 3 step 1200 loss 0.01775
INFO:name:epoch 3 step 1300 loss 0.01621
INFO:name:epoch 3 step 1400 loss 0.02014
INFO:name:epoch 3 step 1500 loss 0.02199
INFO:name:epoch 3 step 1600 loss 0.02425
INFO:name:epoch 3 step 1700 loss 0.01744
INFO:name:epoch 3 step 1800 loss 0.02042
INFO:name:epoch 3 step 1900 loss 0.02087
INFO:name:epoch 3 step 2000 loss 0.02117
INFO:name:epoch 3 step 2100 loss 0.01902
INFO:name:epoch 3 step 2200 loss 0.01953
INFO:name:epoch 3 step 2300 loss 0.02177
INFO:name:epoch 3 step 2400 loss 0.02453
INFO:name:epoch 3 step 2500 loss 0.01739
INFO:name:epoch 3 step 2600 loss 0.02245
INFO:name:epoch 3 step 2700 loss 0.02293
INFO:name:epoch 3 step 2800 loss 0.02021
INFO:name:epoch 3 step 2900 loss 0.02015
INFO:name:epoch 3 step 3000 loss 0.01588
INFO:name:epoch 3 step 3100 loss 0.0223
INFO:name:epoch 3 step 3200 loss 0.01789
INFO:name:epoch 3 step 3300 loss 0.02119
INFO:name:epoch 3 step 3400 loss 0.02236
INFO:name:epoch 3 step 3500 loss 0.02281
INFO:name:epoch 3 step 3600 loss 0.01802
INFO:name:epoch 3 step 3700 loss 0.01841
INFO:name:epoch 3 step 3800 loss 0.02459
INFO:name:epoch 3 step 3900 loss 0.02307
INFO:name:epoch 3 step 4000 loss 0.02087
INFO:name:epoch 3 step 4100 loss 0.02046
INFO:name:epoch 3 step 4200 loss 0.01849
INFO:name:epoch 3 step 4300 loss 0.01555
INFO:name:epoch 3 step 4400 loss 0.01828
INFO:name:epoch 3 step 4500 loss 0.02001
INFO:name:epoch 3 step 4600 loss 0.02074
INFO:name:epoch 3 step 4700 loss 0.02267
INFO:name:epoch 3 step 4800 loss 0.02278
INFO:name:epoch 3 step 4900 loss 0.02339
INFO:name:epoch 3 step 5000 loss 0.02352
INFO:name:epoch 3 step 5100 loss 0.0265
INFO:name:epoch 3 step 5200 loss 0.02292
INFO:name:epoch 3 step 5300 loss 0.0188
INFO:name:epoch 3 step 5400 loss 0.02234
INFO:name:epoch 3 step 5500 loss 0.02157
INFO:name:epoch 3 step 5600 loss 0.02603
INFO:name:epoch 3 step 5700 loss 0.02611
INFO:name:epoch 3 step 5800 loss 0.01992
INFO:name:epoch 3 step 5900 loss 0.02161
INFO:name:epoch 3 step 6000 loss 0.02618
INFO:name:epoch 3 step 6100 loss 0.02098
INFO:name:epoch 3 step 6200 loss 0.02379
INFO:name:epoch 3 step 6300 loss 0.02204
INFO:name:epoch 3 step 6400 loss 0.02348
INFO:name:epoch 3 step 6500 loss 0.02378
INFO:name:epoch 3 step 6600 loss 0.02294
INFO:name:epoch 3 step 6700 loss 0.02177
INFO:name:epoch 3 step 6800 loss 0.02043
INFO:name:epoch 3 step 6900 loss 0.01966
INFO:name:epoch 3 step 7000 loss 0.0262
INFO:name:epoch 3 step 7100 loss 0.0225
INFO:name:epoch 3 step 7200 loss 0.0223
INFO:name:epoch 3 step 7300 loss 0.01944
INFO:name:epoch 3 step 7400 loss 0.0204
INFO:name:epoch 3 step 7500 loss 0.02102
INFO:name:epoch 3 step 7600 loss 0.02059
INFO:name:epoch 3 step 7700 loss 0.01995
INFO:name:epoch 3 step 7800 loss 0.02108
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.424
INFO:name:epoch 4 step 100 loss 0.01941
INFO:name:epoch 4 step 200 loss 0.01584
INFO:name:epoch 4 step 300 loss 0.0134
INFO:name:epoch 4 step 400 loss 0.01141
INFO:name:epoch 4 step 500 loss 0.01636
INFO:name:epoch 4 step 600 loss 0.01694
INFO:name:epoch 4 step 700 loss 0.01325
INFO:name:epoch 4 step 800 loss 0.01308
INFO:name:epoch 4 step 900 loss 0.01641
INFO:name:epoch 4 step 1000 loss 0.01298
INFO:name:epoch 4 step 1100 loss 0.01167
INFO:name:epoch 4 step 1200 loss 0.01582
INFO:name:epoch 4 step 1300 loss 0.01491
INFO:name:epoch 4 step 1400 loss 0.01342
INFO:name:epoch 4 step 1500 loss 0.01865
INFO:name:epoch 4 step 1600 loss 0.01927
INFO:name:epoch 4 step 1700 loss 0.01451
INFO:name:epoch 4 step 1800 loss 0.01221
INFO:name:epoch 4 step 1900 loss 0.01591
INFO:name:epoch 4 step 2000 loss 0.01604
INFO:name:epoch 4 step 2100 loss 0.01604
INFO:name:epoch 4 step 2200 loss 0.0174
INFO:name:epoch 4 step 2300 loss 0.01616
INFO:name:epoch 4 step 2400 loss 0.01215
INFO:name:epoch 4 step 2500 loss 0.01571
INFO:name:epoch 4 step 2600 loss 0.01444
INFO:name:epoch 4 step 2700 loss 0.01487
INFO:name:epoch 4 step 2800 loss 0.01336
INFO:name:epoch 4 step 2900 loss 0.01663
INFO:name:epoch 4 step 3000 loss 0.01351
INFO:name:epoch 4 step 3100 loss 0.01416
INFO:name:epoch 4 step 3200 loss 0.01461
INFO:name:epoch 4 step 3300 loss 0.01278
INFO:name:epoch 4 step 3400 loss 0.01393
INFO:name:epoch 4 step 3500 loss 0.01551
INFO:name:epoch 4 step 3600 loss 0.0158
INFO:name:epoch 4 step 3700 loss 0.01516
INFO:name:epoch 4 step 3800 loss 0.01893
INFO:name:epoch 4 step 3900 loss 0.01376
INFO:name:epoch 4 step 4000 loss 0.01565
INFO:name:epoch 4 step 4100 loss 0.01924
INFO:name:epoch 4 step 4200 loss 0.01519
INFO:name:epoch 4 step 4300 loss 0.01515
INFO:name:epoch 4 step 4400 loss 0.01475
INFO:name:epoch 4 step 4500 loss 0.0148
INFO:name:epoch 4 step 4600 loss 0.01607
INFO:name:epoch 4 step 4700 loss 0.01753
INFO:name:epoch 4 step 4800 loss 0.01552
INFO:name:epoch 4 step 4900 loss 0.0139
INFO:name:epoch 4 step 5000 loss 0.01573
INFO:name:epoch 4 step 5100 loss 0.01315
INFO:name:epoch 4 step 5200 loss 0.01435
INFO:name:epoch 4 step 5300 loss 0.01485
INFO:name:epoch 4 step 5400 loss 0.01549
INFO:name:epoch 4 step 5500 loss 0.01452
INFO:name:epoch 4 step 5600 loss 0.01641
INFO:name:epoch 4 step 5700 loss 0.01647
INFO:name:epoch 4 step 5800 loss 0.01323
INFO:name:epoch 4 step 5900 loss 0.02035
INFO:name:epoch 4 step 6000 loss 0.01399
INFO:name:epoch 4 step 6100 loss 0.01245
INFO:name:epoch 4 step 6200 loss 0.01641
INFO:name:epoch 4 step 6300 loss 0.01407
INFO:name:epoch 4 step 6400 loss 0.01256
INFO:name:epoch 4 step 6500 loss 0.01581
INFO:name:epoch 4 step 6600 loss 0.01889
INFO:name:epoch 4 step 6700 loss 0.01692
INFO:name:epoch 4 step 6800 loss 0.0172
INFO:name:epoch 4 step 6900 loss 0.01376
INFO:name:epoch 4 step 7000 loss 0.01398
INFO:name:epoch 4 step 7100 loss 0.01731
INFO:name:epoch 4 step 7200 loss 0.01784
INFO:name:epoch 4 step 7300 loss 0.01565
INFO:name:epoch 4 step 7400 loss 0.01724
INFO:name:epoch 4 step 7500 loss 0.017
INFO:name:epoch 4 step 7600 loss 0.01643
INFO:name:epoch 4 step 7700 loss 0.01931
INFO:name:epoch 4 step 7800 loss 0.01518
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4417
INFO:name:epoch 5 step 100 loss 0.01263
INFO:name:epoch 5 step 200 loss 0.01216
INFO:name:epoch 5 step 300 loss 0.00999
INFO:name:epoch 5 step 400 loss 0.00908
INFO:name:epoch 5 step 500 loss 0.01131
INFO:name:epoch 5 step 600 loss 0.0133
INFO:name:epoch 5 step 700 loss 0.01021
INFO:name:epoch 5 step 800 loss 0.00962
INFO:name:epoch 5 step 900 loss 0.01229
INFO:name:epoch 5 step 1000 loss 0.01172
INFO:name:epoch 5 step 1100 loss 0.01075
INFO:name:epoch 5 step 1200 loss 0.00958
INFO:name:epoch 5 step 1300 loss 0.01297
INFO:name:epoch 5 step 1400 loss 0.0128
INFO:name:epoch 5 step 1500 loss 0.01607
INFO:name:epoch 5 step 1600 loss 0.01031
INFO:name:epoch 5 step 1700 loss 0.01014
INFO:name:epoch 5 step 1800 loss 0.01101
INFO:name:epoch 5 step 1900 loss 0.01239
INFO:name:epoch 5 step 2000 loss 0.01025
INFO:name:epoch 5 step 2100 loss 0.00851
INFO:name:epoch 5 step 2200 loss 0.01091
INFO:name:epoch 5 step 2300 loss 0.01116
INFO:name:epoch 5 step 2400 loss 0.00965
INFO:name:epoch 5 step 2500 loss 0.01327
INFO:name:epoch 5 step 2600 loss 0.01423
INFO:name:epoch 5 step 2700 loss 0.01236
INFO:name:epoch 5 step 2800 loss 0.01381
INFO:name:epoch 5 step 2900 loss 0.01407
INFO:name:epoch 5 step 3000 loss 0.01241
INFO:name:epoch 5 step 3100 loss 0.01369
INFO:name:epoch 5 step 3200 loss 0.012
INFO:name:epoch 5 step 3300 loss 0.01131
INFO:name:epoch 5 step 3400 loss 0.01219
INFO:name:epoch 5 step 3500 loss 0.01558
INFO:name:epoch 5 step 3600 loss 0.01237
INFO:name:epoch 5 step 3700 loss 0.00887
INFO:name:epoch 5 step 3800 loss 0.01088
INFO:name:epoch 5 step 3900 loss 0.01209
INFO:name:epoch 5 step 4000 loss 0.0085
INFO:name:epoch 5 step 4100 loss 0.0092
INFO:name:epoch 5 step 4200 loss 0.01187
INFO:name:epoch 5 step 4300 loss 0.01254
INFO:name:epoch 5 step 4400 loss 0.01212
INFO:name:epoch 5 step 4500 loss 0.01309
INFO:name:epoch 5 step 4600 loss 0.01131
INFO:name:epoch 5 step 4700 loss 0.01173
INFO:name:epoch 5 step 4800 loss 0.01163
INFO:name:epoch 5 step 4900 loss 0.01259
INFO:name:epoch 5 step 5000 loss 0.01095
INFO:name:epoch 5 step 5100 loss 0.01468
INFO:name:epoch 5 step 5200 loss 0.01386
INFO:name:epoch 5 step 5300 loss 0.01083
INFO:name:epoch 5 step 5400 loss 0.00958
INFO:name:epoch 5 step 5500 loss 0.01264
INFO:name:epoch 5 step 5600 loss 0.0151
INFO:name:epoch 5 step 5700 loss 0.00913
INFO:name:epoch 5 step 5800 loss 0.01478
INFO:name:epoch 5 step 5900 loss 0.01115
INFO:name:epoch 5 step 6000 loss 0.01305
INFO:name:epoch 5 step 6100 loss 0.01676
INFO:name:epoch 5 step 6200 loss 0.01301
INFO:name:epoch 5 step 6300 loss 0.01419
INFO:name:epoch 5 step 6400 loss 0.01218
INFO:name:epoch 5 step 6500 loss 0.01088
INFO:name:epoch 5 step 6600 loss 0.01168
INFO:name:epoch 5 step 6700 loss 0.0136
INFO:name:epoch 5 step 6800 loss 0.01018
INFO:name:epoch 5 step 6900 loss 0.01117
INFO:name:epoch 5 step 7000 loss 0.01354
INFO:name:epoch 5 step 7100 loss 0.01433
INFO:name:epoch 5 step 7200 loss 0.01404
INFO:name:epoch 5 step 7300 loss 0.01231
INFO:name:epoch 5 step 7400 loss 0.01307
INFO:name:epoch 5 step 7500 loss 0.01206
INFO:name:epoch 5 step 7600 loss 0.01167
INFO:name:epoch 5 step 7700 loss 0.01424
INFO:name:epoch 5 step 7800 loss 0.01273
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4358
INFO:name:epoch 6 step 100 loss 0.01003
INFO:name:epoch 6 step 200 loss 0.0096
INFO:name:epoch 6 step 300 loss 0.00999
INFO:name:epoch 6 step 400 loss 0.00893
INFO:name:epoch 6 step 500 loss 0.00858
INFO:name:epoch 6 step 600 loss 0.00729
INFO:name:epoch 6 step 700 loss 0.01223
INFO:name:epoch 6 step 800 loss 0.0068
INFO:name:epoch 6 step 900 loss 0.00875
INFO:name:epoch 6 step 1000 loss 0.00704
INFO:name:epoch 6 step 1100 loss 0.0089
INFO:name:epoch 6 step 1200 loss 0.01075
INFO:name:epoch 6 step 1300 loss 0.01115
INFO:name:epoch 6 step 1400 loss 0.00939
INFO:name:epoch 6 step 1500 loss 0.00788
INFO:name:epoch 6 step 1600 loss 0.01017
INFO:name:epoch 6 step 1700 loss 0.00906
INFO:name:epoch 6 step 1800 loss 0.01094
INFO:name:epoch 6 step 1900 loss 0.00982
INFO:name:epoch 6 step 2000 loss 0.00851
INFO:name:epoch 6 step 2100 loss 0.00893
INFO:name:epoch 6 step 2200 loss 0.00758
INFO:name:epoch 6 step 2300 loss 0.00962
INFO:name:epoch 6 step 2400 loss 0.00882
INFO:name:epoch 6 step 2500 loss 0.01061
INFO:name:epoch 6 step 2600 loss 0.00881
INFO:name:epoch 6 step 2700 loss 0.01083
INFO:name:epoch 6 step 2800 loss 0.00978
INFO:name:epoch 6 step 2900 loss 0.01107
INFO:name:epoch 6 step 3000 loss 0.00914
INFO:name:epoch 6 step 3100 loss 0.01039
INFO:name:epoch 6 step 3200 loss 0.00767
INFO:name:epoch 6 step 3300 loss 0.00865
INFO:name:epoch 6 step 3400 loss 0.00961
INFO:name:epoch 6 step 3500 loss 0.0091
INFO:name:epoch 6 step 3600 loss 0.00956
INFO:name:epoch 6 step 3700 loss 0.01157
INFO:name:epoch 6 step 3800 loss 0.00947
INFO:name:epoch 6 step 3900 loss 0.00704
INFO:name:epoch 6 step 4000 loss 0.00818
INFO:name:epoch 6 step 4100 loss 0.01118
INFO:name:epoch 6 step 4200 loss 0.00932
INFO:name:epoch 6 step 4300 loss 0.00975
INFO:name:epoch 6 step 4400 loss 0.01016
INFO:name:epoch 6 step 4500 loss 0.01031
INFO:name:epoch 6 step 4600 loss 0.00882
INFO:name:epoch 6 step 4700 loss 0.00892
INFO:name:epoch 6 step 4800 loss 0.01252
INFO:name:epoch 6 step 4900 loss 0.00878
INFO:name:epoch 6 step 5000 loss 0.00794
INFO:name:epoch 6 step 5100 loss 0.00781
INFO:name:epoch 6 step 5200 loss 0.01169
INFO:name:epoch 6 step 5300 loss 0.01087
INFO:name:epoch 6 step 5400 loss 0.00936
INFO:name:epoch 6 step 5500 loss 0.00936
INFO:name:epoch 6 step 5600 loss 0.00946
INFO:name:epoch 6 step 5700 loss 0.01064
INFO:name:epoch 6 step 5800 loss 0.00993
INFO:name:epoch 6 step 5900 loss 0.01005
INFO:name:epoch 6 step 6000 loss 0.00868
INFO:name:epoch 6 step 6100 loss 0.00905
INFO:name:epoch 6 step 6200 loss 0.01081
INFO:name:epoch 6 step 6300 loss 0.00778
INFO:name:epoch 6 step 6400 loss 0.00827
INFO:name:epoch 6 step 6500 loss 0.00855
INFO:name:epoch 6 step 6600 loss 0.01064
INFO:name:epoch 6 step 6700 loss 0.01309
INFO:name:epoch 6 step 6800 loss 0.01179
INFO:name:epoch 6 step 6900 loss 0.01092
INFO:name:epoch 6 step 7000 loss 0.01198
INFO:name:epoch 6 step 7100 loss 0.0098
INFO:name:epoch 6 step 7200 loss 0.0118
INFO:name:epoch 6 step 7300 loss 0.00705
INFO:name:epoch 6 step 7400 loss 0.01003
INFO:name:epoch 6 step 7500 loss 0.00939
INFO:name:epoch 6 step 7600 loss 0.01026
INFO:name:epoch 6 step 7700 loss 0.01141
INFO:name:epoch 6 step 7800 loss 0.00924
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4281
INFO:name:epoch 7 step 100 loss 0.00788
INFO:name:epoch 7 step 200 loss 0.00945
INFO:name:epoch 7 step 300 loss 0.0079
INFO:name:epoch 7 step 400 loss 0.00976
INFO:name:epoch 7 step 500 loss 0.00832
INFO:name:epoch 7 step 600 loss 0.00885
INFO:name:epoch 7 step 700 loss 0.0101
INFO:name:epoch 7 step 800 loss 0.00758
INFO:name:epoch 7 step 900 loss 0.00875
INFO:name:epoch 7 step 1000 loss 0.00697
INFO:name:epoch 7 step 1100 loss 0.01163
INFO:name:epoch 7 step 1200 loss 0.00736
INFO:name:epoch 7 step 1300 loss 0.00792
INFO:name:epoch 7 step 1400 loss 0.00777
INFO:name:epoch 7 step 1500 loss 0.01178
INFO:name:epoch 7 step 1600 loss 0.00777
INFO:name:epoch 7 step 1700 loss 0.00737
INFO:name:epoch 7 step 1800 loss 0.00803
INFO:name:epoch 7 step 1900 loss 0.00882
INFO:name:epoch 7 step 2000 loss 0.00586
INFO:name:epoch 7 step 2100 loss 0.01066
INFO:name:epoch 7 step 2200 loss 0.00588
INFO:name:epoch 7 step 2300 loss 0.00791
INFO:name:epoch 7 step 2400 loss 0.00787
INFO:name:epoch 7 step 2500 loss 0.00643
INFO:name:epoch 7 step 2600 loss 0.00824
INFO:name:epoch 7 step 2700 loss 0.00713
INFO:name:epoch 7 step 2800 loss 0.00742
INFO:name:epoch 7 step 2900 loss 0.00998
INFO:name:epoch 7 step 3000 loss 0.00776
INFO:name:epoch 7 step 3100 loss 0.00672
INFO:name:epoch 7 step 3200 loss 0.00835
INFO:name:epoch 7 step 3300 loss 0.00713
INFO:name:epoch 7 step 3400 loss 0.00765
INFO:name:epoch 7 step 3500 loss 0.00762
INFO:name:epoch 7 step 3600 loss 0.00983
INFO:name:epoch 7 step 3700 loss 0.01067
INFO:name:epoch 7 step 3800 loss 0.00905
INFO:name:epoch 7 step 3900 loss 0.00917
INFO:name:epoch 7 step 4000 loss 0.00831
INFO:name:epoch 7 step 4100 loss 0.00667
INFO:name:epoch 7 step 4200 loss 0.00851
INFO:name:epoch 7 step 4300 loss 0.00906
INFO:name:epoch 7 step 4400 loss 0.00728
INFO:name:epoch 7 step 4500 loss 0.00804
INFO:name:epoch 7 step 4600 loss 0.00803
INFO:name:epoch 7 step 4700 loss 0.00744
INFO:name:epoch 7 step 4800 loss 0.00767
INFO:name:epoch 7 step 4900 loss 0.00875
INFO:name:epoch 7 step 5000 loss 0.00834
INFO:name:epoch 7 step 5100 loss 0.0084
INFO:name:epoch 7 step 5200 loss 0.0069
INFO:name:epoch 7 step 5300 loss 0.00675
INFO:name:epoch 7 step 5400 loss 0.00982
INFO:name:epoch 7 step 5500 loss 0.00994
INFO:name:epoch 7 step 5600 loss 0.0101
INFO:name:epoch 7 step 5700 loss 0.00751
INFO:name:epoch 7 step 5800 loss 0.00901
INFO:name:epoch 7 step 5900 loss 0.00929
INFO:name:epoch 7 step 6000 loss 0.00854
INFO:name:epoch 7 step 6100 loss 0.00808
INFO:name:epoch 7 step 6200 loss 0.00729
INFO:name:epoch 7 step 6300 loss 0.01212
INFO:name:epoch 7 step 6400 loss 0.00726
INFO:name:epoch 7 step 6500 loss 0.00703
INFO:name:epoch 7 step 6600 loss 0.00789
INFO:name:epoch 7 step 6700 loss 0.01075
INFO:name:epoch 7 step 6800 loss 0.0091
INFO:name:epoch 7 step 6900 loss 0.00941
INFO:name:epoch 7 step 7000 loss 0.00952
INFO:name:epoch 7 step 7100 loss 0.00682
INFO:name:epoch 7 step 7200 loss 0.00682
INFO:name:epoch 7 step 7300 loss 0.00755
INFO:name:epoch 7 step 7400 loss 0.01039
INFO:name:epoch 7 step 7500 loss 0.00953
INFO:name:epoch 7 step 7600 loss 0.00652
INFO:name:epoch 7 step 7700 loss 0.00888
INFO:name:epoch 7 step 7800 loss 0.00914
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4352
INFO:name:epoch 8 step 100 loss 0.00906
INFO:name:epoch 8 step 200 loss 0.00816
INFO:name:epoch 8 step 300 loss 0.00714
INFO:name:epoch 8 step 400 loss 0.00643
INFO:name:epoch 8 step 500 loss 0.00795
INFO:name:epoch 8 step 600 loss 0.00631
INFO:name:epoch 8 step 700 loss 0.00529
INFO:name:epoch 8 step 800 loss 0.00856
INFO:name:epoch 8 step 900 loss 0.00627
INFO:name:epoch 8 step 1000 loss 0.00802
INFO:name:epoch 8 step 1100 loss 0.00624
INFO:name:epoch 8 step 1200 loss 0.00737
INFO:name:epoch 8 step 1300 loss 0.00756
INFO:name:epoch 8 step 1400 loss 0.0085
INFO:name:epoch 8 step 1500 loss 0.00853
INFO:name:epoch 8 step 1600 loss 0.00725
INFO:name:epoch 8 step 1700 loss 0.00719
INFO:name:epoch 8 step 1800 loss 0.00665
INFO:name:epoch 8 step 1900 loss 0.0059
INFO:name:epoch 8 step 2000 loss 0.0056
INFO:name:epoch 8 step 2100 loss 0.00612
INFO:name:epoch 8 step 2200 loss 0.00938
INFO:name:epoch 8 step 2300 loss 0.00646
INFO:name:epoch 8 step 2400 loss 0.00745
INFO:name:epoch 8 step 2500 loss 0.00653
INFO:name:epoch 8 step 2600 loss 0.00654
INFO:name:epoch 8 step 2700 loss 0.00627
INFO:name:epoch 8 step 2800 loss 0.0071
INFO:name:epoch 8 step 2900 loss 0.00739
INFO:name:epoch 8 step 3000 loss 0.00766
INFO:name:epoch 8 step 3100 loss 0.00666
INFO:name:epoch 8 step 3200 loss 0.00768
INFO:name:epoch 8 step 3300 loss 0.0058
INFO:name:epoch 8 step 3400 loss 0.00753
INFO:name:epoch 8 step 3500 loss 0.00688
INFO:name:epoch 8 step 3600 loss 0.00567
INFO:name:epoch 8 step 3700 loss 0.00731
INFO:name:epoch 8 step 3800 loss 0.00651
INFO:name:epoch 8 step 3900 loss 0.00669
INFO:name:epoch 8 step 4000 loss 0.00964
INFO:name:epoch 8 step 4100 loss 0.00764
INFO:name:epoch 8 step 4200 loss 0.01132
INFO:name:epoch 8 step 4300 loss 0.00495
INFO:name:epoch 8 step 4400 loss 0.00657
INFO:name:epoch 8 step 4500 loss 0.0074
INFO:name:epoch 8 step 4600 loss 0.00816
INFO:name:epoch 8 step 4700 loss 0.00682
INFO:name:epoch 8 step 4800 loss 0.00771
INFO:name:epoch 8 step 4900 loss 0.00803
INFO:name:epoch 8 step 5000 loss 0.00939
INFO:name:epoch 8 step 5100 loss 0.0057
INFO:name:epoch 8 step 5200 loss 0.00626
INFO:name:epoch 8 step 5300 loss 0.00701
INFO:name:epoch 8 step 5400 loss 0.00641
INFO:name:epoch 8 step 5500 loss 0.00724
INFO:name:epoch 8 step 5600 loss 0.00659
INFO:name:epoch 8 step 5700 loss 0.0069
INFO:name:epoch 8 step 5800 loss 0.00587
INFO:name:epoch 8 step 5900 loss 0.00604
INFO:name:epoch 8 step 6000 loss 0.00709
INFO:name:epoch 8 step 6100 loss 0.00911
INFO:name:epoch 8 step 6200 loss 0.00636
INFO:name:epoch 8 step 6300 loss 0.00616
INFO:name:epoch 8 step 6400 loss 0.00741
INFO:name:epoch 8 step 6500 loss 0.00734
INFO:name:epoch 8 step 6600 loss 0.00735
INFO:name:epoch 8 step 6700 loss 0.0079
INFO:name:epoch 8 step 6800 loss 0.00586
INFO:name:epoch 8 step 6900 loss 0.00676
INFO:name:epoch 8 step 7000 loss 0.00604
INFO:name:epoch 8 step 7100 loss 0.00659
INFO:name:epoch 8 step 7200 loss 0.00707
INFO:name:epoch 8 step 7300 loss 0.00667
INFO:name:epoch 8 step 7400 loss 0.0066
INFO:name:epoch 8 step 7500 loss 0.00595
INFO:name:epoch 8 step 7600 loss 0.00686
INFO:name:epoch 8 step 7700 loss 0.00591
INFO:name:epoch 8 step 7800 loss 0.00667
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4416
INFO:name:epoch 9 step 100 loss 0.0075
INFO:name:epoch 9 step 200 loss 0.00522
INFO:name:epoch 9 step 300 loss 0.00893
INFO:name:epoch 9 step 400 loss 0.00452
INFO:name:epoch 9 step 500 loss 0.00738
INFO:name:epoch 9 step 600 loss 0.00861
INFO:name:epoch 9 step 700 loss 0.00722
INFO:name:epoch 9 step 800 loss 0.00606
INFO:name:epoch 9 step 900 loss 0.00551
INFO:name:epoch 9 step 1000 loss 0.00462
INFO:name:epoch 9 step 1100 loss 0.00798
INFO:name:epoch 9 step 1200 loss 0.00659
INFO:name:epoch 9 step 1300 loss 0.00549
INFO:name:epoch 9 step 1400 loss 0.0059
INFO:name:epoch 9 step 1500 loss 0.0072
INFO:name:epoch 9 step 1600 loss 0.00528
INFO:name:epoch 9 step 1700 loss 0.0059
INFO:name:epoch 9 step 1800 loss 0.00631
INFO:name:epoch 9 step 1900 loss 0.005
INFO:name:epoch 9 step 2000 loss 0.00596
INFO:name:epoch 9 step 2100 loss 0.00647
INFO:name:epoch 9 step 2200 loss 0.0084
INFO:name:epoch 9 step 2300 loss 0.00586
INFO:name:epoch 9 step 2400 loss 0.00712
INFO:name:epoch 9 step 2500 loss 0.00687
INFO:name:epoch 9 step 2600 loss 0.00436
INFO:name:epoch 9 step 2700 loss 0.00643
INFO:name:epoch 9 step 2800 loss 0.00642
INFO:name:epoch 9 step 2900 loss 0.00612
INFO:name:epoch 9 step 3000 loss 0.00655
INFO:name:epoch 9 step 3100 loss 0.00737
INFO:name:epoch 9 step 3200 loss 0.00642
INFO:name:epoch 9 step 3300 loss 0.00624
INFO:name:epoch 9 step 3400 loss 0.00608
INFO:name:epoch 9 step 3500 loss 0.00768
INFO:name:epoch 9 step 3600 loss 0.00746
INFO:name:epoch 9 step 3700 loss 0.0075
INFO:name:epoch 9 step 3800 loss 0.00627
INFO:name:epoch 9 step 3900 loss 0.00566
INFO:name:epoch 9 step 4000 loss 0.00409
INFO:name:epoch 9 step 4100 loss 0.0047
INFO:name:epoch 9 step 4200 loss 0.00649
INFO:name:epoch 9 step 4300 loss 0.00723
INFO:name:epoch 9 step 4400 loss 0.00594
INFO:name:epoch 9 step 4500 loss 0.00613
INFO:name:epoch 9 step 4600 loss 0.00493
INFO:name:epoch 9 step 4700 loss 0.00765
INFO:name:epoch 9 step 4800 loss 0.00677
INFO:name:epoch 9 step 4900 loss 0.00672
INFO:name:epoch 9 step 5000 loss 0.0038
INFO:name:epoch 9 step 5100 loss 0.00579
INFO:name:epoch 9 step 5200 loss 0.00699
INFO:name:epoch 9 step 5300 loss 0.00531
INFO:name:epoch 9 step 5400 loss 0.00652
INFO:name:epoch 9 step 5500 loss 0.00748
INFO:name:epoch 9 step 5600 loss 0.00869
INFO:name:epoch 9 step 5700 loss 0.00655
INFO:name:epoch 9 step 5800 loss 0.007
INFO:name:epoch 9 step 5900 loss 0.00813
INFO:name:epoch 9 step 6000 loss 0.00596
INFO:name:epoch 9 step 6100 loss 0.00471
INFO:name:epoch 9 step 6200 loss 0.00576
INFO:name:epoch 9 step 6300 loss 0.00724
INFO:name:epoch 9 step 6400 loss 0.00648
INFO:name:epoch 9 step 6500 loss 0.00836
INFO:name:epoch 9 step 6600 loss 0.00652
INFO:name:epoch 9 step 6700 loss 0.00717
INFO:name:epoch 9 step 6800 loss 0.00546
INFO:name:epoch 9 step 6900 loss 0.00545
INFO:name:epoch 9 step 7000 loss 0.00628
INFO:name:epoch 9 step 7100 loss 0.00763
INFO:name:epoch 9 step 7200 loss 0.00447
INFO:name:epoch 9 step 7300 loss 0.00709
INFO:name:epoch 9 step 7400 loss 0.00605
INFO:name:epoch 9 step 7500 loss 0.00756
INFO:name:epoch 9 step 7600 loss 0.00754
INFO:name:epoch 9 step 7700 loss 0.00768
INFO:name:epoch 9 step 7800 loss 0.00771
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.4331
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
train results ([0.09050348558657823, 0.04371179768882498, 0.029904753778545404, 0.02115477591292522, 0.015304990454668327, 0.012002174840653756, 0.009612144172907419, 0.008366482153262267, 0.0070716279809899555, 0.006456993481206362], [0.4460726395646832, 0.4466102684417106, 0.45015590489257595, 0.42397961082713287, 0.4417081369470409, 0.4357986613064422, 0.4280746819796564, 0.43515282757049323, 0.4415924504288661, 0.4330780499916031])
