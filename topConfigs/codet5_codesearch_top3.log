/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
INFO:name:device: cuda:0, n_gpu: 1
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5-base/resolve/main/config.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5-base/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

INFO:name:[{'insert_modules': ('layer.1.DenseReluDense',), 'bottleneck_dim': (64,), 'non_linearity': 'silu', 'dropout_rate': 0.25, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('layer.0.SelfAttention', 'layer.1'), 'bottleneck_dim': (16, 128), 'non_linearity': 'swish', 'dropout_rate': 0.0, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('layer.0',), 'bottleneck_dim': (64,), 'non_linearity': 'relu', 'dropout_rate': 0.1, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('layer.0', 'layer.1', 'layer.1.DenseReluDense'), 'bottleneck_dim': (128, 256, 128), 'non_linearity': 'relu', 'dropout_rate': 0.25, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('layer.1',), 'bottleneck_dim': (128,), 'non_linearity': 'tanh', 'dropout_rate': 0.0, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('layer.1.DenseReluDense', 'layer.0.SelfAttention', 'layer.0'), 'bottleneck_dim': (64, 16, 64), 'non_linearity': 'silu', 'dropout_rate': 0.15, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('layer.0', 'layer.0.SelfAttention', 'layer.1'), 'bottleneck_dim': (128, 16, 256), 'non_linearity': 'gelu_new', 'dropout_rate': 0.2, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('layer.1.DenseReluDense',), 'bottleneck_dim': (128,), 'non_linearity': 'silu', 'dropout_rate': 0.2, 'normalization': None, 'skip_connection': True}, {'insert_modules': ('layer.0.SelfAttention', 'layer.1'), 'bottleneck_dim': (32, 256), 'non_linearity': 'gelu', 'dropout_rate': 0.2, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('layer.0.SelfAttention', 'layer.0'), 'bottleneck_dim': (32, 128), 'non_linearity': 'relu', 'dropout_rate': 0.2, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('layer.0', 'layer.0.SelfAttention'), 'bottleneck_dim': (128, 16), 'non_linearity': 'gelu', 'dropout_rate': 0.1, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('layer.1', 'layer.0.SelfAttention'), 'bottleneck_dim': (256, 16), 'non_linearity': 'tanh', 'dropout_rate': 0.1, 'normalization': None, 'skip_connection': True}]
[INFO|(OpenDelta)basemodel:700]2025-01-12 18:21:41,140 >> Trainable Ratio: 3782608/226664656=1.668812%
[INFO|(OpenDelta)basemodel:702]2025-01-12 18:21:41,140 >> Delta Parameter Ratio: 3782608/226664656=1.668812%
[INFO|(OpenDelta)basemodel:704]2025-01-12 18:21:41,140 >> Static Memory 0.00 GB, Max Memory 0.00 GB
INFO:name:3.34
/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 10
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 78700
INFO:name:epoch 0 step 100 loss 2.35095
INFO:name:epoch 0 step 200 loss 0.58755
INFO:name:epoch 0 step 300 loss 0.31885
INFO:name:epoch 0 step 400 loss 0.26456
INFO:name:epoch 0 step 500 loss 0.25369
INFO:name:epoch 0 step 600 loss 0.21152
INFO:name:epoch 0 step 700 loss 0.21662
INFO:name:epoch 0 step 800 loss 0.1863
INFO:name:epoch 0 step 900 loss 0.19665
INFO:name:epoch 0 step 1000 loss 0.18515
INFO:name:epoch 0 step 1100 loss 0.18141
INFO:name:epoch 0 step 1200 loss 0.17377
INFO:name:epoch 0 step 1300 loss 0.17472
INFO:name:epoch 0 step 1400 loss 0.15676
INFO:name:epoch 0 step 1500 loss 0.16513
INFO:name:epoch 0 step 1600 loss 0.16849
INFO:name:epoch 0 step 1700 loss 0.13525
INFO:name:epoch 0 step 1800 loss 0.17417
INFO:name:epoch 0 step 1900 loss 0.15973
INFO:name:epoch 0 step 2000 loss 0.17863
INFO:name:epoch 0 step 2100 loss 0.11748
INFO:name:epoch 0 step 2200 loss 0.13904
INFO:name:epoch 0 step 2300 loss 0.15539
INFO:name:epoch 0 step 2400 loss 0.13758
INFO:name:epoch 0 step 2500 loss 0.14218
INFO:name:epoch 0 step 2600 loss 0.14296
INFO:name:epoch 0 step 2700 loss 0.14277
INFO:name:epoch 0 step 2800 loss 0.13567
INFO:name:epoch 0 step 2900 loss 0.14318
INFO:name:epoch 0 step 3000 loss 0.14269
INFO:name:epoch 0 step 3100 loss 0.13401
INFO:name:epoch 0 step 3200 loss 0.14424
INFO:name:epoch 0 step 3300 loss 0.13935
INFO:name:epoch 0 step 3400 loss 0.12864
INFO:name:epoch 0 step 3500 loss 0.12575
INFO:name:epoch 0 step 3600 loss 0.12565
INFO:name:epoch 0 step 3700 loss 0.12824
INFO:name:epoch 0 step 3800 loss 0.13128
INFO:name:epoch 0 step 3900 loss 0.14349
INFO:name:epoch 0 step 4000 loss 0.12106
INFO:name:epoch 0 step 4100 loss 0.12104
INFO:name:epoch 0 step 4200 loss 0.12553
INFO:name:epoch 0 step 4300 loss 0.11758
INFO:name:epoch 0 step 4400 loss 0.12453
INFO:name:epoch 0 step 4500 loss 0.11997
INFO:name:epoch 0 step 4600 loss 0.13359
INFO:name:epoch 0 step 4700 loss 0.12185
INFO:name:epoch 0 step 4800 loss 0.1178
INFO:name:epoch 0 step 4900 loss 0.12209
INFO:name:epoch 0 step 5000 loss 0.13736
INFO:name:epoch 0 step 5100 loss 0.11245
INFO:name:epoch 0 step 5200 loss 0.1216
INFO:name:epoch 0 step 5300 loss 0.12377
INFO:name:epoch 0 step 5400 loss 0.12174
INFO:name:epoch 0 step 5500 loss 0.1216
INFO:name:epoch 0 step 5600 loss 0.11892
INFO:name:epoch 0 step 5700 loss 0.11611
INFO:name:epoch 0 step 5800 loss 0.10975
INFO:name:epoch 0 step 5900 loss 0.12491
INFO:name:epoch 0 step 6000 loss 0.1221
INFO:name:epoch 0 step 6100 loss 0.10967
INFO:name:epoch 0 step 6200 loss 0.12292
INFO:name:epoch 0 step 6300 loss 0.10781
INFO:name:epoch 0 step 6400 loss 0.13746
INFO:name:epoch 0 step 6500 loss 0.1169
INFO:name:epoch 0 step 6600 loss 0.11047
INFO:name:epoch 0 step 6700 loss 0.10723
INFO:name:epoch 0 step 6800 loss 0.10485
INFO:name:epoch 0 step 6900 loss 0.10175
INFO:name:epoch 0 step 7000 loss 0.10288
INFO:name:epoch 0 step 7100 loss 0.11481
INFO:name:epoch 0 step 7200 loss 0.10954
INFO:name:epoch 0 step 7300 loss 0.11126
INFO:name:epoch 0 step 7400 loss 0.11074
INFO:name:epoch 0 step 7500 loss 0.11734
INFO:name:epoch 0 step 7600 loss 0.11007
INFO:name:epoch 0 step 7700 loss 0.10082
INFO:name:epoch 0 step 7800 loss 0.11281
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.328
INFO:name:  ********************
INFO:name:  Best eval mrr:0.328
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2714
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 0.08651
INFO:name:epoch 1 step 200 loss 0.07009
INFO:name:epoch 1 step 300 loss 0.07775
INFO:name:epoch 1 step 400 loss 0.07099
INFO:name:epoch 1 step 500 loss 0.06916
INFO:name:epoch 1 step 600 loss 0.06857
INFO:name:epoch 1 step 700 loss 0.07165
INFO:name:epoch 1 step 800 loss 0.06557
INFO:name:epoch 1 step 900 loss 0.07134
INFO:name:epoch 1 step 1000 loss 0.07701
INFO:name:epoch 1 step 1100 loss 0.06339
INFO:name:epoch 1 step 1200 loss 0.0806
INFO:name:epoch 1 step 1300 loss 0.08566
INFO:name:epoch 1 step 1400 loss 0.07041
INFO:name:epoch 1 step 1500 loss 0.06954
INFO:name:epoch 1 step 1600 loss 0.08535
INFO:name:epoch 1 step 1700 loss 0.07464
INFO:name:epoch 1 step 1800 loss 0.0665
INFO:name:epoch 1 step 1900 loss 0.07443
INFO:name:epoch 1 step 2000 loss 0.08034
INFO:name:epoch 1 step 2100 loss 0.07522
INFO:name:epoch 1 step 2200 loss 0.07981
INFO:name:epoch 1 step 2300 loss 0.07321
INFO:name:epoch 1 step 2400 loss 0.07135
INFO:name:epoch 1 step 2500 loss 0.07112
INFO:name:epoch 1 step 2600 loss 0.06998
INFO:name:epoch 1 step 2700 loss 0.08015
INFO:name:epoch 1 step 2800 loss 0.07203
INFO:name:epoch 1 step 2900 loss 0.08037
INFO:name:epoch 1 step 3000 loss 0.07059
INFO:name:epoch 1 step 3100 loss 0.0787
INFO:name:epoch 1 step 3200 loss 0.08373
INFO:name:epoch 1 step 3300 loss 0.07391
INFO:name:epoch 1 step 3400 loss 0.0743
INFO:name:epoch 1 step 3500 loss 0.07262
INFO:name:epoch 1 step 3600 loss 0.08787
INFO:name:epoch 1 step 3700 loss 0.0702
INFO:name:epoch 1 step 3800 loss 0.07058
INFO:name:epoch 1 step 3900 loss 0.08371
INFO:name:epoch 1 step 4000 loss 0.08394
INFO:name:epoch 1 step 4100 loss 0.08422
INFO:name:epoch 1 step 4200 loss 0.07395
INFO:name:epoch 1 step 4300 loss 0.08621
INFO:name:epoch 1 step 4400 loss 0.06543
INFO:name:epoch 1 step 4500 loss 0.07782
INFO:name:epoch 1 step 4600 loss 0.06875
INFO:name:epoch 1 step 4700 loss 0.06825
INFO:name:epoch 1 step 4800 loss 0.06446
INFO:name:epoch 1 step 4900 loss 0.06754
INFO:name:epoch 1 step 5000 loss 0.07012
INFO:name:epoch 1 step 5100 loss 0.06828
INFO:name:epoch 1 step 5200 loss 0.06462
INFO:name:epoch 1 step 5300 loss 0.07604
INFO:name:epoch 1 step 5400 loss 0.08313
INFO:name:epoch 1 step 5500 loss 0.07881
INFO:name:epoch 1 step 5600 loss 0.07213
INFO:name:epoch 1 step 5700 loss 0.07034
INFO:name:epoch 1 step 5800 loss 0.06941
INFO:name:epoch 1 step 5900 loss 0.0812
INFO:name:epoch 1 step 6000 loss 0.07186
INFO:name:epoch 1 step 6100 loss 0.07166
INFO:name:epoch 1 step 6200 loss 0.06973
INFO:name:epoch 1 step 6300 loss 0.07469
INFO:name:epoch 1 step 6400 loss 0.07647
INFO:name:epoch 1 step 6500 loss 0.05788
INFO:name:epoch 1 step 6600 loss 0.05806
INFO:name:epoch 1 step 6700 loss 0.06194
INFO:name:epoch 1 step 6800 loss 0.07037
INFO:name:epoch 1 step 6900 loss 0.07331
INFO:name:epoch 1 step 7000 loss 0.06145
INFO:name:epoch 1 step 7100 loss 0.06598
INFO:name:epoch 1 step 7200 loss 0.06732
INFO:name:epoch 1 step 7300 loss 0.08044
INFO:name:epoch 1 step 7400 loss 0.0753
INFO:name:epoch 1 step 7500 loss 0.06579
INFO:name:epoch 1 step 7600 loss 0.06488
INFO:name:epoch 1 step 7700 loss 0.07315
INFO:name:epoch 1 step 7800 loss 0.0753
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3496
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3496
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2908
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 2 step 100 loss 0.05414
INFO:name:epoch 2 step 200 loss 0.04849
INFO:name:epoch 2 step 300 loss 0.04525
INFO:name:epoch 2 step 400 loss 0.05431
INFO:name:epoch 2 step 500 loss 0.06063
INFO:name:epoch 2 step 600 loss 0.05058
INFO:name:epoch 2 step 700 loss 0.05198
INFO:name:epoch 2 step 800 loss 0.04837
INFO:name:epoch 2 step 900 loss 0.04525
INFO:name:epoch 2 step 1000 loss 0.05601
INFO:name:epoch 2 step 1100 loss 0.05088
INFO:name:epoch 2 step 1200 loss 0.04904
INFO:name:epoch 2 step 1300 loss 0.05384
INFO:name:epoch 2 step 1400 loss 0.04911
INFO:name:epoch 2 step 1500 loss 0.04801
INFO:name:epoch 2 step 1600 loss 0.05691
INFO:name:epoch 2 step 1700 loss 0.05451
INFO:name:epoch 2 step 1800 loss 0.0506
INFO:name:epoch 2 step 1900 loss 0.05491
INFO:name:epoch 2 step 2000 loss 0.05165
INFO:name:epoch 2 step 2100 loss 0.04969
INFO:name:epoch 2 step 2200 loss 0.05621
INFO:name:epoch 2 step 2300 loss 0.05392
INFO:name:epoch 2 step 2400 loss 0.04511
INFO:name:epoch 2 step 2500 loss 0.05343
INFO:name:epoch 2 step 2600 loss 0.05488
INFO:name:epoch 2 step 2700 loss 0.04387
INFO:name:epoch 2 step 2800 loss 0.04501
INFO:name:epoch 2 step 2900 loss 0.05233
INFO:name:epoch 2 step 3000 loss 0.04221
INFO:name:epoch 2 step 3100 loss 0.05895
INFO:name:epoch 2 step 3200 loss 0.0575
INFO:name:epoch 2 step 3300 loss 0.04905
INFO:name:epoch 2 step 3400 loss 0.05011
INFO:name:epoch 2 step 3500 loss 0.04505
INFO:name:epoch 2 step 3600 loss 0.05589
INFO:name:epoch 2 step 3700 loss 0.05417
INFO:name:epoch 2 step 3800 loss 0.04508
INFO:name:epoch 2 step 3900 loss 0.05568
INFO:name:epoch 2 step 4000 loss 0.04944
INFO:name:epoch 2 step 4100 loss 0.04853
INFO:name:epoch 2 step 4200 loss 0.04105
INFO:name:epoch 2 step 4300 loss 0.04258
INFO:name:epoch 2 step 4400 loss 0.04831
INFO:name:epoch 2 step 4500 loss 0.05453
INFO:name:epoch 2 step 4600 loss 0.04448
INFO:name:epoch 2 step 4700 loss 0.04972
INFO:name:epoch 2 step 4800 loss 0.06188
INFO:name:epoch 2 step 4900 loss 0.0543
INFO:name:epoch 2 step 5000 loss 0.05465
INFO:name:epoch 2 step 5100 loss 0.04921
INFO:name:epoch 2 step 5200 loss 0.05622
INFO:name:epoch 2 step 5300 loss 0.05243
INFO:name:epoch 2 step 5400 loss 0.04667
INFO:name:epoch 2 step 5500 loss 0.05062
INFO:name:epoch 2 step 5600 loss 0.05606
INFO:name:epoch 2 step 5700 loss 0.04266
INFO:name:epoch 2 step 5800 loss 0.04823
INFO:name:epoch 2 step 5900 loss 0.04858
INFO:name:epoch 2 step 6000 loss 0.04859
INFO:name:epoch 2 step 6100 loss 0.06919
INFO:name:epoch 2 step 6200 loss 0.05112
INFO:name:epoch 2 step 6300 loss 0.04646
INFO:name:epoch 2 step 6400 loss 0.04816
INFO:name:epoch 2 step 6500 loss 0.04857
INFO:name:epoch 2 step 6600 loss 0.04923
INFO:name:epoch 2 step 6700 loss 0.05822
INFO:name:epoch 2 step 6800 loss 0.05144
INFO:name:epoch 2 step 6900 loss 0.05419
INFO:name:epoch 2 step 7000 loss 0.04155
INFO:name:epoch 2 step 7100 loss 0.04334
INFO:name:epoch 2 step 7200 loss 0.05495
INFO:name:epoch 2 step 7300 loss 0.05097
INFO:name:epoch 2 step 7400 loss 0.05006
INFO:name:epoch 2 step 7500 loss 0.05014
INFO:name:epoch 2 step 7600 loss 0.05204
INFO:name:epoch 2 step 7700 loss 0.05709
INFO:name:epoch 2 step 7800 loss 0.05169
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3671
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3671
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3055
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.03914
INFO:name:epoch 3 step 200 loss 0.03481
INFO:name:epoch 3 step 300 loss 0.03475
INFO:name:epoch 3 step 400 loss 0.02978
INFO:name:epoch 3 step 500 loss 0.02947
INFO:name:epoch 3 step 600 loss 0.03576
INFO:name:epoch 3 step 700 loss 0.03682
INFO:name:epoch 3 step 800 loss 0.0393
INFO:name:epoch 3 step 900 loss 0.04224
INFO:name:epoch 3 step 1000 loss 0.03808
INFO:name:epoch 3 step 1100 loss 0.03323
INFO:name:epoch 3 step 1200 loss 0.03919
INFO:name:epoch 3 step 1300 loss 0.03339
INFO:name:epoch 3 step 1400 loss 0.03109
INFO:name:epoch 3 step 1500 loss 0.0343
INFO:name:epoch 3 step 1600 loss 0.03815
INFO:name:epoch 3 step 1700 loss 0.03643
INFO:name:epoch 3 step 1800 loss 0.03353
INFO:name:epoch 3 step 1900 loss 0.03651
INFO:name:epoch 3 step 2000 loss 0.03325
INFO:name:epoch 3 step 2100 loss 0.03543
INFO:name:epoch 3 step 2200 loss 0.03388
INFO:name:epoch 3 step 2300 loss 0.03934
INFO:name:epoch 3 step 2400 loss 0.03768
INFO:name:epoch 3 step 2500 loss 0.0388
INFO:name:epoch 3 step 2600 loss 0.0382
INFO:name:epoch 3 step 2700 loss 0.03339
INFO:name:epoch 3 step 2800 loss 0.03151
INFO:name:epoch 3 step 2900 loss 0.03488
INFO:name:epoch 3 step 3000 loss 0.03756
INFO:name:epoch 3 step 3100 loss 0.04051
INFO:name:epoch 3 step 3200 loss 0.03623
INFO:name:epoch 3 step 3300 loss 0.03
INFO:name:epoch 3 step 3400 loss 0.03473
INFO:name:epoch 3 step 3500 loss 0.03381
INFO:name:epoch 3 step 3600 loss 0.03248
INFO:name:epoch 3 step 3700 loss 0.0352
INFO:name:epoch 3 step 3800 loss 0.0415
INFO:name:epoch 3 step 3900 loss 0.03712
INFO:name:epoch 3 step 4000 loss 0.03569
INFO:name:epoch 3 step 4100 loss 0.04238
INFO:name:epoch 3 step 4200 loss 0.03111
INFO:name:epoch 3 step 4300 loss 0.03565
INFO:name:epoch 3 step 4400 loss 0.02919
INFO:name:epoch 3 step 4500 loss 0.04034
INFO:name:epoch 3 step 4600 loss 0.03767
INFO:name:epoch 3 step 4700 loss 0.03715
INFO:name:epoch 3 step 4800 loss 0.04298
INFO:name:epoch 3 step 4900 loss 0.03361
INFO:name:epoch 3 step 5000 loss 0.03857
INFO:name:epoch 3 step 5100 loss 0.03461
INFO:name:epoch 3 step 5200 loss 0.0346
INFO:name:epoch 3 step 5300 loss 0.04067
INFO:name:epoch 3 step 5400 loss 0.03932
INFO:name:epoch 3 step 5500 loss 0.03489
INFO:name:epoch 3 step 5600 loss 0.03725
INFO:name:epoch 3 step 5700 loss 0.04093
INFO:name:epoch 3 step 5800 loss 0.03583
INFO:name:epoch 3 step 5900 loss 0.03708
INFO:name:epoch 3 step 6000 loss 0.03315
INFO:name:epoch 3 step 6100 loss 0.03431
INFO:name:epoch 3 step 6200 loss 0.03323
INFO:name:epoch 3 step 6300 loss 0.03598
INFO:name:epoch 3 step 6400 loss 0.03592
INFO:name:epoch 3 step 6500 loss 0.03887
INFO:name:epoch 3 step 6600 loss 0.04503
INFO:name:epoch 3 step 6700 loss 0.03052
INFO:name:epoch 3 step 6800 loss 0.03926
INFO:name:epoch 3 step 6900 loss 0.04089
INFO:name:epoch 3 step 7000 loss 0.03493
INFO:name:epoch 3 step 7100 loss 0.0372
INFO:name:epoch 3 step 7200 loss 0.03923
INFO:name:epoch 3 step 7300 loss 0.04153
INFO:name:epoch 3 step 7400 loss 0.03201
INFO:name:epoch 3 step 7500 loss 0.03984
INFO:name:epoch 3 step 7600 loss 0.034
INFO:name:epoch 3 step 7700 loss 0.04108
INFO:name:epoch 3 step 7800 loss 0.0361
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3741
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3741
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3119
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 4 step 100 loss 0.03158
INFO:name:epoch 4 step 200 loss 0.02583
INFO:name:epoch 4 step 300 loss 0.02265
INFO:name:epoch 4 step 400 loss 0.02789
INFO:name:epoch 4 step 500 loss 0.02579
INFO:name:epoch 4 step 600 loss 0.02378
INFO:name:epoch 4 step 700 loss 0.02687
INFO:name:epoch 4 step 800 loss 0.02853
INFO:name:epoch 4 step 900 loss 0.02676
INFO:name:epoch 4 step 1000 loss 0.02876
INFO:name:epoch 4 step 1100 loss 0.02311
INFO:name:epoch 4 step 1200 loss 0.02905
INFO:name:epoch 4 step 1300 loss 0.02703
INFO:name:epoch 4 step 1400 loss 0.02228
INFO:name:epoch 4 step 1500 loss 0.02329
INFO:name:epoch 4 step 1600 loss 0.02371
INFO:name:epoch 4 step 1700 loss 0.02708
INFO:name:epoch 4 step 1800 loss 0.02197
INFO:name:epoch 4 step 1900 loss 0.02289
INFO:name:epoch 4 step 2000 loss 0.02624
INFO:name:epoch 4 step 2100 loss 0.02804
INFO:name:epoch 4 step 2200 loss 0.02715
INFO:name:epoch 4 step 2300 loss 0.03144
INFO:name:epoch 4 step 2400 loss 0.0262
INFO:name:epoch 4 step 2500 loss 0.02303
INFO:name:epoch 4 step 2600 loss 0.02449
INFO:name:epoch 4 step 2700 loss 0.03098
INFO:name:epoch 4 step 2800 loss 0.0272
INFO:name:epoch 4 step 2900 loss 0.02518
INFO:name:epoch 4 step 3000 loss 0.02758
INFO:name:epoch 4 step 3100 loss 0.0283
INFO:name:epoch 4 step 3200 loss 0.02533
INFO:name:epoch 4 step 3300 loss 0.02096
INFO:name:epoch 4 step 3400 loss 0.02952
INFO:name:epoch 4 step 3500 loss 0.02386
INFO:name:epoch 4 step 3600 loss 0.025
INFO:name:epoch 4 step 3700 loss 0.02349
INFO:name:epoch 4 step 3800 loss 0.02881
INFO:name:epoch 4 step 3900 loss 0.02436
INFO:name:epoch 4 step 4000 loss 0.02452
INFO:name:epoch 4 step 4100 loss 0.02584
INFO:name:epoch 4 step 4200 loss 0.02721
INFO:name:epoch 4 step 4300 loss 0.02317
INFO:name:epoch 4 step 4400 loss 0.02905
INFO:name:epoch 4 step 4500 loss 0.02608
INFO:name:epoch 4 step 4600 loss 0.02994
INFO:name:epoch 4 step 4700 loss 0.02657
INFO:name:epoch 4 step 4800 loss 0.02665
INFO:name:epoch 4 step 4900 loss 0.0284
INFO:name:epoch 4 step 5000 loss 0.02363
INFO:name:epoch 4 step 5100 loss 0.03095
INFO:name:epoch 4 step 5200 loss 0.02251
INFO:name:epoch 4 step 5300 loss 0.0267
INFO:name:epoch 4 step 5400 loss 0.02929
INFO:name:epoch 4 step 5500 loss 0.02791
INFO:name:epoch 4 step 5600 loss 0.02324
INFO:name:epoch 4 step 5700 loss 0.02783
INFO:name:epoch 4 step 5800 loss 0.02505
INFO:name:epoch 4 step 5900 loss 0.02851
INFO:name:epoch 4 step 6000 loss 0.02031
INFO:name:epoch 4 step 6100 loss 0.02856
INFO:name:epoch 4 step 6200 loss 0.02678
INFO:name:epoch 4 step 6300 loss 0.03245
INFO:name:epoch 4 step 6400 loss 0.02661
INFO:name:epoch 4 step 6500 loss 0.02322
INFO:name:epoch 4 step 6600 loss 0.02813
INFO:name:epoch 4 step 6700 loss 0.02651
INFO:name:epoch 4 step 6800 loss 0.02518
INFO:name:epoch 4 step 6900 loss 0.02984
INFO:name:epoch 4 step 7000 loss 0.03261
INFO:name:epoch 4 step 7100 loss 0.02739
INFO:name:epoch 4 step 7200 loss 0.02833
INFO:name:epoch 4 step 7300 loss 0.03089
INFO:name:epoch 4 step 7400 loss 0.03022
INFO:name:epoch 4 step 7500 loss 0.0262
INFO:name:epoch 4 step 7600 loss 0.02553
INFO:name:epoch 4 step 7700 loss 0.02406
INFO:name:epoch 4 step 7800 loss 0.02374
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3631
INFO:name:epoch 5 step 100 loss 0.021
INFO:name:epoch 5 step 200 loss 0.02218
INFO:name:epoch 5 step 300 loss 0.02248
INFO:name:epoch 5 step 400 loss 0.02245
INFO:name:epoch 5 step 500 loss 0.02052
INFO:name:epoch 5 step 600 loss 0.02129
INFO:name:epoch 5 step 700 loss 0.02034
INFO:name:epoch 5 step 800 loss 0.0216
INFO:name:epoch 5 step 900 loss 0.01679
INFO:name:epoch 5 step 1000 loss 0.02157
INFO:name:epoch 5 step 1100 loss 0.01915
INFO:name:epoch 5 step 1200 loss 0.01688
INFO:name:epoch 5 step 1300 loss 0.01863
INFO:name:epoch 5 step 1400 loss 0.01733
INFO:name:epoch 5 step 1500 loss 0.01944
INFO:name:epoch 5 step 1600 loss 0.01921
INFO:name:epoch 5 step 1700 loss 0.0179
INFO:name:epoch 5 step 1800 loss 0.01979
INFO:name:epoch 5 step 1900 loss 0.01914
INFO:name:epoch 5 step 2000 loss 0.01627
INFO:name:epoch 5 step 2100 loss 0.01593
INFO:name:epoch 5 step 2200 loss 0.02107
INFO:name:epoch 5 step 2300 loss 0.01676
INFO:name:epoch 5 step 2400 loss 0.02106
INFO:name:epoch 5 step 2500 loss 0.02141
INFO:name:epoch 5 step 2600 loss 0.01722
INFO:name:epoch 5 step 2700 loss 0.01836
INFO:name:epoch 5 step 2800 loss 0.02184
INFO:name:epoch 5 step 2900 loss 0.02255
INFO:name:epoch 5 step 3000 loss 0.01767
INFO:name:epoch 5 step 3100 loss 0.02407
INFO:name:epoch 5 step 3200 loss 0.01805
INFO:name:epoch 5 step 3300 loss 0.01814
INFO:name:epoch 5 step 3400 loss 0.01792
INFO:name:epoch 5 step 3500 loss 0.02196
INFO:name:epoch 5 step 3600 loss 0.01905
INFO:name:epoch 5 step 3700 loss 0.018
INFO:name:epoch 5 step 3800 loss 0.02199
INFO:name:epoch 5 step 3900 loss 0.01844
INFO:name:epoch 5 step 4000 loss 0.01877
INFO:name:epoch 5 step 4100 loss 0.01917
INFO:name:epoch 5 step 4200 loss 0.0215
INFO:name:epoch 5 step 4300 loss 0.02134
INFO:name:epoch 5 step 4400 loss 0.01983
INFO:name:epoch 5 step 4500 loss 0.021
INFO:name:epoch 5 step 4600 loss 0.01767
INFO:name:epoch 5 step 4700 loss 0.02181
INFO:name:epoch 5 step 4800 loss 0.01709
INFO:name:epoch 5 step 4900 loss 0.01884
INFO:name:epoch 5 step 5000 loss 0.01878
INFO:name:epoch 5 step 5100 loss 0.0209
INFO:name:epoch 5 step 5200 loss 0.01912
INFO:name:epoch 5 step 5300 loss 0.01948
INFO:name:epoch 5 step 5400 loss 0.02267
INFO:name:epoch 5 step 5500 loss 0.02271
INFO:name:epoch 5 step 5600 loss 0.01838
INFO:name:epoch 5 step 5700 loss 0.02298
INFO:name:epoch 5 step 5800 loss 0.02048
INFO:name:epoch 5 step 5900 loss 0.01984
INFO:name:epoch 5 step 6000 loss 0.01678
INFO:name:epoch 5 step 6100 loss 0.02135
INFO:name:epoch 5 step 6200 loss 0.0178
INFO:name:epoch 5 step 6300 loss 0.01942
INFO:name:epoch 5 step 6400 loss 0.0199
INFO:name:epoch 5 step 6500 loss 0.02036
INFO:name:epoch 5 step 6600 loss 0.02356
INFO:name:epoch 5 step 6700 loss 0.01494
INFO:name:epoch 5 step 6800 loss 0.01965
INFO:name:epoch 5 step 6900 loss 0.02419
INFO:name:epoch 5 step 7000 loss 0.02209
INFO:name:epoch 5 step 7100 loss 0.01905
INFO:name:epoch 5 step 7200 loss 0.02021
INFO:name:epoch 5 step 7300 loss 0.01724
INFO:name:epoch 5 step 7400 loss 0.01829
INFO:name:epoch 5 step 7500 loss 0.02063
INFO:name:epoch 5 step 7600 loss 0.01799
INFO:name:epoch 5 step 7700 loss 0.01956
INFO:name:epoch 5 step 7800 loss 0.01938
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3668
INFO:name:epoch 6 step 100 loss 0.01873
INFO:name:epoch 6 step 200 loss 0.01997
INFO:name:epoch 6 step 300 loss 0.0131
INFO:name:epoch 6 step 400 loss 0.01535
INFO:name:epoch 6 step 500 loss 0.01461
INFO:name:epoch 6 step 600 loss 0.01495
INFO:name:epoch 6 step 700 loss 0.01618
INFO:name:epoch 6 step 800 loss 0.01449
INFO:name:epoch 6 step 900 loss 0.01332
INFO:name:epoch 6 step 1000 loss 0.01476
INFO:name:epoch 6 step 1100 loss 0.01542
INFO:name:epoch 6 step 1200 loss 0.01275
INFO:name:epoch 6 step 1300 loss 0.01655
INFO:name:epoch 6 step 1400 loss 0.0143
INFO:name:epoch 6 step 1500 loss 0.01455
INFO:name:epoch 6 step 1600 loss 0.0135
INFO:name:epoch 6 step 1700 loss 0.01464
INFO:name:epoch 6 step 1800 loss 0.01823
INFO:name:epoch 6 step 1900 loss 0.01734
INFO:name:epoch 6 step 2000 loss 0.01532
INFO:name:epoch 6 step 2100 loss 0.01713
INFO:name:epoch 6 step 2200 loss 0.01429
INFO:name:epoch 6 step 2300 loss 0.01418
INFO:name:epoch 6 step 2400 loss 0.01737
INFO:name:epoch 6 step 2500 loss 0.0151
INFO:name:epoch 6 step 2600 loss 0.01402
INFO:name:epoch 6 step 2700 loss 0.01698
INFO:name:epoch 6 step 2800 loss 0.01707
INFO:name:epoch 6 step 2900 loss 0.01945
INFO:name:epoch 6 step 3000 loss 0.01545
INFO:name:epoch 6 step 3100 loss 0.02078
INFO:name:epoch 6 step 3200 loss 0.01804
INFO:name:epoch 6 step 3300 loss 0.01413
INFO:name:epoch 6 step 3400 loss 0.01489
INFO:name:epoch 6 step 3500 loss 0.01446
INFO:name:epoch 6 step 3600 loss 0.01292
INFO:name:epoch 6 step 3700 loss 0.01691
INFO:name:epoch 6 step 3800 loss 0.01278
INFO:name:epoch 6 step 3900 loss 0.01831
INFO:name:epoch 6 step 4000 loss 0.01803
INFO:name:epoch 6 step 4100 loss 0.01565
INFO:name:epoch 6 step 4200 loss 0.01704
INFO:name:epoch 6 step 4300 loss 0.0181
INFO:name:epoch 6 step 4400 loss 0.01741
INFO:name:epoch 6 step 4500 loss 0.01587
INFO:name:epoch 6 step 4600 loss 0.01672
INFO:name:epoch 6 step 4700 loss 0.01447
INFO:name:epoch 6 step 4800 loss 0.01818
INFO:name:epoch 6 step 4900 loss 0.01881
INFO:name:epoch 6 step 5000 loss 0.0136
INFO:name:epoch 6 step 5100 loss 0.01865
INFO:name:epoch 6 step 5200 loss 0.01396
INFO:name:epoch 6 step 5300 loss 0.01364
INFO:name:epoch 6 step 5400 loss 0.01491
INFO:name:epoch 6 step 5500 loss 0.01547
INFO:name:epoch 6 step 5600 loss 0.01761
INFO:name:epoch 6 step 5700 loss 0.01911
INFO:name:epoch 6 step 5800 loss 0.01949
INFO:name:epoch 6 step 5900 loss 0.01691
INFO:name:epoch 6 step 6000 loss 0.0131
INFO:name:epoch 6 step 6100 loss 0.01131
INFO:name:epoch 6 step 6200 loss 0.01438
INFO:name:epoch 6 step 6300 loss 0.01364
INFO:name:epoch 6 step 6400 loss 0.01173
INFO:name:epoch 6 step 6500 loss 0.01606
INFO:name:epoch 6 step 6600 loss 0.01671
INFO:name:epoch 6 step 6700 loss 0.01591
INFO:name:epoch 6 step 6800 loss 0.01908
INFO:name:epoch 6 step 6900 loss 0.01661
INFO:name:epoch 6 step 7000 loss 0.01783
INFO:name:epoch 6 step 7100 loss 0.01505
INFO:name:epoch 6 step 7200 loss 0.01924
INFO:name:epoch 6 step 7300 loss 0.015
INFO:name:epoch 6 step 7400 loss 0.01816
INFO:name:epoch 6 step 7500 loss 0.01375
INFO:name:epoch 6 step 7600 loss 0.02163
INFO:name:epoch 6 step 7700 loss 0.01449
INFO:name:epoch 6 step 7800 loss 0.01511
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3506
INFO:name:epoch 7 step 100 loss 0.01372
INFO:name:epoch 7 step 200 loss 0.01201
INFO:name:epoch 7 step 300 loss 0.01418
INFO:name:epoch 7 step 400 loss 0.01466
INFO:name:epoch 7 step 500 loss 0.01275
INFO:name:epoch 7 step 600 loss 0.01345
INFO:name:epoch 7 step 700 loss 0.01174
INFO:name:epoch 7 step 800 loss 0.01229
INFO:name:epoch 7 step 900 loss 0.01022
INFO:name:epoch 7 step 1000 loss 0.01065
INFO:name:epoch 7 step 1100 loss 0.01295
INFO:name:epoch 7 step 1200 loss 0.01063
INFO:name:epoch 7 step 1300 loss 0.013
INFO:name:epoch 7 step 1400 loss 0.01221
INFO:name:epoch 7 step 1500 loss 0.01243
INFO:name:epoch 7 step 1600 loss 0.0141
INFO:name:epoch 7 step 1700 loss 0.01312
INFO:name:epoch 7 step 1800 loss 0.01201
INFO:name:epoch 7 step 1900 loss 0.01507
INFO:name:epoch 7 step 2000 loss 0.01183
INFO:name:epoch 7 step 2100 loss 0.01438
INFO:name:epoch 7 step 2200 loss 0.01323
INFO:name:epoch 7 step 2300 loss 0.01369
INFO:name:epoch 7 step 2400 loss 0.01564
INFO:name:epoch 7 step 2500 loss 0.01723
INFO:name:epoch 7 step 2600 loss 0.01346
INFO:name:epoch 7 step 2700 loss 0.01205
INFO:name:epoch 7 step 2800 loss 0.01406
INFO:name:epoch 7 step 2900 loss 0.01308
INFO:name:epoch 7 step 3000 loss 0.01137
INFO:name:epoch 7 step 3100 loss 0.01276
INFO:name:epoch 7 step 3200 loss 0.0147
INFO:name:epoch 7 step 3300 loss 0.01273
INFO:name:epoch 7 step 3400 loss 0.01227
INFO:name:epoch 7 step 3500 loss 0.01126
INFO:name:epoch 7 step 3600 loss 0.01282
INFO:name:epoch 7 step 3700 loss 0.01068
INFO:name:epoch 7 step 3800 loss 0.01177
INFO:name:epoch 7 step 3900 loss 0.01427
INFO:name:epoch 7 step 4000 loss 0.01547
INFO:name:epoch 7 step 4100 loss 0.01415
INFO:name:epoch 7 step 4200 loss 0.01339
INFO:name:epoch 7 step 4300 loss 0.01241
INFO:name:epoch 7 step 4400 loss 0.01418
INFO:name:epoch 7 step 4500 loss 0.01195
INFO:name:epoch 7 step 4600 loss 0.01215
INFO:name:epoch 7 step 4700 loss 0.01293
INFO:name:epoch 7 step 4800 loss 0.01613
INFO:name:epoch 7 step 4900 loss 0.01149
INFO:name:epoch 7 step 5000 loss 0.01731
INFO:name:epoch 7 step 5100 loss 0.01208
INFO:name:epoch 7 step 5200 loss 0.0162
INFO:name:epoch 7 step 5300 loss 0.01526
INFO:name:epoch 7 step 5400 loss 0.01404
INFO:name:epoch 7 step 5500 loss 0.01401
INFO:name:epoch 7 step 5600 loss 0.01325
INFO:name:epoch 7 step 5700 loss 0.01307
INFO:name:epoch 7 step 5800 loss 0.01168
INFO:name:epoch 7 step 5900 loss 0.01611
INFO:name:epoch 7 step 6000 loss 0.0161
INFO:name:epoch 7 step 6100 loss 0.01363
INFO:name:epoch 7 step 6200 loss 0.01191
INFO:name:epoch 7 step 6300 loss 0.01681
INFO:name:epoch 7 step 6400 loss 0.01208
INFO:name:epoch 7 step 6500 loss 0.01507
INFO:name:epoch 7 step 6600 loss 0.01296
INFO:name:epoch 7 step 6700 loss 0.01244
INFO:name:epoch 7 step 6800 loss 0.01292
INFO:name:epoch 7 step 6900 loss 0.00983
INFO:name:epoch 7 step 7000 loss 0.00971
INFO:name:epoch 7 step 7100 loss 0.01121
INFO:name:epoch 7 step 7200 loss 0.01352
INFO:name:epoch 7 step 7300 loss 0.01445
INFO:name:epoch 7 step 7400 loss 0.01439
INFO:name:epoch 7 step 7500 loss 0.01103
INFO:name:epoch 7 step 7600 loss 0.01278
INFO:name:epoch 7 step 7700 loss 0.01084
INFO:name:epoch 7 step 7800 loss 0.01406
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3755
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3755
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3119
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 8 step 100 loss 0.01386
INFO:name:epoch 8 step 200 loss 0.01005
INFO:name:epoch 8 step 300 loss 0.01352
INFO:name:epoch 8 step 400 loss 0.01076
INFO:name:epoch 8 step 500 loss 0.01108
INFO:name:epoch 8 step 600 loss 0.00915
INFO:name:epoch 8 step 700 loss 0.01247
INFO:name:epoch 8 step 800 loss 0.01364
INFO:name:epoch 8 step 900 loss 0.01372
INFO:name:epoch 8 step 1000 loss 0.01019
INFO:name:epoch 8 step 1100 loss 0.00955
INFO:name:epoch 8 step 1200 loss 0.01058
INFO:name:epoch 8 step 1300 loss 0.01125
INFO:name:epoch 8 step 1400 loss 0.01074
INFO:name:epoch 8 step 1500 loss 0.01012
INFO:name:epoch 8 step 1600 loss 0.01256
INFO:name:epoch 8 step 1700 loss 0.01032
INFO:name:epoch 8 step 1800 loss 0.01032
INFO:name:epoch 8 step 1900 loss 0.01097
INFO:name:epoch 8 step 2000 loss 0.01528
INFO:name:epoch 8 step 2100 loss 0.01117
INFO:name:epoch 8 step 2200 loss 0.01158
INFO:name:epoch 8 step 2300 loss 0.00893
INFO:name:epoch 8 step 2400 loss 0.01302
INFO:name:epoch 8 step 2500 loss 0.01202
INFO:name:epoch 8 step 2600 loss 0.01499
INFO:name:epoch 8 step 2700 loss 0.01322
INFO:name:epoch 8 step 2800 loss 0.01189
INFO:name:epoch 8 step 2900 loss 0.01212
INFO:name:epoch 8 step 3000 loss 0.01198
INFO:name:epoch 8 step 3100 loss 0.01061
INFO:name:epoch 8 step 3200 loss 0.01049
INFO:name:epoch 8 step 3300 loss 0.01091
INFO:name:epoch 8 step 3400 loss 0.01157
INFO:name:epoch 8 step 3500 loss 0.01257
INFO:name:epoch 8 step 3600 loss 0.01046
INFO:name:epoch 8 step 3700 loss 0.01016
INFO:name:epoch 8 step 3800 loss 0.01466
INFO:name:epoch 8 step 3900 loss 0.01097
INFO:name:epoch 8 step 4000 loss 0.01323
INFO:name:epoch 8 step 4100 loss 0.0145
INFO:name:epoch 8 step 4200 loss 0.0089
INFO:name:epoch 8 step 4300 loss 0.01207
INFO:name:epoch 8 step 4400 loss 0.01268
INFO:name:epoch 8 step 4500 loss 0.0094
INFO:name:epoch 8 step 4600 loss 0.01105
INFO:name:epoch 8 step 4700 loss 0.01133
INFO:name:epoch 8 step 4800 loss 0.01023
INFO:name:epoch 8 step 4900 loss 0.01258
INFO:name:epoch 8 step 5000 loss 0.01175
INFO:name:epoch 8 step 5100 loss 0.01004
INFO:name:epoch 8 step 5200 loss 0.01097
INFO:name:epoch 8 step 5300 loss 0.01234
INFO:name:epoch 8 step 5400 loss 0.01198
INFO:name:epoch 8 step 5500 loss 0.01067
INFO:name:epoch 8 step 5600 loss 0.0117
INFO:name:epoch 8 step 5700 loss 0.00991
INFO:name:epoch 8 step 5800 loss 0.0112
INFO:name:epoch 8 step 5900 loss 0.01281
INFO:name:epoch 8 step 6000 loss 0.01122
INFO:name:epoch 8 step 6100 loss 0.01133
INFO:name:epoch 8 step 6200 loss 0.01153
INFO:name:epoch 8 step 6300 loss 0.0102
INFO:name:epoch 8 step 6400 loss 0.01169
INFO:name:epoch 8 step 6500 loss 0.01419
INFO:name:epoch 8 step 6600 loss 0.01173
INFO:name:epoch 8 step 6700 loss 0.0099
INFO:name:epoch 8 step 6800 loss 0.01167
INFO:name:epoch 8 step 6900 loss 0.0098
INFO:name:epoch 8 step 7000 loss 0.01008
INFO:name:epoch 8 step 7100 loss 0.01039
INFO:name:epoch 8 step 7200 loss 0.01456
INFO:name:epoch 8 step 7300 loss 0.00892
INFO:name:epoch 8 step 7400 loss 0.01173
INFO:name:epoch 8 step 7500 loss 0.01292
INFO:name:epoch 8 step 7600 loss 0.01059
INFO:name:epoch 8 step 7700 loss 0.00781
INFO:name:epoch 8 step 7800 loss 0.00974
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3693
INFO:name:epoch 9 step 100 loss 0.00916
INFO:name:epoch 9 step 200 loss 0.00991
INFO:name:epoch 9 step 300 loss 0.00967
INFO:name:epoch 9 step 400 loss 0.00927
INFO:name:epoch 9 step 500 loss 0.01456
INFO:name:epoch 9 step 600 loss 0.00823
INFO:name:epoch 9 step 700 loss 0.00867
INFO:name:epoch 9 step 800 loss 0.0104
INFO:name:epoch 9 step 900 loss 0.01137
INFO:name:epoch 9 step 1000 loss 0.01067
INFO:name:epoch 9 step 1100 loss 0.00984
INFO:name:epoch 9 step 1200 loss 0.00988
INFO:name:epoch 9 step 1300 loss 0.01012
INFO:name:epoch 9 step 1400 loss 0.01022
INFO:name:epoch 9 step 1500 loss 0.01295
INFO:name:epoch 9 step 1600 loss 0.0117
INFO:name:epoch 9 step 1700 loss 0.00889
INFO:name:epoch 9 step 1800 loss 0.00982
INFO:name:epoch 9 step 1900 loss 0.01007
INFO:name:epoch 9 step 2000 loss 0.00919
INFO:name:epoch 9 step 2100 loss 0.01281
INFO:name:epoch 9 step 2200 loss 0.01188
INFO:name:epoch 9 step 2300 loss 0.01041
INFO:name:epoch 9 step 2400 loss 0.00998
INFO:name:epoch 9 step 2500 loss 0.00853
INFO:name:epoch 9 step 2600 loss 0.00988
INFO:name:epoch 9 step 2700 loss 0.00937
INFO:name:epoch 9 step 2800 loss 0.00869
INFO:name:epoch 9 step 2900 loss 0.01069
INFO:name:epoch 9 step 3000 loss 0.01036
INFO:name:epoch 9 step 3100 loss 0.00837
INFO:name:epoch 9 step 3200 loss 0.00986
INFO:name:epoch 9 step 3300 loss 0.00921
INFO:name:epoch 9 step 3400 loss 0.01279
INFO:name:epoch 9 step 3500 loss 0.01157
INFO:name:epoch 9 step 3600 loss 0.0108
INFO:name:epoch 9 step 3700 loss 0.01052
INFO:name:epoch 9 step 3800 loss 0.00864
INFO:name:epoch 9 step 3900 loss 0.0127
INFO:name:epoch 9 step 4000 loss 0.0102
INFO:name:epoch 9 step 4100 loss 0.00935
INFO:name:epoch 9 step 4200 loss 0.01074
INFO:name:epoch 9 step 4300 loss 0.01105
INFO:name:epoch 9 step 4400 loss 0.00961
INFO:name:epoch 9 step 4500 loss 0.00864
INFO:name:epoch 9 step 4600 loss 0.01119
INFO:name:epoch 9 step 4700 loss 0.01203
INFO:name:epoch 9 step 4800 loss 0.00796
INFO:name:epoch 9 step 4900 loss 0.0083
INFO:name:epoch 9 step 5000 loss 0.0124
INFO:name:epoch 9 step 5100 loss 0.00955
INFO:name:epoch 9 step 5200 loss 0.00983
INFO:name:epoch 9 step 5300 loss 0.01
INFO:name:epoch 9 step 5400 loss 0.00959
INFO:name:epoch 9 step 5500 loss 0.00873
INFO:name:epoch 9 step 5600 loss 0.00962
INFO:name:epoch 9 step 5700 loss 0.00812
INFO:name:epoch 9 step 5800 loss 0.0093
INFO:name:epoch 9 step 5900 loss 0.01095
INFO:name:epoch 9 step 6000 loss 0.00984
INFO:name:epoch 9 step 6100 loss 0.0123
INFO:name:epoch 9 step 6200 loss 0.00908
INFO:name:epoch 9 step 6300 loss 0.00979
INFO:name:epoch 9 step 6400 loss 0.01143
INFO:name:epoch 9 step 6500 loss 0.00993
INFO:name:epoch 9 step 6600 loss 0.00855
INFO:name:epoch 9 step 6700 loss 0.00988
INFO:name:epoch 9 step 6800 loss 0.00919
INFO:name:epoch 9 step 6900 loss 0.00979
INFO:name:epoch 9 step 7000 loss 0.00864
INFO:name:epoch 9 step 7100 loss 0.00993
INFO:name:epoch 9 step 7200 loss 0.00886
INFO:name:epoch 9 step 7300 loss 0.01244
INFO:name:epoch 9 step 7400 loss 0.00806
INFO:name:epoch 9 step 7500 loss 0.00843
INFO:name:epoch 9 step 7600 loss 0.00934
INFO:name:epoch 9 step 7700 loss 0.00721
INFO:name:epoch 9 step 7800 loss 0.01132
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3651
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:[0, 0, {'insert_modules': ('layer.0.SelfAttention', 'layer.0'), 'bottleneck_dim': (32, 128), 'non_linearity': 'gelu', 'dropout_rate': 0.2, 'normalization': 'layer_norm', 'skip_connection': True}, 0, 0, {'insert_modules': ('layer.0', 'layer.1'), 'bottleneck_dim': (64, 256), 'non_linearity': 'gelu_new', 'dropout_rate': 0.1, 'normalization': 'layer_norm', 'skip_connection': True}, 0, {'insert_modules': ('layer.0.SelfAttention', 'layer.1', 'layer.0'), 'bottleneck_dim': (16, 128, 128), 'non_linearity': 'gelu', 'dropout_rate': 0.3, 'normalization': 'layer_norm', 'skip_connection': True}, 0, 0, 0, {'insert_modules': ('layer.1',), 'bottleneck_dim': (256,), 'non_linearity': 'tanh', 'dropout_rate': 0.0, 'normalization': 'layer_norm', 'skip_connection': True}]
[INFO|(OpenDelta)basemodel:700]2025-01-12 23:41:20,131 >> Trainable Ratio: 1557456/224439504=0.693931%
[INFO|(OpenDelta)basemodel:702]2025-01-12 23:41:20,131 >> Delta Parameter Ratio: 1557456/224439504=0.693931%
[INFO|(OpenDelta)basemodel:704]2025-01-12 23:41:20,131 >> Static Memory 0.44 GB, Max Memory 8.74 GB
INFO:name:1.4000000000000001
/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 10
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 78700
train results ([0.17372813518916458, 0.07304716030500528, 0.050863085200198305, 0.036252122545621344, 0.02644415486345406, 0.01972874872295631, 0.01592757746320173, 0.013193725890503945, 0.011399307181107399, 0.010028067481726393], [0.3280196491156858, 0.34960668616461377, 0.3671277899394862, 0.3740762788569191, 0.36309358604312025, 0.36675692533432463, 0.3505800054890667, 0.37552552193551353, 0.3693243830314572, 0.3650745932329443])
INFO:name:epoch 0 step 100 loss 2.66761
INFO:name:epoch 0 step 200 loss 0.61464
INFO:name:epoch 0 step 300 loss 0.32454
INFO:name:epoch 0 step 400 loss 0.30754
INFO:name:epoch 0 step 500 loss 0.26499
INFO:name:epoch 0 step 600 loss 0.22968
INFO:name:epoch 0 step 700 loss 0.23175
INFO:name:epoch 0 step 800 loss 0.22902
INFO:name:epoch 0 step 900 loss 0.19192
INFO:name:epoch 0 step 1000 loss 0.18048
INFO:name:epoch 0 step 1100 loss 0.20835
INFO:name:epoch 0 step 1200 loss 0.1896
INFO:name:epoch 0 step 1300 loss 0.16986
INFO:name:epoch 0 step 1400 loss 0.18096
INFO:name:epoch 0 step 1500 loss 0.18138
INFO:name:epoch 0 step 1600 loss 0.16761
INFO:name:epoch 0 step 1700 loss 0.17596
INFO:name:epoch 0 step 1800 loss 0.16906
INFO:name:epoch 0 step 1900 loss 0.16544
INFO:name:epoch 0 step 2000 loss 0.14333
INFO:name:epoch 0 step 2100 loss 0.16267
INFO:name:epoch 0 step 2200 loss 0.15357
INFO:name:epoch 0 step 2300 loss 0.15954
INFO:name:epoch 0 step 2400 loss 0.14692
INFO:name:epoch 0 step 2500 loss 0.15431
INFO:name:epoch 0 step 2600 loss 0.14749
INFO:name:epoch 0 step 2700 loss 0.16206
INFO:name:epoch 0 step 2800 loss 0.16693
INFO:name:epoch 0 step 2900 loss 0.15548
INFO:name:epoch 0 step 3000 loss 0.14924
INFO:name:epoch 0 step 3100 loss 0.15554
INFO:name:epoch 0 step 3200 loss 0.14832
INFO:name:epoch 0 step 3300 loss 0.15126
INFO:name:epoch 0 step 3400 loss 0.15722
INFO:name:epoch 0 step 3500 loss 0.14674
INFO:name:epoch 0 step 3600 loss 0.15788
INFO:name:epoch 0 step 3700 loss 0.14771
INFO:name:epoch 0 step 3800 loss 0.13042
INFO:name:epoch 0 step 3900 loss 0.14944
INFO:name:epoch 0 step 4000 loss 0.13665
INFO:name:epoch 0 step 4100 loss 0.12715
INFO:name:epoch 0 step 4200 loss 0.14649
INFO:name:epoch 0 step 4300 loss 0.1369
INFO:name:epoch 0 step 4400 loss 0.13987
INFO:name:epoch 0 step 4500 loss 0.13581
INFO:name:epoch 0 step 4600 loss 0.1249
INFO:name:epoch 0 step 4700 loss 0.14466
INFO:name:epoch 0 step 4800 loss 0.13443
INFO:name:epoch 0 step 4900 loss 0.14021
INFO:name:epoch 0 step 5000 loss 0.13023
INFO:name:epoch 0 step 5100 loss 0.13738
INFO:name:epoch 0 step 5200 loss 0.1356
INFO:name:epoch 0 step 5300 loss 0.1224
INFO:name:epoch 0 step 5400 loss 0.13469
INFO:name:epoch 0 step 5500 loss 0.13258
INFO:name:epoch 0 step 5600 loss 0.13601
INFO:name:epoch 0 step 5700 loss 0.11156
INFO:name:epoch 0 step 5800 loss 0.12031
INFO:name:epoch 0 step 5900 loss 0.13796
INFO:name:epoch 0 step 6000 loss 0.13368
INFO:name:epoch 0 step 6100 loss 0.12435
INFO:name:epoch 0 step 6200 loss 0.11617
INFO:name:epoch 0 step 6300 loss 0.13152
INFO:name:epoch 0 step 6400 loss 0.11331
INFO:name:epoch 0 step 6500 loss 0.13326
INFO:name:epoch 0 step 6600 loss 0.13471
INFO:name:epoch 0 step 6700 loss 0.11203
INFO:name:epoch 0 step 6800 loss 0.12128
INFO:name:epoch 0 step 6900 loss 0.13721
INFO:name:epoch 0 step 7000 loss 0.12573
INFO:name:epoch 0 step 7100 loss 0.13323
INFO:name:epoch 0 step 7200 loss 0.12321
INFO:name:epoch 0 step 7300 loss 0.12893
INFO:name:epoch 0 step 7400 loss 0.11663
INFO:name:epoch 0 step 7500 loss 0.13113
INFO:name:epoch 0 step 7600 loss 0.12712
INFO:name:epoch 0 step 7700 loss 0.13706
INFO:name:epoch 0 step 7800 loss 0.11338
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3248
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3248
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.266
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 0.10369
INFO:name:epoch 1 step 200 loss 0.10167
INFO:name:epoch 1 step 300 loss 0.07967
INFO:name:epoch 1 step 400 loss 0.08742
INFO:name:epoch 1 step 500 loss 0.09099
INFO:name:epoch 1 step 600 loss 0.08043
INFO:name:epoch 1 step 700 loss 0.08155
INFO:name:epoch 1 step 800 loss 0.08435
INFO:name:epoch 1 step 900 loss 0.08703
INFO:name:epoch 1 step 1000 loss 0.08439
INFO:name:epoch 1 step 1100 loss 0.08119
INFO:name:epoch 1 step 1200 loss 0.08994
INFO:name:epoch 1 step 1300 loss 0.07751
INFO:name:epoch 1 step 1400 loss 0.08388
INFO:name:epoch 1 step 1500 loss 0.09559
INFO:name:epoch 1 step 1600 loss 0.08441
INFO:name:epoch 1 step 1700 loss 0.07994
INFO:name:epoch 1 step 1800 loss 0.07768
INFO:name:epoch 1 step 1900 loss 0.10023
INFO:name:epoch 1 step 2000 loss 0.07616
INFO:name:epoch 1 step 2100 loss 0.06489
INFO:name:epoch 1 step 2200 loss 0.06927
INFO:name:epoch 1 step 2300 loss 0.09094
INFO:name:epoch 1 step 2400 loss 0.07988
INFO:name:epoch 1 step 2500 loss 0.08371
INFO:name:epoch 1 step 2600 loss 0.08019
INFO:name:epoch 1 step 2700 loss 0.08689
INFO:name:epoch 1 step 2800 loss 0.07926
INFO:name:epoch 1 step 2900 loss 0.07396
INFO:name:epoch 1 step 3000 loss 0.07779
INFO:name:epoch 1 step 3100 loss 0.08925
INFO:name:epoch 1 step 3200 loss 0.07597
INFO:name:epoch 1 step 3300 loss 0.08042
INFO:name:epoch 1 step 3400 loss 0.08714
INFO:name:epoch 1 step 3500 loss 0.07485
INFO:name:epoch 1 step 3600 loss 0.07911
INFO:name:epoch 1 step 3700 loss 0.08058
INFO:name:epoch 1 step 3800 loss 0.06762
INFO:name:epoch 1 step 3900 loss 0.0893
INFO:name:epoch 1 step 4000 loss 0.08459
INFO:name:epoch 1 step 4100 loss 0.08123
INFO:name:epoch 1 step 4200 loss 0.07864
INFO:name:epoch 1 step 4300 loss 0.08485
INFO:name:epoch 1 step 4400 loss 0.08387
INFO:name:epoch 1 step 4500 loss 0.08498
INFO:name:epoch 1 step 4600 loss 0.07171
INFO:name:epoch 1 step 4700 loss 0.0908
INFO:name:epoch 1 step 4800 loss 0.08058
INFO:name:epoch 1 step 4900 loss 0.07855
INFO:name:epoch 1 step 5000 loss 0.06613
INFO:name:epoch 1 step 5100 loss 0.08958
INFO:name:epoch 1 step 5200 loss 0.08043
INFO:name:epoch 1 step 5300 loss 0.07723
INFO:name:epoch 1 step 5400 loss 0.07086
INFO:name:epoch 1 step 5500 loss 0.07251
INFO:name:epoch 1 step 5600 loss 0.0843
INFO:name:epoch 1 step 5700 loss 0.08699
INFO:name:epoch 1 step 5800 loss 0.08354
INFO:name:epoch 1 step 5900 loss 0.07193
INFO:name:epoch 1 step 6000 loss 0.08328
INFO:name:epoch 1 step 6100 loss 0.08951
INFO:name:epoch 1 step 6200 loss 0.07618
INFO:name:epoch 1 step 6300 loss 0.08175
INFO:name:epoch 1 step 6400 loss 0.07145
INFO:name:epoch 1 step 6500 loss 0.07838
INFO:name:epoch 1 step 6600 loss 0.07794
INFO:name:epoch 1 step 6700 loss 0.07311
INFO:name:epoch 1 step 6800 loss 0.07991
INFO:name:epoch 1 step 6900 loss 0.07756
INFO:name:epoch 1 step 7000 loss 0.08074
INFO:name:epoch 1 step 7100 loss 0.08097
INFO:name:epoch 1 step 7200 loss 0.08106
INFO:name:epoch 1 step 7300 loss 0.07826
INFO:name:epoch 1 step 7400 loss 0.07079
INFO:name:epoch 1 step 7500 loss 0.08568
INFO:name:epoch 1 step 7600 loss 0.06961
INFO:name:epoch 1 step 7700 loss 0.08106
INFO:name:epoch 1 step 7800 loss 0.07279
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3582
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3582
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2974
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 2 step 100 loss 0.06889
INFO:name:epoch 2 step 200 loss 0.05426
INFO:name:epoch 2 step 300 loss 0.05305
INFO:name:epoch 2 step 400 loss 0.05526
INFO:name:epoch 2 step 500 loss 0.06383
INFO:name:epoch 2 step 600 loss 0.06244
INFO:name:epoch 2 step 700 loss 0.05768
INFO:name:epoch 2 step 800 loss 0.05868
INFO:name:epoch 2 step 900 loss 0.06194
INFO:name:epoch 2 step 1000 loss 0.05663
INFO:name:epoch 2 step 1100 loss 0.06557
INFO:name:epoch 2 step 1200 loss 0.0598
INFO:name:epoch 2 step 1300 loss 0.06204
INFO:name:epoch 2 step 1400 loss 0.05386
INFO:name:epoch 2 step 1500 loss 0.06078
INFO:name:epoch 2 step 1600 loss 0.05818
INFO:name:epoch 2 step 1700 loss 0.06801
INFO:name:epoch 2 step 1800 loss 0.06057
INFO:name:epoch 2 step 1900 loss 0.06729
INFO:name:epoch 2 step 2000 loss 0.05681
INFO:name:epoch 2 step 2100 loss 0.05432
INFO:name:epoch 2 step 2200 loss 0.05798
INFO:name:epoch 2 step 2300 loss 0.05429
INFO:name:epoch 2 step 2400 loss 0.06263
INFO:name:epoch 2 step 2500 loss 0.05806
INFO:name:epoch 2 step 2600 loss 0.04975
INFO:name:epoch 2 step 2700 loss 0.05465
INFO:name:epoch 2 step 2800 loss 0.05863
INFO:name:epoch 2 step 2900 loss 0.06315
INFO:name:epoch 2 step 3000 loss 0.06136
INFO:name:epoch 2 step 3100 loss 0.05123
INFO:name:epoch 2 step 3200 loss 0.06098
INFO:name:epoch 2 step 3300 loss 0.05448
INFO:name:epoch 2 step 3400 loss 0.056
INFO:name:epoch 2 step 3500 loss 0.06556
INFO:name:epoch 2 step 3600 loss 0.06536
INFO:name:epoch 2 step 3700 loss 0.05123
INFO:name:epoch 2 step 3800 loss 0.05944
INFO:name:epoch 2 step 3900 loss 0.06926
INFO:name:epoch 2 step 4000 loss 0.05221
INFO:name:epoch 2 step 4100 loss 0.06647
INFO:name:epoch 2 step 4200 loss 0.06114
INFO:name:epoch 2 step 4300 loss 0.06396
INFO:name:epoch 2 step 4400 loss 0.06179
INFO:name:epoch 2 step 4500 loss 0.05673
INFO:name:epoch 2 step 4600 loss 0.06177
INFO:name:epoch 2 step 4700 loss 0.05843
INFO:name:epoch 2 step 4800 loss 0.04631
INFO:name:epoch 2 step 4900 loss 0.06353
INFO:name:epoch 2 step 5000 loss 0.0568
INFO:name:epoch 2 step 5100 loss 0.05822
INFO:name:epoch 2 step 5200 loss 0.05954
INFO:name:epoch 2 step 5300 loss 0.07149
INFO:name:epoch 2 step 5400 loss 0.05723
INFO:name:epoch 2 step 5500 loss 0.05413
INFO:name:epoch 2 step 5600 loss 0.05349
INFO:name:epoch 2 step 5700 loss 0.06592
INFO:name:epoch 2 step 5800 loss 0.06128
INFO:name:epoch 2 step 5900 loss 0.05821
INFO:name:epoch 2 step 6000 loss 0.05481
INFO:name:epoch 2 step 6100 loss 0.0479
INFO:name:epoch 2 step 6200 loss 0.0668
INFO:name:epoch 2 step 6300 loss 0.05552
INFO:name:epoch 2 step 6400 loss 0.073
INFO:name:epoch 2 step 6500 loss 0.05475
INFO:name:epoch 2 step 6600 loss 0.05369
INFO:name:epoch 2 step 6700 loss 0.04973
INFO:name:epoch 2 step 6800 loss 0.06964
INFO:name:epoch 2 step 6900 loss 0.06837
INFO:name:epoch 2 step 7000 loss 0.06219
INFO:name:epoch 2 step 7100 loss 0.06475
INFO:name:epoch 2 step 7200 loss 0.05112
INFO:name:epoch 2 step 7300 loss 0.05504
INFO:name:epoch 2 step 7400 loss 0.05398
INFO:name:epoch 2 step 7500 loss 0.05883
INFO:name:epoch 2 step 7600 loss 0.0534
INFO:name:epoch 2 step 7700 loss 0.0679
INFO:name:epoch 2 step 7800 loss 0.05602
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3598
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3598
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3025
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.0485
INFO:name:epoch 3 step 200 loss 0.0472
INFO:name:epoch 3 step 300 loss 0.04804
INFO:name:epoch 3 step 400 loss 0.04367
INFO:name:epoch 3 step 500 loss 0.03935
INFO:name:epoch 3 step 600 loss 0.04434
INFO:name:epoch 3 step 700 loss 0.04527
INFO:name:epoch 3 step 800 loss 0.04701
INFO:name:epoch 3 step 900 loss 0.04284
INFO:name:epoch 3 step 1000 loss 0.04282
INFO:name:epoch 3 step 1100 loss 0.05024
INFO:name:epoch 3 step 1200 loss 0.04162
INFO:name:epoch 3 step 1300 loss 0.0442
INFO:name:epoch 3 step 1400 loss 0.04453
INFO:name:epoch 3 step 1500 loss 0.05066
INFO:name:epoch 3 step 1600 loss 0.04578
INFO:name:epoch 3 step 1700 loss 0.04663
INFO:name:epoch 3 step 1800 loss 0.03745
INFO:name:epoch 3 step 1900 loss 0.04825
INFO:name:epoch 3 step 2000 loss 0.04396
INFO:name:epoch 3 step 2100 loss 0.04398
INFO:name:epoch 3 step 2200 loss 0.04307
INFO:name:epoch 3 step 2300 loss 0.04028
INFO:name:epoch 3 step 2400 loss 0.04646
INFO:name:epoch 3 step 2500 loss 0.04759
INFO:name:epoch 3 step 2600 loss 0.04468
INFO:name:epoch 3 step 2700 loss 0.0443
INFO:name:epoch 3 step 2800 loss 0.04017
INFO:name:epoch 3 step 2900 loss 0.0459
INFO:name:epoch 3 step 3000 loss 0.03783
INFO:name:epoch 3 step 3100 loss 0.0462
INFO:name:epoch 3 step 3200 loss 0.04511
INFO:name:epoch 3 step 3300 loss 0.04255
INFO:name:epoch 3 step 3400 loss 0.03334
INFO:name:epoch 3 step 3500 loss 0.04956
INFO:name:epoch 3 step 3600 loss 0.04343
INFO:name:epoch 3 step 3700 loss 0.03932
INFO:name:epoch 3 step 3800 loss 0.04247
INFO:name:epoch 3 step 3900 loss 0.0463
INFO:name:epoch 3 step 4000 loss 0.04714
INFO:name:epoch 3 step 4100 loss 0.05495
INFO:name:epoch 3 step 4200 loss 0.04225
INFO:name:epoch 3 step 4300 loss 0.04508
INFO:name:epoch 3 step 4400 loss 0.04329
INFO:name:epoch 3 step 4500 loss 0.05266
INFO:name:epoch 3 step 4600 loss 0.05384
INFO:name:epoch 3 step 4700 loss 0.04013
INFO:name:epoch 3 step 4800 loss 0.04367
INFO:name:epoch 3 step 4900 loss 0.05031
INFO:name:epoch 3 step 5000 loss 0.0376
INFO:name:epoch 3 step 5100 loss 0.04057
INFO:name:epoch 3 step 5200 loss 0.0433
INFO:name:epoch 3 step 5300 loss 0.04701
INFO:name:epoch 3 step 5400 loss 0.05069
INFO:name:epoch 3 step 5500 loss 0.04772
INFO:name:epoch 3 step 5600 loss 0.04183
INFO:name:epoch 3 step 5700 loss 0.04295
INFO:name:epoch 3 step 5800 loss 0.04965
INFO:name:epoch 3 step 5900 loss 0.04271
INFO:name:epoch 3 step 6000 loss 0.04528
INFO:name:epoch 3 step 6100 loss 0.03768
INFO:name:epoch 3 step 6200 loss 0.04875
INFO:name:epoch 3 step 6300 loss 0.04565
INFO:name:epoch 3 step 6400 loss 0.05
INFO:name:epoch 3 step 6500 loss 0.05155
INFO:name:epoch 3 step 6600 loss 0.04588
INFO:name:epoch 3 step 6700 loss 0.0437
INFO:name:epoch 3 step 6800 loss 0.04966
INFO:name:epoch 3 step 6900 loss 0.04254
INFO:name:epoch 3 step 7000 loss 0.0452
INFO:name:epoch 3 step 7100 loss 0.04296
INFO:name:epoch 3 step 7200 loss 0.04631
INFO:name:epoch 3 step 7300 loss 0.04269
INFO:name:epoch 3 step 7400 loss 0.04726
INFO:name:epoch 3 step 7500 loss 0.04205
INFO:name:epoch 3 step 7600 loss 0.04054
INFO:name:epoch 3 step 7700 loss 0.04006
INFO:name:epoch 3 step 7800 loss 0.04502
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.362
INFO:name:  ********************
INFO:name:  Best eval mrr:0.362
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3057
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 4 step 100 loss 0.03499
INFO:name:epoch 4 step 200 loss 0.03547
INFO:name:epoch 4 step 300 loss 0.03272
INFO:name:epoch 4 step 400 loss 0.03364
INFO:name:epoch 4 step 500 loss 0.03662
INFO:name:epoch 4 step 600 loss 0.03434
INFO:name:epoch 4 step 700 loss 0.03455
INFO:name:epoch 4 step 800 loss 0.03405
INFO:name:epoch 4 step 900 loss 0.0328
INFO:name:epoch 4 step 1000 loss 0.03097
INFO:name:epoch 4 step 1100 loss 0.0312
INFO:name:epoch 4 step 1200 loss 0.03817
INFO:name:epoch 4 step 1300 loss 0.03496
INFO:name:epoch 4 step 1400 loss 0.03289
INFO:name:epoch 4 step 1500 loss 0.026
INFO:name:epoch 4 step 1600 loss 0.04109
INFO:name:epoch 4 step 1700 loss 0.03192
INFO:name:epoch 4 step 1800 loss 0.03304
INFO:name:epoch 4 step 1900 loss 0.02978
INFO:name:epoch 4 step 2000 loss 0.03348
INFO:name:epoch 4 step 2100 loss 0.03865
INFO:name:epoch 4 step 2200 loss 0.03432
INFO:name:epoch 4 step 2300 loss 0.0367
INFO:name:epoch 4 step 2400 loss 0.03068
INFO:name:epoch 4 step 2500 loss 0.03764
INFO:name:epoch 4 step 2600 loss 0.02892
INFO:name:epoch 4 step 2700 loss 0.0304
INFO:name:epoch 4 step 2800 loss 0.03297
INFO:name:epoch 4 step 2900 loss 0.02805
INFO:name:epoch 4 step 3000 loss 0.03259
INFO:name:epoch 4 step 3100 loss 0.04075
INFO:name:epoch 4 step 3200 loss 0.03879
INFO:name:epoch 4 step 3300 loss 0.03592
INFO:name:epoch 4 step 3400 loss 0.0402
INFO:name:epoch 4 step 3500 loss 0.03679
INFO:name:epoch 4 step 3600 loss 0.03515
INFO:name:epoch 4 step 3700 loss 0.03598
INFO:name:epoch 4 step 3800 loss 0.037
INFO:name:epoch 4 step 3900 loss 0.03326
INFO:name:epoch 4 step 4000 loss 0.03541
INFO:name:epoch 4 step 4100 loss 0.04058
INFO:name:epoch 4 step 4200 loss 0.03764
INFO:name:epoch 4 step 4300 loss 0.03893
INFO:name:epoch 4 step 4400 loss 0.02854
INFO:name:epoch 4 step 4500 loss 0.03634
INFO:name:epoch 4 step 4600 loss 0.03513
INFO:name:epoch 4 step 4700 loss 0.03261
INFO:name:epoch 4 step 4800 loss 0.03263
INFO:name:epoch 4 step 4900 loss 0.03221
INFO:name:epoch 4 step 5000 loss 0.0362
INFO:name:epoch 4 step 5100 loss 0.03446
INFO:name:epoch 4 step 5200 loss 0.02959
INFO:name:epoch 4 step 5300 loss 0.03494
INFO:name:epoch 4 step 5400 loss 0.03923
INFO:name:epoch 4 step 5500 loss 0.03005
INFO:name:epoch 4 step 5600 loss 0.04047
INFO:name:epoch 4 step 5700 loss 0.03693
INFO:name:epoch 4 step 5800 loss 0.04752
INFO:name:epoch 4 step 5900 loss 0.03184
INFO:name:epoch 4 step 6000 loss 0.02728
INFO:name:epoch 4 step 6100 loss 0.03325
INFO:name:epoch 4 step 6200 loss 0.03324
INFO:name:epoch 4 step 6300 loss 0.03983
INFO:name:epoch 4 step 6400 loss 0.02927
INFO:name:epoch 4 step 6500 loss 0.03587
INFO:name:epoch 4 step 6600 loss 0.03824
INFO:name:epoch 4 step 6700 loss 0.03096
INFO:name:epoch 4 step 6800 loss 0.04713
INFO:name:epoch 4 step 6900 loss 0.0378
INFO:name:epoch 4 step 7000 loss 0.0343
INFO:name:epoch 4 step 7100 loss 0.03408
INFO:name:epoch 4 step 7200 loss 0.03502
INFO:name:epoch 4 step 7300 loss 0.04227
INFO:name:epoch 4 step 7400 loss 0.03835
INFO:name:epoch 4 step 7500 loss 0.03711
INFO:name:epoch 4 step 7600 loss 0.03526
INFO:name:epoch 4 step 7700 loss 0.04372
INFO:name:epoch 4 step 7800 loss 0.03887
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3456
INFO:name:epoch 5 step 100 loss 0.03105
INFO:name:epoch 5 step 200 loss 0.03008
INFO:name:epoch 5 step 300 loss 0.02764
INFO:name:epoch 5 step 400 loss 0.03116
INFO:name:epoch 5 step 500 loss 0.02191
INFO:name:epoch 5 step 600 loss 0.03301
INFO:name:epoch 5 step 700 loss 0.02822
INFO:name:epoch 5 step 800 loss 0.02838
INFO:name:epoch 5 step 900 loss 0.02648
INFO:name:epoch 5 step 1000 loss 0.02753
INFO:name:epoch 5 step 1100 loss 0.02443
INFO:name:epoch 5 step 1200 loss 0.02851
INFO:name:epoch 5 step 1300 loss 0.02143
INFO:name:epoch 5 step 1400 loss 0.02903
INFO:name:epoch 5 step 1500 loss 0.02776
INFO:name:epoch 5 step 1600 loss 0.02857
INFO:name:epoch 5 step 1700 loss 0.02425
INFO:name:epoch 5 step 1800 loss 0.0263
INFO:name:epoch 5 step 1900 loss 0.02887
INFO:name:epoch 5 step 2000 loss 0.02511
INFO:name:epoch 5 step 2100 loss 0.02939
INFO:name:epoch 5 step 2200 loss 0.02509
INFO:name:epoch 5 step 2300 loss 0.02303
INFO:name:epoch 5 step 2400 loss 0.02964
INFO:name:epoch 5 step 2500 loss 0.02471
INFO:name:epoch 5 step 2600 loss 0.02332
INFO:name:epoch 5 step 2700 loss 0.0265
INFO:name:epoch 5 step 2800 loss 0.02728
INFO:name:epoch 5 step 2900 loss 0.02551
INFO:name:epoch 5 step 3000 loss 0.02787
INFO:name:epoch 5 step 3100 loss 0.02769
INFO:name:epoch 5 step 3200 loss 0.0272
INFO:name:epoch 5 step 3300 loss 0.02931
INFO:name:epoch 5 step 3400 loss 0.02173
INFO:name:epoch 5 step 3500 loss 0.02414
INFO:name:epoch 5 step 3600 loss 0.02781
INFO:name:epoch 5 step 3700 loss 0.02512
INFO:name:epoch 5 step 3800 loss 0.02961
INFO:name:epoch 5 step 3900 loss 0.02853
INFO:name:epoch 5 step 4000 loss 0.02839
INFO:name:epoch 5 step 4100 loss 0.02795
INFO:name:epoch 5 step 4200 loss 0.02718
INFO:name:epoch 5 step 4300 loss 0.02894
INFO:name:epoch 5 step 4400 loss 0.02679
INFO:name:epoch 5 step 4500 loss 0.02946
INFO:name:epoch 5 step 4600 loss 0.02672
INFO:name:epoch 5 step 4700 loss 0.02841
INFO:name:epoch 5 step 4800 loss 0.02499
INFO:name:epoch 5 step 4900 loss 0.02675
INFO:name:epoch 5 step 5000 loss 0.02479
INFO:name:epoch 5 step 5100 loss 0.02659
INFO:name:epoch 5 step 5200 loss 0.02941
INFO:name:epoch 5 step 5300 loss 0.02706
INFO:name:epoch 5 step 5400 loss 0.03059
INFO:name:epoch 5 step 5500 loss 0.02845
INFO:name:epoch 5 step 5600 loss 0.02775
INFO:name:epoch 5 step 5700 loss 0.02759
INFO:name:epoch 5 step 5800 loss 0.02739
INFO:name:epoch 5 step 5900 loss 0.02497
INFO:name:epoch 5 step 6000 loss 0.02313
INFO:name:epoch 5 step 6100 loss 0.02902
INFO:name:epoch 5 step 6200 loss 0.0283
INFO:name:epoch 5 step 6300 loss 0.0291
INFO:name:epoch 5 step 6400 loss 0.02961
INFO:name:epoch 5 step 6500 loss 0.03165
INFO:name:epoch 5 step 6600 loss 0.02432
INFO:name:epoch 5 step 6700 loss 0.02581
INFO:name:epoch 5 step 6800 loss 0.03178
INFO:name:epoch 5 step 6900 loss 0.02585
INFO:name:epoch 5 step 7000 loss 0.02746
INFO:name:epoch 5 step 7100 loss 0.02749
INFO:name:epoch 5 step 7200 loss 0.0265
INFO:name:epoch 5 step 7300 loss 0.02499
INFO:name:epoch 5 step 7400 loss 0.03189
INFO:name:epoch 5 step 7500 loss 0.02802
INFO:name:epoch 5 step 7600 loss 0.03013
INFO:name:epoch 5 step 7700 loss 0.02864
INFO:name:epoch 5 step 7800 loss 0.03456
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3716
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3716
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3099
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 6 step 100 loss 0.02192
INFO:name:epoch 6 step 200 loss 0.02434
INFO:name:epoch 6 step 300 loss 0.02158
INFO:name:epoch 6 step 400 loss 0.01614
INFO:name:epoch 6 step 500 loss 0.01844
INFO:name:epoch 6 step 600 loss 0.02263
INFO:name:epoch 6 step 700 loss 0.01803
INFO:name:epoch 6 step 800 loss 0.0225
INFO:name:epoch 6 step 900 loss 0.02191
INFO:name:epoch 6 step 1000 loss 0.02746
INFO:name:epoch 6 step 1100 loss 0.02342
INFO:name:epoch 6 step 1200 loss 0.02356
INFO:name:epoch 6 step 1300 loss 0.0221
INFO:name:epoch 6 step 1400 loss 0.02502
INFO:name:epoch 6 step 1500 loss 0.02057
INFO:name:epoch 6 step 1600 loss 0.02454
INFO:name:epoch 6 step 1700 loss 0.02278
INFO:name:epoch 6 step 1800 loss 0.0211
INFO:name:epoch 6 step 1900 loss 0.02277
INFO:name:epoch 6 step 2000 loss 0.02487
INFO:name:epoch 6 step 2100 loss 0.02365
INFO:name:epoch 6 step 2200 loss 0.01915
INFO:name:epoch 6 step 2300 loss 0.02172
INFO:name:epoch 6 step 2400 loss 0.02468
INFO:name:epoch 6 step 2500 loss 0.02064
INFO:name:epoch 6 step 2600 loss 0.02166
INFO:name:epoch 6 step 2700 loss 0.02268
INFO:name:epoch 6 step 2800 loss 0.02277
INFO:name:epoch 6 step 2900 loss 0.02348
INFO:name:epoch 6 step 3000 loss 0.02287
INFO:name:epoch 6 step 3100 loss 0.02866
INFO:name:epoch 6 step 3200 loss 0.02592
INFO:name:epoch 6 step 3300 loss 0.02196
INFO:name:epoch 6 step 3400 loss 0.02363
INFO:name:epoch 6 step 3500 loss 0.02788
INFO:name:epoch 6 step 3600 loss 0.02117
INFO:name:epoch 6 step 3700 loss 0.02142
INFO:name:epoch 6 step 3800 loss 0.02032
INFO:name:epoch 6 step 3900 loss 0.02119
INFO:name:epoch 6 step 4000 loss 0.02229
INFO:name:epoch 6 step 4100 loss 0.02378
INFO:name:epoch 6 step 4200 loss 0.01915
INFO:name:epoch 6 step 4300 loss 0.02094
INFO:name:epoch 6 step 4400 loss 0.01761
INFO:name:epoch 6 step 4500 loss 0.01984
INFO:name:epoch 6 step 4600 loss 0.0218
INFO:name:epoch 6 step 4700 loss 0.02658
INFO:name:epoch 6 step 4800 loss 0.0246
INFO:name:epoch 6 step 4900 loss 0.02197
INFO:name:epoch 6 step 5000 loss 0.02119
INFO:name:epoch 6 step 5100 loss 0.02091
INFO:name:epoch 6 step 5200 loss 0.02632
INFO:name:epoch 6 step 5300 loss 0.02232
INFO:name:epoch 6 step 5400 loss 0.02635
INFO:name:epoch 6 step 5500 loss 0.02039
INFO:name:epoch 6 step 5600 loss 0.02418
INFO:name:epoch 6 step 5700 loss 0.02064
INFO:name:epoch 6 step 5800 loss 0.02205
INFO:name:epoch 6 step 5900 loss 0.02619
INFO:name:epoch 6 step 6000 loss 0.02262
INFO:name:epoch 6 step 6100 loss 0.01743
INFO:name:epoch 6 step 6200 loss 0.02407
INFO:name:epoch 6 step 6300 loss 0.02096
INFO:name:epoch 6 step 6400 loss 0.02447
INFO:name:epoch 6 step 6500 loss 0.01997
INFO:name:epoch 6 step 6600 loss 0.02203
INFO:name:epoch 6 step 6700 loss 0.02265
INFO:name:epoch 6 step 6800 loss 0.0215
INFO:name:epoch 6 step 6900 loss 0.02456
INFO:name:epoch 6 step 7000 loss 0.02379
INFO:name:epoch 6 step 7100 loss 0.02036
INFO:name:epoch 6 step 7200 loss 0.02091
INFO:name:epoch 6 step 7300 loss 0.02472
INFO:name:epoch 6 step 7400 loss 0.02516
INFO:name:epoch 6 step 7500 loss 0.02136
INFO:name:epoch 6 step 7600 loss 0.02398
INFO:name:epoch 6 step 7700 loss 0.02147
INFO:name:epoch 6 step 7800 loss 0.02663
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3622
INFO:name:epoch 7 step 100 loss 0.01911
INFO:name:epoch 7 step 200 loss 0.02217
INFO:name:epoch 7 step 300 loss 0.01901
INFO:name:epoch 7 step 400 loss 0.0157
INFO:name:epoch 7 step 500 loss 0.01991
INFO:name:epoch 7 step 600 loss 0.01546
INFO:name:epoch 7 step 700 loss 0.01827
INFO:name:epoch 7 step 800 loss 0.01835
INFO:name:epoch 7 step 900 loss 0.01746
INFO:name:epoch 7 step 1000 loss 0.01929
INFO:name:epoch 7 step 1100 loss 0.01978
INFO:name:epoch 7 step 1200 loss 0.01526
INFO:name:epoch 7 step 1300 loss 0.0204
INFO:name:epoch 7 step 1400 loss 0.01613
INFO:name:epoch 7 step 1500 loss 0.01796
INFO:name:epoch 7 step 1600 loss 0.01652
INFO:name:epoch 7 step 1700 loss 0.01937
INFO:name:epoch 7 step 1800 loss 0.01604
INFO:name:epoch 7 step 1900 loss 0.01776
INFO:name:epoch 7 step 2000 loss 0.01746
INFO:name:epoch 7 step 2100 loss 0.01711
INFO:name:epoch 7 step 2200 loss 0.01883
INFO:name:epoch 7 step 2300 loss 0.01816
INFO:name:epoch 7 step 2400 loss 0.01738
INFO:name:epoch 7 step 2500 loss 0.02147
INFO:name:epoch 7 step 2600 loss 0.01838
INFO:name:epoch 7 step 2700 loss 0.01726
INFO:name:epoch 7 step 2800 loss 0.01894
INFO:name:epoch 7 step 2900 loss 0.01689
INFO:name:epoch 7 step 3000 loss 0.02012
INFO:name:epoch 7 step 3100 loss 0.01796
INFO:name:epoch 7 step 3200 loss 0.01994
INFO:name:epoch 7 step 3300 loss 0.01499
INFO:name:epoch 7 step 3400 loss 0.01961
INFO:name:epoch 7 step 3500 loss 0.01976
INFO:name:epoch 7 step 3600 loss 0.01395
INFO:name:epoch 7 step 3700 loss 0.01684
INFO:name:epoch 7 step 3800 loss 0.0185
INFO:name:epoch 7 step 3900 loss 0.01709
INFO:name:epoch 7 step 4000 loss 0.01958
INFO:name:epoch 7 step 4100 loss 0.01968
INFO:name:epoch 7 step 4200 loss 0.01621
INFO:name:epoch 7 step 4300 loss 0.01975
INFO:name:epoch 7 step 4400 loss 0.02277
INFO:name:epoch 7 step 4500 loss 0.0156
INFO:name:epoch 7 step 4600 loss 0.01917
INFO:name:epoch 7 step 4700 loss 0.0143
INFO:name:epoch 7 step 4800 loss 0.01894
INFO:name:epoch 7 step 4900 loss 0.01908
INFO:name:epoch 7 step 5000 loss 0.02057
INFO:name:epoch 7 step 5100 loss 0.02029
INFO:name:epoch 7 step 5200 loss 0.01806
INFO:name:epoch 7 step 5300 loss 0.01705
INFO:name:epoch 7 step 5400 loss 0.02079
INFO:name:epoch 7 step 5500 loss 0.01658
INFO:name:epoch 7 step 5600 loss 0.02409
INFO:name:epoch 7 step 5700 loss 0.01942
INFO:name:epoch 7 step 5800 loss 0.01717
INFO:name:epoch 7 step 5900 loss 0.01734
INFO:name:epoch 7 step 6000 loss 0.01892
INFO:name:epoch 7 step 6100 loss 0.02041
INFO:name:epoch 7 step 6200 loss 0.01586
INFO:name:epoch 7 step 6300 loss 0.01729
INFO:name:epoch 7 step 6400 loss 0.01817
INFO:name:epoch 7 step 6500 loss 0.01641
INFO:name:epoch 7 step 6600 loss 0.02021
INFO:name:epoch 7 step 6700 loss 0.01608
INFO:name:epoch 7 step 6800 loss 0.01495
INFO:name:epoch 7 step 6900 loss 0.01862
INFO:name:epoch 7 step 7000 loss 0.02035
INFO:name:epoch 7 step 7100 loss 0.01809
INFO:name:epoch 7 step 7200 loss 0.02363
INFO:name:epoch 7 step 7300 loss 0.01814
INFO:name:epoch 7 step 7400 loss 0.01754
INFO:name:epoch 7 step 7500 loss 0.0172
INFO:name:epoch 7 step 7600 loss 0.01922
INFO:name:epoch 7 step 7700 loss 0.01667
INFO:name:epoch 7 step 7800 loss 0.01752
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.365
INFO:name:epoch 8 step 100 loss 0.01861
INFO:name:epoch 8 step 200 loss 0.01382
INFO:name:epoch 8 step 300 loss 0.01464
INFO:name:epoch 8 step 400 loss 0.01674
INFO:name:epoch 8 step 500 loss 0.01691
INFO:name:epoch 8 step 600 loss 0.01628
INFO:name:epoch 8 step 700 loss 0.01973
INFO:name:epoch 8 step 800 loss 0.01494
INFO:name:epoch 8 step 900 loss 0.0169
INFO:name:epoch 8 step 1000 loss 0.0167
INFO:name:epoch 8 step 1100 loss 0.01516
INFO:name:epoch 8 step 1200 loss 0.01527
INFO:name:epoch 8 step 1300 loss 0.01821
INFO:name:epoch 8 step 1400 loss 0.01669
INFO:name:epoch 8 step 1500 loss 0.01696
INFO:name:epoch 8 step 1600 loss 0.01623
INFO:name:epoch 8 step 1700 loss 0.02012
INFO:name:epoch 8 step 1800 loss 0.01263
INFO:name:epoch 8 step 1900 loss 0.01652
INFO:name:epoch 8 step 2000 loss 0.01698
INFO:name:epoch 8 step 2100 loss 0.0173
INFO:name:epoch 8 step 2200 loss 0.01426
INFO:name:epoch 8 step 2300 loss 0.01877
INFO:name:epoch 8 step 2400 loss 0.01406
INFO:name:epoch 8 step 2500 loss 0.01619
INFO:name:epoch 8 step 2600 loss 0.01631
INFO:name:epoch 8 step 2700 loss 0.01411
INFO:name:epoch 8 step 2800 loss 0.01467
INFO:name:epoch 8 step 2900 loss 0.01601
INFO:name:epoch 8 step 3000 loss 0.01654
INFO:name:epoch 8 step 3100 loss 0.01793
INFO:name:epoch 8 step 3200 loss 0.01662
INFO:name:epoch 8 step 3300 loss 0.01395
INFO:name:epoch 8 step 3400 loss 0.01568
INFO:name:epoch 8 step 3500 loss 0.01693
INFO:name:epoch 8 step 3600 loss 0.01675
INFO:name:epoch 8 step 3700 loss 0.01729
INFO:name:epoch 8 step 3800 loss 0.01604
INFO:name:epoch 8 step 3900 loss 0.014
INFO:name:epoch 8 step 4000 loss 0.01971
INFO:name:epoch 8 step 4100 loss 0.01344
INFO:name:epoch 8 step 4200 loss 0.01342
INFO:name:epoch 8 step 4300 loss 0.01773
INFO:name:epoch 8 step 4400 loss 0.01494
INFO:name:epoch 8 step 4500 loss 0.01478
INFO:name:epoch 8 step 4600 loss 0.0159
INFO:name:epoch 8 step 4700 loss 0.0156
INFO:name:epoch 8 step 4800 loss 0.01544
INFO:name:epoch 8 step 4900 loss 0.01659
INFO:name:epoch 8 step 5000 loss 0.01439
INFO:name:epoch 8 step 5100 loss 0.01418
INFO:name:epoch 8 step 5200 loss 0.0175
INFO:name:epoch 8 step 5300 loss 0.0145
INFO:name:epoch 8 step 5400 loss 0.01499
INFO:name:epoch 8 step 5500 loss 0.01618
INFO:name:epoch 8 step 5600 loss 0.01448
INFO:name:epoch 8 step 5700 loss 0.01561
INFO:name:epoch 8 step 5800 loss 0.01981
INFO:name:epoch 8 step 5900 loss 0.01342
INFO:name:epoch 8 step 6000 loss 0.01853
INFO:name:epoch 8 step 6100 loss 0.01766
INFO:name:epoch 8 step 6200 loss 0.01883
INFO:name:epoch 8 step 6300 loss 0.01573
INFO:name:epoch 8 step 6400 loss 0.01297
INFO:name:epoch 8 step 6500 loss 0.01435
INFO:name:epoch 8 step 6600 loss 0.01356
INFO:name:epoch 8 step 6700 loss 0.02001
INFO:name:epoch 8 step 6800 loss 0.0146
INFO:name:epoch 8 step 6900 loss 0.01725
INFO:name:epoch 8 step 7000 loss 0.01456
INFO:name:epoch 8 step 7100 loss 0.01717
INFO:name:epoch 8 step 7200 loss 0.01355
INFO:name:epoch 8 step 7300 loss 0.01649
INFO:name:epoch 8 step 7400 loss 0.01486
INFO:name:epoch 8 step 7500 loss 0.01637
INFO:name:epoch 8 step 7600 loss 0.01147
INFO:name:epoch 8 step 7700 loss 0.01417
INFO:name:epoch 8 step 7800 loss 0.01891
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3653
INFO:name:epoch 9 step 100 loss 0.01277
INFO:name:epoch 9 step 200 loss 0.01587
INFO:name:epoch 9 step 300 loss 0.01457
INFO:name:epoch 9 step 400 loss 0.01431
INFO:name:epoch 9 step 500 loss 0.01462
INFO:name:epoch 9 step 600 loss 0.01532
INFO:name:epoch 9 step 700 loss 0.01314
INFO:name:epoch 9 step 800 loss 0.0171
INFO:name:epoch 9 step 900 loss 0.01494
INFO:name:epoch 9 step 1000 loss 0.01446
INFO:name:epoch 9 step 1100 loss 0.01316
INFO:name:epoch 9 step 1200 loss 0.01215
INFO:name:epoch 9 step 1300 loss 0.01543
INFO:name:epoch 9 step 1400 loss 0.01335
INFO:name:epoch 9 step 1500 loss 0.01342
INFO:name:epoch 9 step 1600 loss 0.01243
INFO:name:epoch 9 step 1700 loss 0.01352
INFO:name:epoch 9 step 1800 loss 0.01194
INFO:name:epoch 9 step 1900 loss 0.01783
INFO:name:epoch 9 step 2000 loss 0.01656
INFO:name:epoch 9 step 2100 loss 0.01566
INFO:name:epoch 9 step 2200 loss 0.01593
INFO:name:epoch 9 step 2300 loss 0.01089
INFO:name:epoch 9 step 2400 loss 0.01271
INFO:name:epoch 9 step 2500 loss 0.01491
INFO:name:epoch 9 step 2600 loss 0.01442
INFO:name:epoch 9 step 2700 loss 0.01343
INFO:name:epoch 9 step 2800 loss 0.01395
INFO:name:epoch 9 step 2900 loss 0.01629
INFO:name:epoch 9 step 3000 loss 0.01483
INFO:name:epoch 9 step 3100 loss 0.0152
INFO:name:epoch 9 step 3200 loss 0.01455
INFO:name:epoch 9 step 3300 loss 0.01584
INFO:name:epoch 9 step 3400 loss 0.01625
INFO:name:epoch 9 step 3500 loss 0.01387
INFO:name:epoch 9 step 3600 loss 0.01195
INFO:name:epoch 9 step 3700 loss 0.01355
INFO:name:epoch 9 step 3800 loss 0.0146
INFO:name:epoch 9 step 3900 loss 0.01343
INFO:name:epoch 9 step 4000 loss 0.01755
INFO:name:epoch 9 step 4100 loss 0.01377
INFO:name:epoch 9 step 4200 loss 0.01445
INFO:name:epoch 9 step 4300 loss 0.01527
INFO:name:epoch 9 step 4400 loss 0.0141
INFO:name:epoch 9 step 4500 loss 0.01287
INFO:name:epoch 9 step 4600 loss 0.01473
INFO:name:epoch 9 step 4700 loss 0.01391
INFO:name:epoch 9 step 4800 loss 0.01165
INFO:name:epoch 9 step 4900 loss 0.01322
INFO:name:epoch 9 step 5000 loss 0.01575
INFO:name:epoch 9 step 5100 loss 0.01635
INFO:name:epoch 9 step 5200 loss 0.01121
INFO:name:epoch 9 step 5300 loss 0.01539
INFO:name:epoch 9 step 5400 loss 0.01734
INFO:name:epoch 9 step 5500 loss 0.01375
INFO:name:epoch 9 step 5600 loss 0.0148
INFO:name:epoch 9 step 5700 loss 0.0117
INFO:name:epoch 9 step 5800 loss 0.01667
INFO:name:epoch 9 step 5900 loss 0.0142
INFO:name:epoch 9 step 6000 loss 0.01687
INFO:name:epoch 9 step 6100 loss 0.01229
INFO:name:epoch 9 step 6200 loss 0.01307
INFO:name:epoch 9 step 6300 loss 0.01439
INFO:name:epoch 9 step 6400 loss 0.01355
INFO:name:epoch 9 step 6500 loss 0.01617
INFO:name:epoch 9 step 6600 loss 0.0111
INFO:name:epoch 9 step 6700 loss 0.01368
INFO:name:epoch 9 step 6800 loss 0.01349
INFO:name:epoch 9 step 6900 loss 0.01144
INFO:name:epoch 9 step 7000 loss 0.0146
INFO:name:epoch 9 step 7100 loss 0.01205
INFO:name:epoch 9 step 7200 loss 0.01288
INFO:name:epoch 9 step 7300 loss 0.01298
INFO:name:epoch 9 step 7400 loss 0.01982
INFO:name:epoch 9 step 7500 loss 0.01271
INFO:name:epoch 9 step 7600 loss 0.01699
INFO:name:epoch 9 step 7700 loss 0.01511
INFO:name:epoch 9 step 7800 loss 0.0141
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3635
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:[0, 0, {'insert_modules': ('layer.0.SelfAttention', 'layer.0'), 'bottleneck_dim': (32, 128), 'non_linearity': 'gelu', 'dropout_rate': 0.2, 'normalization': 'layer_norm', 'skip_connection': True}, 0, {'insert_modules': ('layer.1.DenseReluDense', 'layer.1'), 'bottleneck_dim': (128, 128), 'non_linearity': 'relu', 'dropout_rate': 0.15, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('layer.0', 'layer.1'), 'bottleneck_dim': (64, 256), 'non_linearity': 'gelu_new', 'dropout_rate': 0.1, 'normalization': 'layer_norm', 'skip_connection': True}, 0, {'insert_modules': ('layer.0.SelfAttention', 'layer.1', 'layer.0'), 'bottleneck_dim': (16, 128, 128), 'non_linearity': 'gelu', 'dropout_rate': 0.3, 'normalization': 'layer_norm', 'skip_connection': True}, 0, 0, {'insert_modules': ('layer.0.SelfAttention', 'layer.0'), 'bottleneck_dim': (32, 128), 'non_linearity': 'gelu_new', 'dropout_rate': 0.0, 'normalization': 'layer_norm', 'skip_connection': True}, {'insert_modules': ('layer.0', 'layer.1'), 'bottleneck_dim': (64, 128), 'non_linearity': 'relu', 'dropout_rate': 0.25, 'normalization': 'layer_norm', 'skip_connection': True}]
[INFO|(OpenDelta)basemodel:700]2025-01-13 04:21:01,074 >> Trainable Ratio: 2103024/224985072=0.934739%
[INFO|(OpenDelta)basemodel:702]2025-01-13 04:21:01,074 >> Delta Parameter Ratio: 2103024/224985072=0.934739%
[INFO|(OpenDelta)basemodel:704]2025-01-13 04:21:01,074 >> Static Memory 0.87 GB, Max Memory 8.74 GB
INFO:name:1.8800000000000001
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 10
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 78700
train results ([0.19106658302310245, 0.08109544676682987, 0.0590392346224976, 0.044674572946587776, 0.035182375999886766, 0.02732473371145576, 0.02255183697573567, 0.01829476628072082, 0.01593712149963778, 0.014262255099560625], [0.32479733122878207, 0.3582373262480077, 0.3598199517476013, 0.36200566751382834, 0.34562251684520917, 0.37161760199689314, 0.36218689934515547, 0.36501057332572434, 0.36529611884027796, 0.36352870432686135])
INFO:name:epoch 0 step 100 loss 2.39317
INFO:name:epoch 0 step 200 loss 0.47775
INFO:name:epoch 0 step 300 loss 0.31267
INFO:name:epoch 0 step 400 loss 0.26998
INFO:name:epoch 0 step 500 loss 0.25044
INFO:name:epoch 0 step 600 loss 0.25443
INFO:name:epoch 0 step 700 loss 0.22011
INFO:name:epoch 0 step 800 loss 0.20881
INFO:name:epoch 0 step 900 loss 0.22186
INFO:name:epoch 0 step 1000 loss 0.1953
INFO:name:epoch 0 step 1100 loss 0.18938
INFO:name:epoch 0 step 1200 loss 0.17666
INFO:name:epoch 0 step 1300 loss 0.19399
INFO:name:epoch 0 step 1400 loss 0.19338
INFO:name:epoch 0 step 1500 loss 0.17475
INFO:name:epoch 0 step 1600 loss 0.16622
INFO:name:epoch 0 step 1700 loss 0.16905
INFO:name:epoch 0 step 1800 loss 0.18157
INFO:name:epoch 0 step 1900 loss 0.15936
INFO:name:epoch 0 step 2000 loss 0.17686
INFO:name:epoch 0 step 2100 loss 0.16497
INFO:name:epoch 0 step 2200 loss 0.15196
INFO:name:epoch 0 step 2300 loss 0.16211
INFO:name:epoch 0 step 2400 loss 0.17087
INFO:name:epoch 0 step 2500 loss 0.13924
INFO:name:epoch 0 step 2600 loss 0.1594
INFO:name:epoch 0 step 2700 loss 0.16627
INFO:name:epoch 0 step 2800 loss 0.15236
INFO:name:epoch 0 step 2900 loss 0.14537
INFO:name:epoch 0 step 3000 loss 0.15683
INFO:name:epoch 0 step 3100 loss 0.13985
INFO:name:epoch 0 step 3200 loss 0.13763
INFO:name:epoch 0 step 3300 loss 0.14435
INFO:name:epoch 0 step 3400 loss 0.13536
INFO:name:epoch 0 step 3500 loss 0.13569
INFO:name:epoch 0 step 3600 loss 0.13067
INFO:name:epoch 0 step 3700 loss 0.12937
INFO:name:epoch 0 step 3800 loss 0.14046
INFO:name:epoch 0 step 3900 loss 0.12758
INFO:name:epoch 0 step 4000 loss 0.13078
INFO:name:epoch 0 step 4100 loss 0.13669
INFO:name:epoch 0 step 4200 loss 0.12749
INFO:name:epoch 0 step 4300 loss 0.15615
INFO:name:epoch 0 step 4400 loss 0.13259
INFO:name:epoch 0 step 4500 loss 0.12789
INFO:name:epoch 0 step 4600 loss 0.13412
INFO:name:epoch 0 step 4700 loss 0.14013
INFO:name:epoch 0 step 4800 loss 0.12667
INFO:name:epoch 0 step 4900 loss 0.16345
INFO:name:epoch 0 step 5000 loss 0.12809
INFO:name:epoch 0 step 5100 loss 0.12856
INFO:name:epoch 0 step 5200 loss 0.1216
INFO:name:epoch 0 step 5300 loss 0.13586
INFO:name:epoch 0 step 5400 loss 0.13821
INFO:name:epoch 0 step 5500 loss 0.13341
INFO:name:epoch 0 step 5600 loss 0.11842
INFO:name:epoch 0 step 5700 loss 0.12428
INFO:name:epoch 0 step 5800 loss 0.14104
INFO:name:epoch 0 step 5900 loss 0.1506
INFO:name:epoch 0 step 6000 loss 0.12229
INFO:name:epoch 0 step 6100 loss 0.11107
INFO:name:epoch 0 step 6200 loss 0.11158
INFO:name:epoch 0 step 6300 loss 0.12184
INFO:name:epoch 0 step 6400 loss 0.1237
INFO:name:epoch 0 step 6500 loss 0.11198
INFO:name:epoch 0 step 6600 loss 0.12081
INFO:name:epoch 0 step 6700 loss 0.14376
INFO:name:epoch 0 step 6800 loss 0.11302
INFO:name:epoch 0 step 6900 loss 0.12189
INFO:name:epoch 0 step 7000 loss 0.12062
INFO:name:epoch 0 step 7100 loss 0.12308
INFO:name:epoch 0 step 7200 loss 0.13876
INFO:name:epoch 0 step 7300 loss 0.11505
INFO:name:epoch 0 step 7400 loss 0.13684
INFO:name:epoch 0 step 7500 loss 0.1217
INFO:name:epoch 0 step 7600 loss 0.1118
INFO:name:epoch 0 step 7700 loss 0.11285
INFO:name:epoch 0 step 7800 loss 0.12876
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3292
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3292
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2695
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 0.08663
INFO:name:epoch 1 step 200 loss 0.08776
INFO:name:epoch 1 step 300 loss 0.07267
INFO:name:epoch 1 step 400 loss 0.08056
INFO:name:epoch 1 step 500 loss 0.08682
INFO:name:epoch 1 step 600 loss 0.08518
INFO:name:epoch 1 step 700 loss 0.08134
INFO:name:epoch 1 step 800 loss 0.06469
INFO:name:epoch 1 step 900 loss 0.07989
INFO:name:epoch 1 step 1000 loss 0.08288
INFO:name:epoch 1 step 1100 loss 0.08409
INFO:name:epoch 1 step 1200 loss 0.09574
INFO:name:epoch 1 step 1300 loss 0.08112
INFO:name:epoch 1 step 1400 loss 0.07295
INFO:name:epoch 1 step 1500 loss 0.07611
INFO:name:epoch 1 step 1600 loss 0.07746
INFO:name:epoch 1 step 1700 loss 0.07791
INFO:name:epoch 1 step 1800 loss 0.08373
INFO:name:epoch 1 step 1900 loss 0.08003
INFO:name:epoch 1 step 2000 loss 0.07246
INFO:name:epoch 1 step 2100 loss 0.08395
INFO:name:epoch 1 step 2200 loss 0.0813
INFO:name:epoch 1 step 2300 loss 0.06765
INFO:name:epoch 1 step 2400 loss 0.06459
INFO:name:epoch 1 step 2500 loss 0.07939
INFO:name:epoch 1 step 2600 loss 0.08617
INFO:name:epoch 1 step 2700 loss 0.08585
INFO:name:epoch 1 step 2800 loss 0.07653
INFO:name:epoch 1 step 2900 loss 0.09114
INFO:name:epoch 1 step 3000 loss 0.07885
INFO:name:epoch 1 step 3100 loss 0.08983
INFO:name:epoch 1 step 3200 loss 0.09423
INFO:name:epoch 1 step 3300 loss 0.08227
INFO:name:epoch 1 step 3400 loss 0.08136
INFO:name:epoch 1 step 3500 loss 0.08689
INFO:name:epoch 1 step 3600 loss 0.07105
INFO:name:epoch 1 step 3700 loss 0.08142
INFO:name:epoch 1 step 3800 loss 0.07792
INFO:name:epoch 1 step 3900 loss 0.08019
INFO:name:epoch 1 step 4000 loss 0.07899
INFO:name:epoch 1 step 4100 loss 0.07506
INFO:name:epoch 1 step 4200 loss 0.07333
INFO:name:epoch 1 step 4300 loss 0.06703
INFO:name:epoch 1 step 4400 loss 0.07995
INFO:name:epoch 1 step 4500 loss 0.09071
INFO:name:epoch 1 step 4600 loss 0.07963
INFO:name:epoch 1 step 4700 loss 0.06379
INFO:name:epoch 1 step 4800 loss 0.07634
INFO:name:epoch 1 step 4900 loss 0.06223
INFO:name:epoch 1 step 5000 loss 0.07139
INFO:name:epoch 1 step 5100 loss 0.0837
INFO:name:epoch 1 step 5200 loss 0.06641
INFO:name:epoch 1 step 5300 loss 0.07731
INFO:name:epoch 1 step 5400 loss 0.06806
INFO:name:epoch 1 step 5500 loss 0.08835
INFO:name:epoch 1 step 5600 loss 0.09184
INFO:name:epoch 1 step 5700 loss 0.08985
INFO:name:epoch 1 step 5800 loss 0.07906
INFO:name:epoch 1 step 5900 loss 0.08554
INFO:name:epoch 1 step 6000 loss 0.08159
INFO:name:epoch 1 step 6100 loss 0.07663
INFO:name:epoch 1 step 6200 loss 0.08112
INFO:name:epoch 1 step 6300 loss 0.07552
INFO:name:epoch 1 step 6400 loss 0.07516
INFO:name:epoch 1 step 6500 loss 0.07095
INFO:name:epoch 1 step 6600 loss 0.07824
INFO:name:epoch 1 step 6700 loss 0.08254
INFO:name:epoch 1 step 6800 loss 0.08294
INFO:name:epoch 1 step 6900 loss 0.0817
INFO:name:epoch 1 step 7000 loss 0.07722
INFO:name:epoch 1 step 7100 loss 0.07087
INFO:name:epoch 1 step 7200 loss 0.07303
INFO:name:epoch 1 step 7300 loss 0.0675
INFO:name:epoch 1 step 7400 loss 0.06725
INFO:name:epoch 1 step 7500 loss 0.07752
INFO:name:epoch 1 step 7600 loss 0.08017
INFO:name:epoch 1 step 7700 loss 0.06278
INFO:name:epoch 1 step 7800 loss 0.07102
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3221
INFO:name:epoch 2 step 100 loss 0.05759
INFO:name:epoch 2 step 200 loss 0.04877
INFO:name:epoch 2 step 300 loss 0.06255
INFO:name:epoch 2 step 400 loss 0.04978
INFO:name:epoch 2 step 500 loss 0.04832
INFO:name:epoch 2 step 600 loss 0.05733
INFO:name:epoch 2 step 700 loss 0.05906
INFO:name:epoch 2 step 800 loss 0.05608
INFO:name:epoch 2 step 900 loss 0.04977
INFO:name:epoch 2 step 1000 loss 0.05806
INFO:name:epoch 2 step 1100 loss 0.04786
INFO:name:epoch 2 step 1200 loss 0.04821
INFO:name:epoch 2 step 1300 loss 0.06182
INFO:name:epoch 2 step 1400 loss 0.04896
INFO:name:epoch 2 step 1500 loss 0.04941
INFO:name:epoch 2 step 1600 loss 0.0593
INFO:name:epoch 2 step 1700 loss 0.04987
INFO:name:epoch 2 step 1800 loss 0.05225
INFO:name:epoch 2 step 1900 loss 0.06763
INFO:name:epoch 2 step 2000 loss 0.05929
INFO:name:epoch 2 step 2100 loss 0.05289
INFO:name:epoch 2 step 2200 loss 0.0623
INFO:name:epoch 2 step 2300 loss 0.06109
INFO:name:epoch 2 step 2400 loss 0.0586
INFO:name:epoch 2 step 2500 loss 0.0551
INFO:name:epoch 2 step 2600 loss 0.05053
INFO:name:epoch 2 step 2700 loss 0.0449
INFO:name:epoch 2 step 2800 loss 0.06384
INFO:name:epoch 2 step 2900 loss 0.0565
INFO:name:epoch 2 step 3000 loss 0.0546
INFO:name:epoch 2 step 3100 loss 0.04738
INFO:name:epoch 2 step 3200 loss 0.05964
INFO:name:epoch 2 step 3300 loss 0.05972
INFO:name:epoch 2 step 3400 loss 0.06113
INFO:name:epoch 2 step 3500 loss 0.05337
INFO:name:epoch 2 step 3600 loss 0.05437
INFO:name:epoch 2 step 3700 loss 0.04993
INFO:name:epoch 2 step 3800 loss 0.05646
INFO:name:epoch 2 step 3900 loss 0.05982
INFO:name:epoch 2 step 4000 loss 0.05693
INFO:name:epoch 2 step 4100 loss 0.05759
INFO:name:epoch 2 step 4200 loss 0.05017
INFO:name:epoch 2 step 4300 loss 0.0596
INFO:name:epoch 2 step 4400 loss 0.04688
INFO:name:epoch 2 step 4500 loss 0.0544
INFO:name:epoch 2 step 4600 loss 0.05182
INFO:name:epoch 2 step 4700 loss 0.05133
INFO:name:epoch 2 step 4800 loss 0.04885
INFO:name:epoch 2 step 4900 loss 0.05716
INFO:name:epoch 2 step 5000 loss 0.06138
INFO:name:epoch 2 step 5100 loss 0.05838
INFO:name:epoch 2 step 5200 loss 0.0529
INFO:name:epoch 2 step 5300 loss 0.06251
INFO:name:epoch 2 step 5400 loss 0.05855
INFO:name:epoch 2 step 5500 loss 0.04963
INFO:name:epoch 2 step 5600 loss 0.05237
INFO:name:epoch 2 step 5700 loss 0.05892
INFO:name:epoch 2 step 5800 loss 0.05473
INFO:name:epoch 2 step 5900 loss 0.0517
INFO:name:epoch 2 step 6000 loss 0.04846
INFO:name:epoch 2 step 6100 loss 0.04009
INFO:name:epoch 2 step 6200 loss 0.05068
INFO:name:epoch 2 step 6300 loss 0.05389
INFO:name:epoch 2 step 6400 loss 0.05959
INFO:name:epoch 2 step 6500 loss 0.04878
INFO:name:epoch 2 step 6600 loss 0.05818
INFO:name:epoch 2 step 6700 loss 0.05424
INFO:name:epoch 2 step 6800 loss 0.06033
INFO:name:epoch 2 step 6900 loss 0.05738
INFO:name:epoch 2 step 7000 loss 0.06077
INFO:name:epoch 2 step 7100 loss 0.04925
INFO:name:epoch 2 step 7200 loss 0.05803
INFO:name:epoch 2 step 7300 loss 0.05624
INFO:name:epoch 2 step 7400 loss 0.059
INFO:name:epoch 2 step 7500 loss 0.05066
INFO:name:epoch 2 step 7600 loss 0.05439
INFO:name:epoch 2 step 7700 loss 0.05452
INFO:name:epoch 2 step 7800 loss 0.05333
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3659
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3659
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3047
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.04293
INFO:name:epoch 3 step 200 loss 0.03597
INFO:name:epoch 3 step 300 loss 0.0352
INFO:name:epoch 3 step 400 loss 0.03098
INFO:name:epoch 3 step 500 loss 0.0376
INFO:name:epoch 3 step 600 loss 0.03567
INFO:name:epoch 3 step 700 loss 0.03766
INFO:name:epoch 3 step 800 loss 0.0437
INFO:name:epoch 3 step 900 loss 0.03743
INFO:name:epoch 3 step 1000 loss 0.03986
INFO:name:epoch 3 step 1100 loss 0.04756
INFO:name:epoch 3 step 1200 loss 0.04293
INFO:name:epoch 3 step 1300 loss 0.03861
INFO:name:epoch 3 step 1400 loss 0.03592
INFO:name:epoch 3 step 1500 loss 0.03639
INFO:name:epoch 3 step 1600 loss 0.03362
INFO:name:epoch 3 step 1700 loss 0.03371
INFO:name:epoch 3 step 1800 loss 0.03671
INFO:name:epoch 3 step 1900 loss 0.04216
INFO:name:epoch 3 step 2000 loss 0.03718
INFO:name:epoch 3 step 2100 loss 0.03447
INFO:name:epoch 3 step 2200 loss 0.04005
INFO:name:epoch 3 step 2300 loss 0.04257
INFO:name:epoch 3 step 2400 loss 0.03951
INFO:name:epoch 3 step 2500 loss 0.04474
INFO:name:epoch 3 step 2600 loss 0.04085
INFO:name:epoch 3 step 2700 loss 0.03908
INFO:name:epoch 3 step 2800 loss 0.04367
INFO:name:epoch 3 step 2900 loss 0.03558
INFO:name:epoch 3 step 3000 loss 0.04012
INFO:name:epoch 3 step 3100 loss 0.03869
INFO:name:epoch 3 step 3200 loss 0.03683
INFO:name:epoch 3 step 3300 loss 0.0407
INFO:name:epoch 3 step 3400 loss 0.03766
INFO:name:epoch 3 step 3500 loss 0.03587
INFO:name:epoch 3 step 3600 loss 0.04179
INFO:name:epoch 3 step 3700 loss 0.03655
INFO:name:epoch 3 step 3800 loss 0.04083
INFO:name:epoch 3 step 3900 loss 0.04273
INFO:name:epoch 3 step 4000 loss 0.04676
INFO:name:epoch 3 step 4100 loss 0.04097
INFO:name:epoch 3 step 4200 loss 0.03699
INFO:name:epoch 3 step 4300 loss 0.03206
INFO:name:epoch 3 step 4400 loss 0.03573
INFO:name:epoch 3 step 4500 loss 0.03788
INFO:name:epoch 3 step 4600 loss 0.04714
INFO:name:epoch 3 step 4700 loss 0.03891
INFO:name:epoch 3 step 4800 loss 0.0383
INFO:name:epoch 3 step 4900 loss 0.03742
INFO:name:epoch 3 step 5000 loss 0.04126
INFO:name:epoch 3 step 5100 loss 0.03637
INFO:name:epoch 3 step 5200 loss 0.03718
INFO:name:epoch 3 step 5300 loss 0.04737
INFO:name:epoch 3 step 5400 loss 0.0441
INFO:name:epoch 3 step 5500 loss 0.04378
INFO:name:epoch 3 step 5600 loss 0.04708
INFO:name:epoch 3 step 5700 loss 0.04187
INFO:name:epoch 3 step 5800 loss 0.0372
INFO:name:epoch 3 step 5900 loss 0.04352
INFO:name:epoch 3 step 6000 loss 0.04184
INFO:name:epoch 3 step 6100 loss 0.04748
INFO:name:epoch 3 step 6200 loss 0.0384
INFO:name:epoch 3 step 6300 loss 0.04284
INFO:name:epoch 3 step 6400 loss 0.04552
INFO:name:epoch 3 step 6500 loss 0.03633
INFO:name:epoch 3 step 6600 loss 0.04185
INFO:name:epoch 3 step 6700 loss 0.04202
INFO:name:epoch 3 step 6800 loss 0.04044
INFO:name:epoch 3 step 6900 loss 0.04219
INFO:name:epoch 3 step 7000 loss 0.04201
INFO:name:epoch 3 step 7100 loss 0.05081
INFO:name:epoch 3 step 7200 loss 0.03819
INFO:name:epoch 3 step 7300 loss 0.03408
INFO:name:epoch 3 step 7400 loss 0.03923
INFO:name:epoch 3 step 7500 loss 0.04035
INFO:name:epoch 3 step 7600 loss 0.03546
INFO:name:epoch 3 step 7700 loss 0.03783
INFO:name:epoch 3 step 7800 loss 0.03726
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3584
INFO:name:epoch 4 step 100 loss 0.03719
INFO:name:epoch 4 step 200 loss 0.02407
INFO:name:epoch 4 step 300 loss 0.03223
INFO:name:epoch 4 step 400 loss 0.03242
INFO:name:epoch 4 step 500 loss 0.03069
INFO:name:epoch 4 step 600 loss 0.0304
INFO:name:epoch 4 step 700 loss 0.02096
INFO:name:epoch 4 step 800 loss 0.02851
INFO:name:epoch 4 step 900 loss 0.03429
INFO:name:epoch 4 step 1000 loss 0.02921
INFO:name:epoch 4 step 1100 loss 0.02231
INFO:name:epoch 4 step 1200 loss 0.02676
INFO:name:epoch 4 step 1300 loss 0.02829
INFO:name:epoch 4 step 1400 loss 0.02566
INFO:name:epoch 4 step 1500 loss 0.02247
INFO:name:epoch 4 step 1600 loss 0.02477
INFO:name:epoch 4 step 1700 loss 0.03296
INFO:name:epoch 4 step 1800 loss 0.02932
INFO:name:epoch 4 step 1900 loss 0.02877
INFO:name:epoch 4 step 2000 loss 0.03079
INFO:name:epoch 4 step 2100 loss 0.02585
INFO:name:epoch 4 step 2200 loss 0.03146
INFO:name:epoch 4 step 2300 loss 0.02683
INFO:name:epoch 4 step 2400 loss 0.02826
INFO:name:epoch 4 step 2500 loss 0.03025
INFO:name:epoch 4 step 2600 loss 0.02741
INFO:name:epoch 4 step 2700 loss 0.03011
INFO:name:epoch 4 step 2800 loss 0.02825
INFO:name:epoch 4 step 2900 loss 0.0259
INFO:name:epoch 4 step 3000 loss 0.03093
INFO:name:epoch 4 step 3100 loss 0.02984
INFO:name:epoch 4 step 3200 loss 0.02913
INFO:name:epoch 4 step 3300 loss 0.03086
INFO:name:epoch 4 step 3400 loss 0.03343
INFO:name:epoch 4 step 3500 loss 0.03005
INFO:name:epoch 4 step 3600 loss 0.02763
INFO:name:epoch 4 step 3700 loss 0.03348
INFO:name:epoch 4 step 3800 loss 0.02775
INFO:name:epoch 4 step 3900 loss 0.03086
INFO:name:epoch 4 step 4000 loss 0.03132
INFO:name:epoch 4 step 4100 loss 0.02907
INFO:name:epoch 4 step 4200 loss 0.03247
INFO:name:epoch 4 step 4300 loss 0.02966
INFO:name:epoch 4 step 4400 loss 0.03331
INFO:name:epoch 4 step 4500 loss 0.03046
INFO:name:epoch 4 step 4600 loss 0.0263
INFO:name:epoch 4 step 4700 loss 0.02622
INFO:name:epoch 4 step 4800 loss 0.02469
INFO:name:epoch 4 step 4900 loss 0.03099
INFO:name:epoch 4 step 5000 loss 0.02592
INFO:name:epoch 4 step 5100 loss 0.02983
INFO:name:epoch 4 step 5200 loss 0.02915
INFO:name:epoch 4 step 5300 loss 0.03282
INFO:name:epoch 4 step 5400 loss 0.03249
INFO:name:epoch 4 step 5500 loss 0.02837
INFO:name:epoch 4 step 5600 loss 0.03072
INFO:name:epoch 4 step 5700 loss 0.02969
INFO:name:epoch 4 step 5800 loss 0.03039
INFO:name:epoch 4 step 5900 loss 0.0317
INFO:name:epoch 4 step 6000 loss 0.02961
INFO:name:epoch 4 step 6100 loss 0.03347
INFO:name:epoch 4 step 6200 loss 0.03504
INFO:name:epoch 4 step 6300 loss 0.02957
INFO:name:epoch 4 step 6400 loss 0.02641
INFO:name:epoch 4 step 6500 loss 0.02722
INFO:name:epoch 4 step 6600 loss 0.02998
INFO:name:epoch 4 step 6700 loss 0.02969
INFO:name:epoch 4 step 6800 loss 0.03304
INFO:name:epoch 4 step 6900 loss 0.03297
INFO:name:epoch 4 step 7000 loss 0.02691
INFO:name:epoch 4 step 7100 loss 0.03131
INFO:name:epoch 4 step 7200 loss 0.03173
INFO:name:epoch 4 step 7300 loss 0.03473
INFO:name:epoch 4 step 7400 loss 0.02771
INFO:name:epoch 4 step 7500 loss 0.03192
INFO:name:epoch 4 step 7600 loss 0.03351
INFO:name:epoch 4 step 7700 loss 0.03165
INFO:name:epoch 4 step 7800 loss 0.03134
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3842
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3842
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3199
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 5 step 100 loss 0.02693
INFO:name:epoch 5 step 200 loss 0.02264
INFO:name:epoch 5 step 300 loss 0.01921
INFO:name:epoch 5 step 400 loss 0.02376
INFO:name:epoch 5 step 500 loss 0.02135
INFO:name:epoch 5 step 600 loss 0.02057
INFO:name:epoch 5 step 700 loss 0.01866
INFO:name:epoch 5 step 800 loss 0.02127
INFO:name:epoch 5 step 900 loss 0.01892
INFO:name:epoch 5 step 1000 loss 0.02094
INFO:name:epoch 5 step 1100 loss 0.02621
INFO:name:epoch 5 step 1200 loss 0.0198
INFO:name:epoch 5 step 1300 loss 0.01915
INFO:name:epoch 5 step 1400 loss 0.02234
INFO:name:epoch 5 step 1500 loss 0.01859
INFO:name:epoch 5 step 1600 loss 0.02288
INFO:name:epoch 5 step 1700 loss 0.0217
INFO:name:epoch 5 step 1800 loss 0.01815
INFO:name:epoch 5 step 1900 loss 0.02135
INFO:name:epoch 5 step 2000 loss 0.02157
INFO:name:epoch 5 step 2100 loss 0.02204
INFO:name:epoch 5 step 2200 loss 0.02132
INFO:name:epoch 5 step 2300 loss 0.02482
INFO:name:epoch 5 step 2400 loss 0.02149
INFO:name:epoch 5 step 2500 loss 0.0248
INFO:name:epoch 5 step 2600 loss 0.02153
INFO:name:epoch 5 step 2700 loss 0.01993
INFO:name:epoch 5 step 2800 loss 0.01724
INFO:name:epoch 5 step 2900 loss 0.02618
INFO:name:epoch 5 step 3000 loss 0.02128
INFO:name:epoch 5 step 3100 loss 0.02082
INFO:name:epoch 5 step 3200 loss 0.02104
INFO:name:epoch 5 step 3300 loss 0.02169
INFO:name:epoch 5 step 3400 loss 0.02285
INFO:name:epoch 5 step 3500 loss 0.02267
INFO:name:epoch 5 step 3600 loss 0.01783
INFO:name:epoch 5 step 3700 loss 0.02362
INFO:name:epoch 5 step 3800 loss 0.02194
INFO:name:epoch 5 step 3900 loss 0.02224
INFO:name:epoch 5 step 4000 loss 0.01874
INFO:name:epoch 5 step 4100 loss 0.01926
INFO:name:epoch 5 step 4200 loss 0.02496
INFO:name:epoch 5 step 4300 loss 0.02479
INFO:name:epoch 5 step 4400 loss 0.02286
INFO:name:epoch 5 step 4500 loss 0.01794
INFO:name:epoch 5 step 4600 loss 0.01957
INFO:name:epoch 5 step 4700 loss 0.02039
INFO:name:epoch 5 step 4800 loss 0.02347
INFO:name:epoch 5 step 4900 loss 0.02285
INFO:name:epoch 5 step 5000 loss 0.02313
INFO:name:epoch 5 step 5100 loss 0.0211
INFO:name:epoch 5 step 5200 loss 0.02774
INFO:name:epoch 5 step 5300 loss 0.01967
INFO:name:epoch 5 step 5400 loss 0.02074
INFO:name:epoch 5 step 5500 loss 0.01691
INFO:name:epoch 5 step 5600 loss 0.02139
INFO:name:epoch 5 step 5700 loss 0.02431
INFO:name:epoch 5 step 5800 loss 0.02547
INFO:name:epoch 5 step 5900 loss 0.02536
INFO:name:epoch 5 step 6000 loss 0.0243
INFO:name:epoch 5 step 6100 loss 0.02256
INFO:name:epoch 5 step 6200 loss 0.01982
INFO:name:epoch 5 step 6300 loss 0.01936
INFO:name:epoch 5 step 6400 loss 0.01932
INFO:name:epoch 5 step 6500 loss 0.02474
INFO:name:epoch 5 step 6600 loss 0.02392
INFO:name:epoch 5 step 6700 loss 0.02408
INFO:name:epoch 5 step 6800 loss 0.02324
INFO:name:epoch 5 step 6900 loss 0.02219
INFO:name:epoch 5 step 7000 loss 0.02334
INFO:name:epoch 5 step 7100 loss 0.02325
INFO:name:epoch 5 step 7200 loss 0.02247
INFO:name:epoch 5 step 7300 loss 0.03145
INFO:name:epoch 5 step 7400 loss 0.0198
INFO:name:epoch 5 step 7500 loss 0.03173
INFO:name:epoch 5 step 7600 loss 0.02596
INFO:name:epoch 5 step 7700 loss 0.02226
INFO:name:epoch 5 step 7800 loss 0.03121
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3658
INFO:name:epoch 6 step 100 loss 0.01881
INFO:name:epoch 6 step 200 loss 0.01766
INFO:name:epoch 6 step 300 loss 0.01476
INFO:name:epoch 6 step 400 loss 0.01742
INFO:name:epoch 6 step 500 loss 0.01525
INFO:name:epoch 6 step 600 loss 0.01871
INFO:name:epoch 6 step 700 loss 0.01712
INFO:name:epoch 6 step 800 loss 0.01299
INFO:name:epoch 6 step 900 loss 0.01511
INFO:name:epoch 6 step 1000 loss 0.01698
INFO:name:epoch 6 step 1100 loss 0.01739
INFO:name:epoch 6 step 1200 loss 0.01834
INFO:name:epoch 6 step 1300 loss 0.01881
INFO:name:epoch 6 step 1400 loss 0.02101
INFO:name:epoch 6 step 1500 loss 0.01526
INFO:name:epoch 6 step 1600 loss 0.01507
INFO:name:epoch 6 step 1700 loss 0.01372
INFO:name:epoch 6 step 1800 loss 0.01653
INFO:name:epoch 6 step 1900 loss 0.01677
INFO:name:epoch 6 step 2000 loss 0.01746
INFO:name:epoch 6 step 2100 loss 0.0143
INFO:name:epoch 6 step 2200 loss 0.01781
INFO:name:epoch 6 step 2300 loss 0.01878
INFO:name:epoch 6 step 2400 loss 0.01928
INFO:name:epoch 6 step 2500 loss 0.01817
INFO:name:epoch 6 step 2600 loss 0.01631
INFO:name:epoch 6 step 2700 loss 0.01737
INFO:name:epoch 6 step 2800 loss 0.01596
INFO:name:epoch 6 step 2900 loss 0.01615
INFO:name:epoch 6 step 3000 loss 0.01654
INFO:name:epoch 6 step 3100 loss 0.01688
INFO:name:epoch 6 step 3200 loss 0.01548
INFO:name:epoch 6 step 3300 loss 0.01952
INFO:name:epoch 6 step 3400 loss 0.01682
INFO:name:epoch 6 step 3500 loss 0.01836
INFO:name:epoch 6 step 3600 loss 0.01504
INFO:name:epoch 6 step 3700 loss 0.01849
INFO:name:epoch 6 step 3800 loss 0.01818
INFO:name:epoch 6 step 3900 loss 0.01922
INFO:name:epoch 6 step 4000 loss 0.01716
INFO:name:epoch 6 step 4100 loss 0.01611
INFO:name:epoch 6 step 4200 loss 0.0212
INFO:name:epoch 6 step 4300 loss 0.01512
INFO:name:epoch 6 step 4400 loss 0.01811
INFO:name:epoch 6 step 4500 loss 0.01815
INFO:name:epoch 6 step 4600 loss 0.01912
INFO:name:epoch 6 step 4700 loss 0.0143
INFO:name:epoch 6 step 4800 loss 0.01791
INFO:name:epoch 6 step 4900 loss 0.01769
INFO:name:epoch 6 step 5000 loss 0.01835
INFO:name:epoch 6 step 5100 loss 0.01891
INFO:name:epoch 6 step 5200 loss 0.0189
INFO:name:epoch 6 step 5300 loss 0.01853
INFO:name:epoch 6 step 5400 loss 0.0176
INFO:name:epoch 6 step 5500 loss 0.01788
INFO:name:epoch 6 step 5600 loss 0.02043
INFO:name:epoch 6 step 5700 loss 0.0199
INFO:name:epoch 6 step 5800 loss 0.02198
INFO:name:epoch 6 step 5900 loss 0.02017
INFO:name:epoch 6 step 6000 loss 0.01972
INFO:name:epoch 6 step 6100 loss 0.02009
INFO:name:epoch 6 step 6200 loss 0.01825
INFO:name:epoch 6 step 6300 loss 0.02258
INFO:name:epoch 6 step 6400 loss 0.01651
INFO:name:epoch 6 step 6500 loss 0.02013
INFO:name:epoch 6 step 6600 loss 0.0158
INFO:name:epoch 6 step 6700 loss 0.02452
INFO:name:epoch 6 step 6800 loss 0.01698
INFO:name:epoch 6 step 6900 loss 0.01772
INFO:name:epoch 6 step 7000 loss 0.01979
INFO:name:epoch 6 step 7100 loss 0.01918
INFO:name:epoch 6 step 7200 loss 0.01623
INFO:name:epoch 6 step 7300 loss 0.01736
INFO:name:epoch 6 step 7400 loss 0.0155
INFO:name:epoch 6 step 7500 loss 0.02132
INFO:name:epoch 6 step 7600 loss 0.01912
INFO:name:epoch 6 step 7700 loss 0.01907
INFO:name:epoch 6 step 7800 loss 0.01764
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3602
INFO:name:epoch 7 step 100 loss 0.01425
INFO:name:epoch 7 step 200 loss 0.01192
INFO:name:epoch 7 step 300 loss 0.01542
INFO:name:epoch 7 step 400 loss 0.01315
INFO:name:epoch 7 step 500 loss 0.01244
INFO:name:epoch 7 step 600 loss 0.01155
INFO:name:epoch 7 step 700 loss 0.01159
INFO:name:epoch 7 step 800 loss 0.0135
INFO:name:epoch 7 step 900 loss 0.01652
INFO:name:epoch 7 step 1000 loss 0.01353
INFO:name:epoch 7 step 1100 loss 0.01368
INFO:name:epoch 7 step 1200 loss 0.0144
INFO:name:epoch 7 step 1300 loss 0.01433
INFO:name:epoch 7 step 1400 loss 0.01528
INFO:name:epoch 7 step 1500 loss 0.01217
INFO:name:epoch 7 step 1600 loss 0.01421
INFO:name:epoch 7 step 1700 loss 0.01201
INFO:name:epoch 7 step 1800 loss 0.0119
INFO:name:epoch 7 step 1900 loss 0.0134
INFO:name:epoch 7 step 2000 loss 0.01488
INFO:name:epoch 7 step 2100 loss 0.01331
INFO:name:epoch 7 step 2200 loss 0.01213
INFO:name:epoch 7 step 2300 loss 0.01521
INFO:name:epoch 7 step 2400 loss 0.01498
INFO:name:epoch 7 step 2500 loss 0.01001
INFO:name:epoch 7 step 2600 loss 0.0133
INFO:name:epoch 7 step 2700 loss 0.01696
INFO:name:epoch 7 step 2800 loss 0.01321
INFO:name:epoch 7 step 2900 loss 0.01497
INFO:name:epoch 7 step 3000 loss 0.01527
INFO:name:epoch 7 step 3100 loss 0.01684
INFO:name:epoch 7 step 3200 loss 0.01864
INFO:name:epoch 7 step 3300 loss 0.01798
INFO:name:epoch 7 step 3400 loss 0.01702
INFO:name:epoch 7 step 3500 loss 0.01642
INFO:name:epoch 7 step 3600 loss 0.01534
INFO:name:epoch 7 step 3700 loss 0.01446
INFO:name:epoch 7 step 3800 loss 0.01811
INFO:name:epoch 7 step 3900 loss 0.01556
INFO:name:epoch 7 step 4000 loss 0.01454
INFO:name:epoch 7 step 4100 loss 0.01469
INFO:name:epoch 7 step 4200 loss 0.01773
INFO:name:epoch 7 step 4300 loss 0.01269
INFO:name:epoch 7 step 4400 loss 0.01572
INFO:name:epoch 7 step 4500 loss 0.01436
INFO:name:epoch 7 step 4600 loss 0.01425
INFO:name:epoch 7 step 4700 loss 0.01373
INFO:name:epoch 7 step 4800 loss 0.01538
INFO:name:epoch 7 step 4900 loss 0.01556
INFO:name:epoch 7 step 5000 loss 0.01276
INFO:name:epoch 7 step 5100 loss 0.01557
INFO:name:epoch 7 step 5200 loss 0.01213
INFO:name:epoch 7 step 5300 loss 0.01515
INFO:name:epoch 7 step 5400 loss 0.01333
INFO:name:epoch 7 step 5500 loss 0.01521
INFO:name:epoch 7 step 5600 loss 0.01475
INFO:name:epoch 7 step 5700 loss 0.0117
INFO:name:epoch 7 step 5800 loss 0.01715
INFO:name:epoch 7 step 5900 loss 0.01646
INFO:name:epoch 7 step 6000 loss 0.01831
INFO:name:epoch 7 step 6100 loss 0.01808
INFO:name:epoch 7 step 6200 loss 0.01374
INFO:name:epoch 7 step 6300 loss 0.01455
INFO:name:epoch 7 step 6400 loss 0.01501
INFO:name:epoch 7 step 6500 loss 0.0166
INFO:name:epoch 7 step 6600 loss 0.01283
INFO:name:epoch 7 step 6700 loss 0.01555
INFO:name:epoch 7 step 6800 loss 0.01483
INFO:name:epoch 7 step 6900 loss 0.01653
INFO:name:epoch 7 step 7000 loss 0.01674
INFO:name:epoch 7 step 7100 loss 0.01536
INFO:name:epoch 7 step 7200 loss 0.01599
INFO:name:epoch 7 step 7300 loss 0.01299
INFO:name:epoch 7 step 7400 loss 0.01654
INFO:name:epoch 7 step 7500 loss 0.01506
INFO:name:epoch 7 step 7600 loss 0.01447
INFO:name:epoch 7 step 7700 loss 0.0161
INFO:name:epoch 7 step 7800 loss 0.01201
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3659
INFO:name:epoch 8 step 100 loss 0.0133
INFO:name:epoch 8 step 200 loss 0.01113
INFO:name:epoch 8 step 300 loss 0.01205
INFO:name:epoch 8 step 400 loss 0.01231
INFO:name:epoch 8 step 500 loss 0.01495
INFO:name:epoch 8 step 600 loss 0.01129
INFO:name:epoch 8 step 700 loss 0.01243
INFO:name:epoch 8 step 800 loss 0.01107
INFO:name:epoch 8 step 900 loss 0.00989
INFO:name:epoch 8 step 1000 loss 0.0102
INFO:name:epoch 8 step 1100 loss 0.01103
INFO:name:epoch 8 step 1200 loss 0.01034
INFO:name:epoch 8 step 1300 loss 0.01482
INFO:name:epoch 8 step 1400 loss 0.01065
INFO:name:epoch 8 step 1500 loss 0.01188
INFO:name:epoch 8 step 1600 loss 0.01154
INFO:name:epoch 8 step 1700 loss 0.01121
INFO:name:epoch 8 step 1800 loss 0.01271
INFO:name:epoch 8 step 1900 loss 0.0157
INFO:name:epoch 8 step 2000 loss 0.01318
INFO:name:epoch 8 step 2100 loss 0.01189
INFO:name:epoch 8 step 2200 loss 0.01191
INFO:name:epoch 8 step 2300 loss 0.01004
INFO:name:epoch 8 step 2400 loss 0.01296
INFO:name:epoch 8 step 2500 loss 0.01199
INFO:name:epoch 8 step 2600 loss 0.01188
INFO:name:epoch 8 step 2700 loss 0.01132
INFO:name:epoch 8 step 2800 loss 0.0122
INFO:name:epoch 8 step 2900 loss 0.01141
INFO:name:epoch 8 step 3000 loss 0.01372
INFO:name:epoch 8 step 3100 loss 0.01151
INFO:name:epoch 8 step 3200 loss 0.01471
INFO:name:epoch 8 step 3300 loss 0.01113
INFO:name:epoch 8 step 3400 loss 0.01339
INFO:name:epoch 8 step 3500 loss 0.01387
INFO:name:epoch 8 step 3600 loss 0.01202
INFO:name:epoch 8 step 3700 loss 0.01262
INFO:name:epoch 8 step 3800 loss 0.01337
INFO:name:epoch 8 step 3900 loss 0.01065
INFO:name:epoch 8 step 4000 loss 0.01181
INFO:name:epoch 8 step 4100 loss 0.01454
INFO:name:epoch 8 step 4200 loss 0.01269
INFO:name:epoch 8 step 4300 loss 0.01179
INFO:name:epoch 8 step 4400 loss 0.01113
INFO:name:epoch 8 step 4500 loss 0.00981
INFO:name:epoch 8 step 4600 loss 0.01262
INFO:name:epoch 8 step 4700 loss 0.01099
INFO:name:epoch 8 step 4800 loss 0.01078
INFO:name:epoch 8 step 4900 loss 0.0124
INFO:name:epoch 8 step 5000 loss 0.0122
INFO:name:epoch 8 step 5100 loss 0.01332
INFO:name:epoch 8 step 5200 loss 0.01272
INFO:name:epoch 8 step 5300 loss 0.01358
INFO:name:epoch 8 step 5400 loss 0.01186
INFO:name:epoch 8 step 5500 loss 0.01455
INFO:name:epoch 8 step 5600 loss 0.01228
INFO:name:epoch 8 step 5700 loss 0.01449
INFO:name:epoch 8 step 5800 loss 0.01389
INFO:name:epoch 8 step 5900 loss 0.01264
INFO:name:epoch 8 step 6000 loss 0.01214
INFO:name:epoch 8 step 6100 loss 0.01418
INFO:name:epoch 8 step 6200 loss 0.01068
INFO:name:epoch 8 step 6300 loss 0.0136
INFO:name:epoch 8 step 6400 loss 0.0126
INFO:name:epoch 8 step 6500 loss 0.01296
INFO:name:epoch 8 step 6600 loss 0.01174
INFO:name:epoch 8 step 6700 loss 0.01429
INFO:name:epoch 8 step 6800 loss 0.01042
INFO:name:epoch 8 step 6900 loss 0.01152
INFO:name:epoch 8 step 7000 loss 0.01342
INFO:name:epoch 8 step 7100 loss 0.01101
INFO:name:epoch 8 step 7200 loss 0.01197
INFO:name:epoch 8 step 7300 loss 0.00955
INFO:name:epoch 8 step 7400 loss 0.01233
INFO:name:epoch 8 step 7500 loss 0.00949
INFO:name:epoch 8 step 7600 loss 0.0135
INFO:name:epoch 8 step 7700 loss 0.01414
INFO:name:epoch 8 step 7800 loss 0.01416
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3659
INFO:name:epoch 9 step 100 loss 0.01263
INFO:name:epoch 9 step 200 loss 0.0113
INFO:name:epoch 9 step 300 loss 0.0088
INFO:name:epoch 9 step 400 loss 0.01088
INFO:name:epoch 9 step 500 loss 0.01112
INFO:name:epoch 9 step 600 loss 0.01232
INFO:name:epoch 9 step 700 loss 0.01049
INFO:name:epoch 9 step 800 loss 0.01349
INFO:name:epoch 9 step 900 loss 0.01013
INFO:name:epoch 9 step 1000 loss 0.01161
INFO:name:epoch 9 step 1100 loss 0.00896
INFO:name:epoch 9 step 1200 loss 0.01214
INFO:name:epoch 9 step 1300 loss 0.01324
INFO:name:epoch 9 step 1400 loss 0.01085
INFO:name:epoch 9 step 1500 loss 0.01127
INFO:name:epoch 9 step 1600 loss 0.01107
INFO:name:epoch 9 step 1700 loss 0.01117
INFO:name:epoch 9 step 1800 loss 0.00934
INFO:name:epoch 9 step 1900 loss 0.00992
INFO:name:epoch 9 step 2000 loss 0.00973
INFO:name:epoch 9 step 2100 loss 0.00906
INFO:name:epoch 9 step 2200 loss 0.01042
INFO:name:epoch 9 step 2300 loss 0.01047
INFO:name:epoch 9 step 2400 loss 0.01147
INFO:name:epoch 9 step 2500 loss 0.01249
INFO:name:epoch 9 step 2600 loss 0.01202
INFO:name:epoch 9 step 2700 loss 0.00895
INFO:name:epoch 9 step 2800 loss 0.01452
INFO:name:epoch 9 step 2900 loss 0.00865
INFO:name:epoch 9 step 3000 loss 0.01265
INFO:name:epoch 9 step 3100 loss 0.00912
INFO:name:epoch 9 step 3200 loss 0.01624
INFO:name:epoch 9 step 3300 loss 0.01184
INFO:name:epoch 9 step 3400 loss 0.01339
INFO:name:epoch 9 step 3500 loss 0.0083
INFO:name:epoch 9 step 3600 loss 0.01052
INFO:name:epoch 9 step 3700 loss 0.01128
INFO:name:epoch 9 step 3800 loss 0.00992
INFO:name:epoch 9 step 3900 loss 0.01101
INFO:name:epoch 9 step 4000 loss 0.01165
INFO:name:epoch 9 step 4100 loss 0.00898
INFO:name:epoch 9 step 4200 loss 0.01097
INFO:name:epoch 9 step 4300 loss 0.00977
INFO:name:epoch 9 step 4400 loss 0.01034
INFO:name:epoch 9 step 4500 loss 0.01195
INFO:name:epoch 9 step 4600 loss 0.01049
INFO:name:epoch 9 step 4700 loss 0.01004
INFO:name:epoch 9 step 4800 loss 0.0133
INFO:name:epoch 9 step 4900 loss 0.01064
INFO:name:epoch 9 step 5000 loss 0.01035
INFO:name:epoch 9 step 5100 loss 0.01141
INFO:name:epoch 9 step 5200 loss 0.01224
INFO:name:epoch 9 step 5300 loss 0.01214
INFO:name:epoch 9 step 5400 loss 0.00894
INFO:name:epoch 9 step 5500 loss 0.0116
INFO:name:epoch 9 step 5600 loss 0.0102
INFO:name:epoch 9 step 5700 loss 0.01323
INFO:name:epoch 9 step 5800 loss 0.01149
INFO:name:epoch 9 step 5900 loss 0.00887
INFO:name:epoch 9 step 6000 loss 0.0099
INFO:name:epoch 9 step 6100 loss 0.01039
INFO:name:epoch 9 step 6200 loss 0.00822
INFO:name:epoch 9 step 6300 loss 0.01184
INFO:name:epoch 9 step 6400 loss 0.01184
INFO:name:epoch 9 step 6500 loss 0.01265
INFO:name:epoch 9 step 6600 loss 0.01391
INFO:name:epoch 9 step 6700 loss 0.01015
INFO:name:epoch 9 step 6800 loss 0.01316
INFO:name:epoch 9 step 6900 loss 0.00867
INFO:name:epoch 9 step 7000 loss 0.00974
INFO:name:epoch 9 step 7100 loss 0.01109
INFO:name:epoch 9 step 7200 loss 0.01212
INFO:name:epoch 9 step 7300 loss 0.01149
INFO:name:epoch 9 step 7400 loss 0.00992
INFO:name:epoch 9 step 7500 loss 0.00914
INFO:name:epoch 9 step 7600 loss 0.00903
INFO:name:epoch 9 step 7700 loss 0.00976
INFO:name:epoch 9 step 7800 loss 0.00998
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3606
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
train results ([0.18372035018751798, 0.0783173493317771, 0.05476200861490676, 0.039726391707742566, 0.02963479757201716, 0.022180492916985223, 0.017774762047904104, 0.01461737921419379, 0.012293448576776446, 0.010939010523991563], [0.3291513154050367, 0.322078457758014, 0.3658669042553626, 0.35835217124030183, 0.3842027302924041, 0.3658167696715918, 0.36024805799035287, 0.3659414795238156, 0.3659246031285799, 0.36056186923586225])
