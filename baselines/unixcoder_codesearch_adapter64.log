/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
INFO:name:device: cuda:2, n_gpu: 1
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/unixcoder-base/resolve/main/config.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/unixcoder-base/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

root
├── embeddings (RobertaEmbeddings)
│   ├── word_embeddings (Embedding) weight:[51416, 768]
│   ├── position_embeddings (Embedding) weight:[1026, 768]
│   ├── token_type_embeddings (Embedding) weight:[10, 768]
│   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
├── encoder (RobertaEncoder)
│   └── layer (ModuleList)
│       └── 0-11(RobertaLayer)
│           ├── attention (RobertaAttention)
│           │   ├── self (RobertaSelfAttention)
│           │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│           │   └── output (RobertaSelfOutput)
│           │       ├── dense (Linear) weight:[768, 768] bias:[768]
│           │       │   └── adapter (AdapterLayer)
│           │       │       └── modulelist (Sequential)
│           │       │           ├── down_proj (Linear) weight:[64, 768] bias:[64]
│           │       │           └── up_proj (Linear) weight:[768, 64] bias:[768]
│           │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│           ├── intermediate (RobertaIntermediate)
│           │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│           └── output (RobertaOutput)
│               ├── dense (Linear) weight:[768, 3072] bias:[768]
│               │   └── adapter (AdapterLayer)
│               │       └── modulelist (Sequential)
│               │           ├── down_proj (Linear) weight:[64, 768] bias:[64]
│               │           └── up_proj (Linear) weight:[768, 64] bias:[768]
│               └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── pooler (RobertaPooler)
    └── dense (Linear) weight:[768, 768] bias:[768]
[INFO|(OpenDelta)basemodel:700]2025-01-16 00:12:33,570 >> Trainable Ratio: 2379264/128308992=1.854324%
[INFO|(OpenDelta)basemodel:702]2025-01-16 00:12:33,571 >> Delta Parameter Ratio: 2379264/128308992=1.854324%
[INFO|(OpenDelta)basemodel:704]2025-01-16 00:12:33,571 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 10
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 78700
INFO:name:epoch 0 step 100 loss 3.52829
INFO:name:epoch 0 step 200 loss 3.4664
INFO:name:epoch 0 step 300 loss 3.46604
INFO:name:epoch 0 step 400 loss 3.46591
INFO:name:epoch 0 step 500 loss 3.46553
INFO:name:epoch 0 step 600 loss 3.22555
INFO:name:epoch 0 step 700 loss 2.97774
INFO:name:epoch 0 step 800 loss 2.89779
INFO:name:epoch 0 step 900 loss 2.63461
INFO:name:epoch 0 step 1000 loss 2.31992
INFO:name:epoch 0 step 1100 loss 2.16574
INFO:name:epoch 0 step 1200 loss 1.95561
INFO:name:epoch 0 step 1300 loss 1.72832
INFO:name:epoch 0 step 1400 loss 1.60203
INFO:name:epoch 0 step 1500 loss 1.45554
INFO:name:epoch 0 step 1600 loss 1.42359
INFO:name:epoch 0 step 1700 loss 1.27754
INFO:name:epoch 0 step 1800 loss 1.25162
INFO:name:epoch 0 step 1900 loss 1.14909
INFO:name:epoch 0 step 2000 loss 1.16669
INFO:name:epoch 0 step 2100 loss 1.07571
INFO:name:epoch 0 step 2200 loss 1.05134
INFO:name:epoch 0 step 2300 loss 1.09555
INFO:name:epoch 0 step 2400 loss 1.04268
INFO:name:epoch 0 step 2500 loss 1.00721
INFO:name:epoch 0 step 2600 loss 0.98219
INFO:name:epoch 0 step 2700 loss 0.97663
INFO:name:epoch 0 step 2800 loss 0.92923
INFO:name:epoch 0 step 2900 loss 0.88818
INFO:name:epoch 0 step 3000 loss 0.83786
INFO:name:epoch 0 step 3100 loss 0.85354
INFO:name:epoch 0 step 3200 loss 0.77869
INFO:name:epoch 0 step 3300 loss 0.79094
INFO:name:epoch 0 step 3400 loss 0.8018
INFO:name:epoch 0 step 3500 loss 0.80047
INFO:name:epoch 0 step 3600 loss 0.74604
INFO:name:epoch 0 step 3700 loss 0.75942
INFO:name:epoch 0 step 3800 loss 0.76779
INFO:name:epoch 0 step 3900 loss 0.72684
INFO:name:epoch 0 step 4000 loss 0.69507
INFO:name:epoch 0 step 4100 loss 0.73011
INFO:name:epoch 0 step 4200 loss 0.69041
INFO:name:epoch 0 step 4300 loss 0.72973
INFO:name:epoch 0 step 4400 loss 0.66651
INFO:name:epoch 0 step 4500 loss 0.64181
INFO:name:epoch 0 step 4600 loss 0.64262
INFO:name:epoch 0 step 4700 loss 0.66641
INFO:name:epoch 0 step 4800 loss 0.63049
INFO:name:epoch 0 step 4900 loss 0.62926
INFO:name:epoch 0 step 5000 loss 0.61851
INFO:name:epoch 0 step 5100 loss 0.56738
INFO:name:epoch 0 step 5200 loss 0.62809
INFO:name:epoch 0 step 5300 loss 0.59025
INFO:name:epoch 0 step 5400 loss 0.56317
INFO:name:epoch 0 step 5500 loss 0.57904
INFO:name:epoch 0 step 5600 loss 0.55855
INFO:name:epoch 0 step 5700 loss 0.55632
INFO:name:epoch 0 step 5800 loss 0.54159
INFO:name:epoch 0 step 5900 loss 0.54867
INFO:name:epoch 0 step 6000 loss 0.53273
INFO:name:epoch 0 step 6100 loss 0.48837
INFO:name:epoch 0 step 6200 loss 0.50951
INFO:name:epoch 0 step 6300 loss 0.55217
INFO:name:epoch 0 step 6400 loss 0.49532
INFO:name:epoch 0 step 6500 loss 0.48783
INFO:name:epoch 0 step 6600 loss 0.49057
INFO:name:epoch 0 step 6700 loss 0.52821
INFO:name:epoch 0 step 6800 loss 0.47646
INFO:name:epoch 0 step 6900 loss 0.50547
INFO:name:epoch 0 step 7000 loss 0.50005
INFO:name:epoch 0 step 7100 loss 0.49781
INFO:name:epoch 0 step 7200 loss 0.46614
INFO:name:epoch 0 step 7300 loss 0.44147
INFO:name:epoch 0 step 7400 loss 0.4919
INFO:name:epoch 0 step 7500 loss 0.48952
INFO:name:epoch 0 step 7600 loss 0.46377
INFO:name:epoch 0 step 7700 loss 0.45242
INFO:name:epoch 0 step 7800 loss 0.42231
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0685
INFO:name:  ********************
INFO:name:  Best eval mrr:0.0685
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0443
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 0.39898
INFO:name:epoch 1 step 200 loss 0.39419
INFO:name:epoch 1 step 300 loss 0.34108
INFO:name:epoch 1 step 400 loss 0.33977
INFO:name:epoch 1 step 500 loss 0.3598
INFO:name:epoch 1 step 600 loss 0.35217
INFO:name:epoch 1 step 700 loss 0.34616
INFO:name:epoch 1 step 800 loss 0.32079
INFO:name:epoch 1 step 900 loss 0.35167
INFO:name:epoch 1 step 1000 loss 0.34458
INFO:name:epoch 1 step 1100 loss 0.31601
INFO:name:epoch 1 step 1200 loss 0.29487
INFO:name:epoch 1 step 1300 loss 0.32944
INFO:name:epoch 1 step 1400 loss 0.32991
INFO:name:epoch 1 step 1500 loss 0.32936
INFO:name:epoch 1 step 1600 loss 0.31305
INFO:name:epoch 1 step 1700 loss 0.27893
INFO:name:epoch 1 step 1800 loss 0.30418
INFO:name:epoch 1 step 1900 loss 0.29239
INFO:name:epoch 1 step 2000 loss 0.30288
INFO:name:epoch 1 step 2100 loss 0.29411
INFO:name:epoch 1 step 2200 loss 0.31769
INFO:name:epoch 1 step 2300 loss 0.29082
INFO:name:epoch 1 step 2400 loss 0.29532
INFO:name:epoch 1 step 2500 loss 0.29391
INFO:name:epoch 1 step 2600 loss 0.31311
INFO:name:epoch 1 step 2700 loss 0.28639
INFO:name:epoch 1 step 2800 loss 0.29524
INFO:name:epoch 1 step 2900 loss 0.26752
INFO:name:epoch 1 step 3000 loss 0.30616
INFO:name:epoch 1 step 3100 loss 0.27384
INFO:name:epoch 1 step 3200 loss 0.29953
INFO:name:epoch 1 step 3300 loss 0.29275
INFO:name:epoch 1 step 3400 loss 0.26202
INFO:name:epoch 1 step 3500 loss 0.29825
INFO:name:epoch 1 step 3600 loss 0.28986
INFO:name:epoch 1 step 3700 loss 0.25782
INFO:name:epoch 1 step 3800 loss 0.29097
INFO:name:epoch 1 step 3900 loss 0.27529
INFO:name:epoch 1 step 4000 loss 0.2756
INFO:name:epoch 1 step 4100 loss 0.25488
INFO:name:epoch 1 step 4200 loss 0.27955
INFO:name:epoch 1 step 4300 loss 0.27026
INFO:name:epoch 1 step 4400 loss 0.28091
INFO:name:epoch 1 step 4500 loss 0.24958
INFO:name:epoch 1 step 4600 loss 0.27359
INFO:name:epoch 1 step 4700 loss 0.26766
INFO:name:epoch 1 step 4800 loss 0.26214
INFO:name:epoch 1 step 4900 loss 0.25241
INFO:name:epoch 1 step 5000 loss 0.28298
INFO:name:epoch 1 step 5100 loss 0.26663
INFO:name:epoch 1 step 5200 loss 0.25008
INFO:name:epoch 1 step 5300 loss 0.24096
INFO:name:epoch 1 step 5400 loss 0.2688
INFO:name:epoch 1 step 5500 loss 0.24354
INFO:name:epoch 1 step 5600 loss 0.26598
INFO:name:epoch 1 step 5700 loss 0.25587
INFO:name:epoch 1 step 5800 loss 0.25472
INFO:name:epoch 1 step 5900 loss 0.2543
INFO:name:epoch 1 step 6000 loss 0.2331
INFO:name:epoch 1 step 6100 loss 0.22502
INFO:name:epoch 1 step 6200 loss 0.22862
INFO:name:epoch 1 step 6300 loss 0.25445
INFO:name:epoch 1 step 6400 loss 0.25272
INFO:name:epoch 1 step 6500 loss 0.2481
INFO:name:epoch 1 step 6600 loss 0.24634
INFO:name:epoch 1 step 6700 loss 0.21935
INFO:name:epoch 1 step 6800 loss 0.24624
INFO:name:epoch 1 step 6900 loss 0.21219
INFO:name:epoch 1 step 7000 loss 0.24224
INFO:name:epoch 1 step 7100 loss 0.25079
INFO:name:epoch 1 step 7200 loss 0.23795
INFO:name:epoch 1 step 7300 loss 0.25141
INFO:name:epoch 1 step 7400 loss 0.23156
INFO:name:epoch 1 step 7500 loss 0.23217
INFO:name:epoch 1 step 7600 loss 0.21962
INFO:name:epoch 1 step 7700 loss 0.22076
INFO:name:epoch 1 step 7800 loss 0.21857
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0973
INFO:name:  ********************
INFO:name:  Best eval mrr:0.0973
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0664
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 2 step 100 loss 0.19595
INFO:name:epoch 2 step 200 loss 0.1894
INFO:name:epoch 2 step 300 loss 0.20422
INFO:name:epoch 2 step 400 loss 0.19481
INFO:name:epoch 2 step 500 loss 0.21817
INFO:name:epoch 2 step 600 loss 0.20397
INFO:name:epoch 2 step 700 loss 0.20124
INFO:name:epoch 2 step 800 loss 0.20003
INFO:name:epoch 2 step 900 loss 0.17543
INFO:name:epoch 2 step 1000 loss 0.19498
INFO:name:epoch 2 step 1100 loss 0.20428
INFO:name:epoch 2 step 1200 loss 0.19926
INFO:name:epoch 2 step 1300 loss 0.20188
INFO:name:epoch 2 step 1400 loss 0.18061
INFO:name:epoch 2 step 1500 loss 0.20431
INFO:name:epoch 2 step 1600 loss 0.17765
INFO:name:epoch 2 step 1700 loss 0.21449
INFO:name:epoch 2 step 1800 loss 0.17703
INFO:name:epoch 2 step 1900 loss 0.20246
INFO:name:epoch 2 step 2000 loss 0.20906
INFO:name:epoch 2 step 2100 loss 0.18722
INFO:name:epoch 2 step 2200 loss 0.19573
INFO:name:epoch 2 step 2300 loss 0.20492
INFO:name:epoch 2 step 2400 loss 0.17881
INFO:name:epoch 2 step 2500 loss 0.2201
INFO:name:epoch 2 step 2600 loss 0.19142
INFO:name:epoch 2 step 2700 loss 0.18791
INFO:name:epoch 2 step 2800 loss 0.1752
INFO:name:epoch 2 step 2900 loss 0.19035
INFO:name:epoch 2 step 3000 loss 0.18935
INFO:name:epoch 2 step 3100 loss 0.18305
INFO:name:epoch 2 step 3200 loss 0.18339
INFO:name:epoch 2 step 3300 loss 0.20567
INFO:name:epoch 2 step 3400 loss 0.17499
INFO:name:epoch 2 step 3500 loss 0.18725
INFO:name:epoch 2 step 3600 loss 0.17926
INFO:name:epoch 2 step 3700 loss 0.18047
INFO:name:epoch 2 step 3800 loss 0.20587
INFO:name:epoch 2 step 3900 loss 0.19357
INFO:name:epoch 2 step 4000 loss 0.18879
INFO:name:epoch 2 step 4100 loss 0.17378
INFO:name:epoch 2 step 4200 loss 0.19471
INFO:name:epoch 2 step 4300 loss 0.18989
INFO:name:epoch 2 step 4400 loss 0.1947
INFO:name:epoch 2 step 4500 loss 0.1941
INFO:name:epoch 2 step 4600 loss 0.17702
INFO:name:epoch 2 step 4700 loss 0.18615
INFO:name:epoch 2 step 4800 loss 0.18562
INFO:name:epoch 2 step 4900 loss 0.17529
INFO:name:epoch 2 step 5000 loss 0.20946
INFO:name:epoch 2 step 5100 loss 0.18926
INFO:name:epoch 2 step 5200 loss 0.1611
INFO:name:epoch 2 step 5300 loss 0.19217
INFO:name:epoch 2 step 5400 loss 0.20108
INFO:name:epoch 2 step 5500 loss 0.16887
INFO:name:epoch 2 step 5600 loss 0.17768
INFO:name:epoch 2 step 5700 loss 0.18551
INFO:name:epoch 2 step 5800 loss 0.18837
INFO:name:epoch 2 step 5900 loss 0.18088
INFO:name:epoch 2 step 6000 loss 0.18114
INFO:name:epoch 2 step 6100 loss 0.1905
INFO:name:epoch 2 step 6200 loss 0.16942
INFO:name:epoch 2 step 6300 loss 0.17713
INFO:name:epoch 2 step 6400 loss 0.1891
INFO:name:epoch 2 step 6500 loss 0.17198
INFO:name:epoch 2 step 6600 loss 0.17248
INFO:name:epoch 2 step 6700 loss 0.1641
INFO:name:epoch 2 step 6800 loss 0.2013
INFO:name:epoch 2 step 6900 loss 0.17443
INFO:name:epoch 2 step 7000 loss 0.17829
INFO:name:epoch 2 step 7100 loss 0.19711
INFO:name:epoch 2 step 7200 loss 0.1788
INFO:name:epoch 2 step 7300 loss 0.19984
INFO:name:epoch 2 step 7400 loss 0.19459
INFO:name:epoch 2 step 7500 loss 0.15633
INFO:name:epoch 2 step 7600 loss 0.18502
INFO:name:epoch 2 step 7700 loss 0.18266
INFO:name:epoch 2 step 7800 loss 0.19224
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1324
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1324
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0934
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.14794
INFO:name:epoch 3 step 200 loss 0.14072
INFO:name:epoch 3 step 300 loss 0.14771
INFO:name:epoch 3 step 400 loss 0.15336
INFO:name:epoch 3 step 500 loss 0.12288
INFO:name:epoch 3 step 600 loss 0.14201
INFO:name:epoch 3 step 700 loss 0.1551
INFO:name:epoch 3 step 800 loss 0.15623
INFO:name:epoch 3 step 900 loss 0.15
INFO:name:epoch 3 step 1000 loss 0.14262
INFO:name:epoch 3 step 1100 loss 0.16661
INFO:name:epoch 3 step 1200 loss 0.16411
INFO:name:epoch 3 step 1300 loss 0.14784
INFO:name:epoch 3 step 1400 loss 0.12947
INFO:name:epoch 3 step 1500 loss 0.14573
INFO:name:epoch 3 step 1600 loss 0.15399
INFO:name:epoch 3 step 1700 loss 0.14867
INFO:name:epoch 3 step 1800 loss 0.16614
INFO:name:epoch 3 step 1900 loss 0.15228
INFO:name:epoch 3 step 2000 loss 0.14777
INFO:name:epoch 3 step 2100 loss 0.13066
INFO:name:epoch 3 step 2200 loss 0.14185
INFO:name:epoch 3 step 2300 loss 0.134
INFO:name:epoch 3 step 2400 loss 0.15117
INFO:name:epoch 3 step 2500 loss 0.14173
INFO:name:epoch 3 step 2600 loss 0.15665
INFO:name:epoch 3 step 2700 loss 0.14941
INFO:name:epoch 3 step 2800 loss 0.13243
INFO:name:epoch 3 step 2900 loss 0.16151
INFO:name:epoch 3 step 3000 loss 0.13513
INFO:name:epoch 3 step 3100 loss 0.156
INFO:name:epoch 3 step 3200 loss 0.14272
INFO:name:epoch 3 step 3300 loss 0.13604
INFO:name:epoch 3 step 3400 loss 0.1391
INFO:name:epoch 3 step 3500 loss 0.13239
INFO:name:epoch 3 step 3600 loss 0.15275
INFO:name:epoch 3 step 3700 loss 0.14129
INFO:name:epoch 3 step 3800 loss 0.13393
INFO:name:epoch 3 step 3900 loss 0.1201
INFO:name:epoch 3 step 4000 loss 0.16405
INFO:name:epoch 3 step 4100 loss 0.13211
INFO:name:epoch 3 step 4200 loss 0.13554
INFO:name:epoch 3 step 4300 loss 0.14224
INFO:name:epoch 3 step 4400 loss 0.14378
INFO:name:epoch 3 step 4500 loss 0.14002
INFO:name:epoch 3 step 4600 loss 0.16501
INFO:name:epoch 3 step 4700 loss 0.15203
INFO:name:epoch 3 step 4800 loss 0.15169
INFO:name:epoch 3 step 4900 loss 0.16651
INFO:name:epoch 3 step 5000 loss 0.14229
INFO:name:epoch 3 step 5100 loss 0.14591
INFO:name:epoch 3 step 5200 loss 0.15604
INFO:name:epoch 3 step 5300 loss 0.15872
INFO:name:epoch 3 step 5400 loss 0.13736
INFO:name:epoch 3 step 5500 loss 0.15092
INFO:name:epoch 3 step 5600 loss 0.12008
INFO:name:epoch 3 step 5700 loss 0.14223
INFO:name:epoch 3 step 5800 loss 0.15367
INFO:name:epoch 3 step 5900 loss 0.14556
INFO:name:epoch 3 step 6000 loss 0.14785
INFO:name:epoch 3 step 6100 loss 0.14709
INFO:name:epoch 3 step 6200 loss 0.14764
INFO:name:epoch 3 step 6300 loss 0.14563
INFO:name:epoch 3 step 6400 loss 0.13058
INFO:name:epoch 3 step 6500 loss 0.13513
INFO:name:epoch 3 step 6600 loss 0.13963
INFO:name:epoch 3 step 6700 loss 0.1503
INFO:name:epoch 3 step 6800 loss 0.1335
INFO:name:epoch 3 step 6900 loss 0.1296
INFO:name:epoch 3 step 7000 loss 0.12447
INFO:name:epoch 3 step 7100 loss 0.14273
INFO:name:epoch 3 step 7200 loss 0.14316
INFO:name:epoch 3 step 7300 loss 0.14475
INFO:name:epoch 3 step 7400 loss 0.12543
INFO:name:epoch 3 step 7500 loss 0.11455
INFO:name:epoch 3 step 7600 loss 0.14467
INFO:name:epoch 3 step 7700 loss 0.14782
INFO:name:epoch 3 step 7800 loss 0.12997
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1503
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1503
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1075
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 4 step 100 loss 0.134
INFO:name:epoch 4 step 200 loss 0.12297
INFO:name:epoch 4 step 300 loss 0.11684
INFO:name:epoch 4 step 400 loss 0.10013
INFO:name:epoch 4 step 500 loss 0.12651
INFO:name:epoch 4 step 600 loss 0.11527
INFO:name:epoch 4 step 700 loss 0.10586
INFO:name:epoch 4 step 800 loss 0.12101
INFO:name:epoch 4 step 900 loss 0.12245
INFO:name:epoch 4 step 1000 loss 0.12358
INFO:name:epoch 4 step 1100 loss 0.09371
INFO:name:epoch 4 step 1200 loss 0.12557
INFO:name:epoch 4 step 1300 loss 0.13945
INFO:name:epoch 4 step 1400 loss 0.11709
INFO:name:epoch 4 step 1500 loss 0.10041
INFO:name:epoch 4 step 1600 loss 0.10005
INFO:name:epoch 4 step 1700 loss 0.11721
INFO:name:epoch 4 step 1800 loss 0.1228
INFO:name:epoch 4 step 1900 loss 0.12322
INFO:name:epoch 4 step 2000 loss 0.11356
INFO:name:epoch 4 step 2100 loss 0.13152
INFO:name:epoch 4 step 2200 loss 0.12688
INFO:name:epoch 4 step 2300 loss 0.11321
INFO:name:epoch 4 step 2400 loss 0.12633
INFO:name:epoch 4 step 2500 loss 0.11114
INFO:name:epoch 4 step 2600 loss 0.11393
INFO:name:epoch 4 step 2700 loss 0.10056
INFO:name:epoch 4 step 2800 loss 0.12046
INFO:name:epoch 4 step 2900 loss 0.0999
INFO:name:epoch 4 step 3000 loss 0.12106
INFO:name:epoch 4 step 3100 loss 0.09839
INFO:name:epoch 4 step 3200 loss 0.11459
INFO:name:epoch 4 step 3300 loss 0.12199
INFO:name:epoch 4 step 3400 loss 0.12563
INFO:name:epoch 4 step 3500 loss 0.10176
INFO:name:epoch 4 step 3600 loss 0.12309
INFO:name:epoch 4 step 3700 loss 0.1193
INFO:name:epoch 4 step 3800 loss 0.11629
INFO:name:epoch 4 step 3900 loss 0.12305
INFO:name:epoch 4 step 4000 loss 0.12089
INFO:name:epoch 4 step 4100 loss 0.12452
INFO:name:epoch 4 step 4200 loss 0.10547
INFO:name:epoch 4 step 4300 loss 0.11191
INFO:name:epoch 4 step 4400 loss 0.1193
INFO:name:epoch 4 step 4500 loss 0.11045
INFO:name:epoch 4 step 4600 loss 0.11107
INFO:name:epoch 4 step 4700 loss 0.11816
INFO:name:epoch 4 step 4800 loss 0.12181
INFO:name:epoch 4 step 4900 loss 0.10689
INFO:name:epoch 4 step 5000 loss 0.1125
INFO:name:epoch 4 step 5100 loss 0.11464
INFO:name:epoch 4 step 5200 loss 0.10228
INFO:name:epoch 4 step 5300 loss 0.11765
INFO:name:epoch 4 step 5400 loss 0.11602
INFO:name:epoch 4 step 5500 loss 0.09828
INFO:name:epoch 4 step 5600 loss 0.10686
INFO:name:epoch 4 step 5700 loss 0.11714
INFO:name:epoch 4 step 5800 loss 0.10206
INFO:name:epoch 4 step 5900 loss 0.1293
INFO:name:epoch 4 step 6000 loss 0.12677
INFO:name:epoch 4 step 6100 loss 0.11226
INFO:name:epoch 4 step 6200 loss 0.1219
INFO:name:epoch 4 step 6300 loss 0.11897
INFO:name:epoch 4 step 6400 loss 0.10147
INFO:name:epoch 4 step 6500 loss 0.10887
INFO:name:epoch 4 step 6600 loss 0.11125
INFO:name:epoch 4 step 6700 loss 0.11284
INFO:name:epoch 4 step 6800 loss 0.10175
INFO:name:epoch 4 step 6900 loss 0.13069
INFO:name:epoch 4 step 7000 loss 0.10809
INFO:name:epoch 4 step 7100 loss 0.10957
INFO:name:epoch 4 step 7200 loss 0.11599
INFO:name:epoch 4 step 7300 loss 0.11495
INFO:name:epoch 4 step 7400 loss 0.10934
INFO:name:epoch 4 step 7500 loss 0.11042
INFO:name:epoch 4 step 7600 loss 0.10555
INFO:name:epoch 4 step 7700 loss 0.12153
INFO:name:epoch 4 step 7800 loss 0.10818
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.174
INFO:name:  ********************
INFO:name:  Best eval mrr:0.174
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1311
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 5 step 100 loss 0.09609
INFO:name:epoch 5 step 200 loss 0.07766
INFO:name:epoch 5 step 300 loss 0.09691
INFO:name:epoch 5 step 400 loss 0.09747
INFO:name:epoch 5 step 500 loss 0.09762
INFO:name:epoch 5 step 600 loss 0.08945
INFO:name:epoch 5 step 700 loss 0.08564
INFO:name:epoch 5 step 800 loss 0.10408
INFO:name:epoch 5 step 900 loss 0.09627
INFO:name:epoch 5 step 1000 loss 0.08976
INFO:name:epoch 5 step 1100 loss 0.0818
INFO:name:epoch 5 step 1200 loss 0.09053
INFO:name:epoch 5 step 1300 loss 0.08676
INFO:name:epoch 5 step 1400 loss 0.09083
INFO:name:epoch 5 step 1500 loss 0.11442
INFO:name:epoch 5 step 1600 loss 0.09291
INFO:name:epoch 5 step 1700 loss 0.10042
INFO:name:epoch 5 step 1800 loss 0.1011
INFO:name:epoch 5 step 1900 loss 0.10648
INFO:name:epoch 5 step 2000 loss 0.08428
INFO:name:epoch 5 step 2100 loss 0.08663
INFO:name:epoch 5 step 2200 loss 0.08233
INFO:name:epoch 5 step 2300 loss 0.09939
INFO:name:epoch 5 step 2400 loss 0.09782
INFO:name:epoch 5 step 2500 loss 0.08241
INFO:name:epoch 5 step 2600 loss 0.08646
INFO:name:epoch 5 step 2700 loss 0.10517
INFO:name:epoch 5 step 2800 loss 0.10313
INFO:name:epoch 5 step 2900 loss 0.10466
INFO:name:epoch 5 step 3000 loss 0.10277
INFO:name:epoch 5 step 3100 loss 0.09579
INFO:name:epoch 5 step 3200 loss 0.08678
INFO:name:epoch 5 step 3300 loss 0.09282
INFO:name:epoch 5 step 3400 loss 0.09222
INFO:name:epoch 5 step 3500 loss 0.08462
INFO:name:epoch 5 step 3600 loss 0.09653
INFO:name:epoch 5 step 3700 loss 0.08835
INFO:name:epoch 5 step 3800 loss 0.08741
INFO:name:epoch 5 step 3900 loss 0.08453
INFO:name:epoch 5 step 4000 loss 0.10011
INFO:name:epoch 5 step 4100 loss 0.09119
INFO:name:epoch 5 step 4200 loss 0.08199
INFO:name:epoch 5 step 4300 loss 0.08918
INFO:name:epoch 5 step 4400 loss 0.09465
INFO:name:epoch 5 step 4500 loss 0.08861
INFO:name:epoch 5 step 4600 loss 0.1125
INFO:name:epoch 5 step 4700 loss 0.0997
INFO:name:epoch 5 step 4800 loss 0.0983
INFO:name:epoch 5 step 4900 loss 0.1105
INFO:name:epoch 5 step 5000 loss 0.09473
INFO:name:epoch 5 step 5100 loss 0.09656
INFO:name:epoch 5 step 5200 loss 0.10995
INFO:name:epoch 5 step 5300 loss 0.08693
INFO:name:epoch 5 step 5400 loss 0.08319
INFO:name:epoch 5 step 5500 loss 0.08773
INFO:name:epoch 5 step 5600 loss 0.09378
INFO:name:epoch 5 step 5700 loss 0.08165
INFO:name:epoch 5 step 5800 loss 0.09532
INFO:name:epoch 5 step 5900 loss 0.08408
INFO:name:epoch 5 step 6000 loss 0.11248
INFO:name:epoch 5 step 6100 loss 0.10298
INFO:name:epoch 5 step 6200 loss 0.09368
INFO:name:epoch 5 step 6300 loss 0.08835
INFO:name:epoch 5 step 6400 loss 0.08561
INFO:name:epoch 5 step 6500 loss 0.09534
INFO:name:epoch 5 step 6600 loss 0.08552
INFO:name:epoch 5 step 6700 loss 0.09584
INFO:name:epoch 5 step 6800 loss 0.08025
INFO:name:epoch 5 step 6900 loss 0.09267
INFO:name:epoch 5 step 7000 loss 0.09199
INFO:name:epoch 5 step 7100 loss 0.11018
INFO:name:epoch 5 step 7200 loss 0.09957
INFO:name:epoch 5 step 7300 loss 0.08581
INFO:name:epoch 5 step 7400 loss 0.0967
INFO:name:epoch 5 step 7500 loss 0.11035
INFO:name:epoch 5 step 7600 loss 0.08381
INFO:name:epoch 5 step 7700 loss 0.09767
INFO:name:epoch 5 step 7800 loss 0.08236
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1828
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1828
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1395
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 6 step 100 loss 0.08687
INFO:name:epoch 6 step 200 loss 0.08351
INFO:name:epoch 6 step 300 loss 0.08597
INFO:name:epoch 6 step 400 loss 0.07406
INFO:name:epoch 6 step 500 loss 0.0751
INFO:name:epoch 6 step 600 loss 0.06995
INFO:name:epoch 6 step 700 loss 0.08072
INFO:name:epoch 6 step 800 loss 0.08256
INFO:name:epoch 6 step 900 loss 0.08211
INFO:name:epoch 6 step 1000 loss 0.08006
INFO:name:epoch 6 step 1100 loss 0.07728
INFO:name:epoch 6 step 1200 loss 0.08536
INFO:name:epoch 6 step 1300 loss 0.0851
INFO:name:epoch 6 step 1400 loss 0.07415
INFO:name:epoch 6 step 1500 loss 0.08935
INFO:name:epoch 6 step 1600 loss 0.08036
INFO:name:epoch 6 step 1700 loss 0.07944
INFO:name:epoch 6 step 1800 loss 0.07251
INFO:name:epoch 6 step 1900 loss 0.07137
INFO:name:epoch 6 step 2000 loss 0.0779
INFO:name:epoch 6 step 2100 loss 0.07045
INFO:name:epoch 6 step 2200 loss 0.08284
INFO:name:epoch 6 step 2300 loss 0.08304
INFO:name:epoch 6 step 2400 loss 0.07116
INFO:name:epoch 6 step 2500 loss 0.08786
INFO:name:epoch 6 step 2600 loss 0.08529
INFO:name:epoch 6 step 2700 loss 0.07018
INFO:name:epoch 6 step 2800 loss 0.07634
INFO:name:epoch 6 step 2900 loss 0.07839
INFO:name:epoch 6 step 3000 loss 0.07306
INFO:name:epoch 6 step 3100 loss 0.08366
INFO:name:epoch 6 step 3200 loss 0.08694
INFO:name:epoch 6 step 3300 loss 0.08565
INFO:name:epoch 6 step 3400 loss 0.08585
INFO:name:epoch 6 step 3500 loss 0.07538
INFO:name:epoch 6 step 3600 loss 0.07338
INFO:name:epoch 6 step 3700 loss 0.07878
INFO:name:epoch 6 step 3800 loss 0.07392
INFO:name:epoch 6 step 3900 loss 0.07912
INFO:name:epoch 6 step 4000 loss 0.07771
INFO:name:epoch 6 step 4100 loss 0.09095
INFO:name:epoch 6 step 4200 loss 0.06802
INFO:name:epoch 6 step 4300 loss 0.06897
INFO:name:epoch 6 step 4400 loss 0.09395
INFO:name:epoch 6 step 4500 loss 0.06839
INFO:name:epoch 6 step 4600 loss 0.07068
INFO:name:epoch 6 step 4700 loss 0.08946
INFO:name:epoch 6 step 4800 loss 0.08268
INFO:name:epoch 6 step 4900 loss 0.08034
INFO:name:epoch 6 step 5000 loss 0.08516
INFO:name:epoch 6 step 5100 loss 0.07146
INFO:name:epoch 6 step 5200 loss 0.08333
INFO:name:epoch 6 step 5300 loss 0.06662
INFO:name:epoch 6 step 5400 loss 0.08091
INFO:name:epoch 6 step 5500 loss 0.08325
INFO:name:epoch 6 step 5600 loss 0.07885
INFO:name:epoch 6 step 5700 loss 0.09216
INFO:name:epoch 6 step 5800 loss 0.08972
INFO:name:epoch 6 step 5900 loss 0.0692
INFO:name:epoch 6 step 6000 loss 0.08164
INFO:name:epoch 6 step 6100 loss 0.08286
INFO:name:epoch 6 step 6200 loss 0.07012
INFO:name:epoch 6 step 6300 loss 0.07825
INFO:name:epoch 6 step 6400 loss 0.08567
INFO:name:epoch 6 step 6500 loss 0.07354
INFO:name:epoch 6 step 6600 loss 0.07469
INFO:name:epoch 6 step 6700 loss 0.08856
INFO:name:epoch 6 step 6800 loss 0.0705
INFO:name:epoch 6 step 6900 loss 0.07831
INFO:name:epoch 6 step 7000 loss 0.08659
INFO:name:epoch 6 step 7100 loss 0.08119
INFO:name:epoch 6 step 7200 loss 0.0868
INFO:name:epoch 6 step 7300 loss 0.0757
INFO:name:epoch 6 step 7400 loss 0.08954
INFO:name:epoch 6 step 7500 loss 0.08475
INFO:name:epoch 6 step 7600 loss 0.07732
INFO:name:epoch 6 step 7700 loss 0.06357
INFO:name:epoch 6 step 7800 loss 0.09497
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1866
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1866
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1406
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 7 step 100 loss 0.0708
INFO:name:epoch 7 step 200 loss 0.06093
INFO:name:epoch 7 step 300 loss 0.06937
INFO:name:epoch 7 step 400 loss 0.06458
INFO:name:epoch 7 step 500 loss 0.06468
INFO:name:epoch 7 step 600 loss 0.06599
INFO:name:epoch 7 step 700 loss 0.06878
INFO:name:epoch 7 step 800 loss 0.06843
INFO:name:epoch 7 step 900 loss 0.05788
INFO:name:epoch 7 step 1000 loss 0.05313
INFO:name:epoch 7 step 1100 loss 0.04909
INFO:name:epoch 7 step 1200 loss 0.06458
INFO:name:epoch 7 step 1300 loss 0.07062
INFO:name:epoch 7 step 1400 loss 0.0698
INFO:name:epoch 7 step 1500 loss 0.07778
INFO:name:epoch 7 step 1600 loss 0.07265
INFO:name:epoch 7 step 1700 loss 0.06934
INFO:name:epoch 7 step 1800 loss 0.06371
INFO:name:epoch 7 step 1900 loss 0.07151
INFO:name:epoch 7 step 2000 loss 0.06164
INFO:name:epoch 7 step 2100 loss 0.07081
INFO:name:epoch 7 step 2200 loss 0.06069
INFO:name:epoch 7 step 2300 loss 0.07763
INFO:name:epoch 7 step 2400 loss 0.07969
INFO:name:epoch 7 step 2500 loss 0.05812
INFO:name:epoch 7 step 2600 loss 0.06167
INFO:name:epoch 7 step 2700 loss 0.07726
INFO:name:epoch 7 step 2800 loss 0.05817
INFO:name:epoch 7 step 2900 loss 0.06815
INFO:name:epoch 7 step 3000 loss 0.07823
INFO:name:epoch 7 step 3100 loss 0.06913
INFO:name:epoch 7 step 3200 loss 0.0744
INFO:name:epoch 7 step 3300 loss 0.0627
INFO:name:epoch 7 step 3400 loss 0.0608
INFO:name:epoch 7 step 3500 loss 0.06574
INFO:name:epoch 7 step 3600 loss 0.07513
INFO:name:epoch 7 step 3700 loss 0.0555
INFO:name:epoch 7 step 3800 loss 0.06677
INFO:name:epoch 7 step 3900 loss 0.0636
INFO:name:epoch 7 step 4000 loss 0.06716
INFO:name:epoch 7 step 4100 loss 0.0603
INFO:name:epoch 7 step 4200 loss 0.07088
INFO:name:epoch 7 step 4300 loss 0.07567
INFO:name:epoch 7 step 4400 loss 0.07893
INFO:name:epoch 7 step 4500 loss 0.06671
INFO:name:epoch 7 step 4600 loss 0.07182
INFO:name:epoch 7 step 4700 loss 0.06318
INFO:name:epoch 7 step 4800 loss 0.06228
INFO:name:epoch 7 step 4900 loss 0.07088
INFO:name:epoch 7 step 5000 loss 0.06539
INFO:name:epoch 7 step 5100 loss 0.06528
INFO:name:epoch 7 step 5200 loss 0.07071
INFO:name:epoch 7 step 5300 loss 0.07585
INFO:name:epoch 7 step 5400 loss 0.06539
INFO:name:epoch 7 step 5500 loss 0.0701
INFO:name:epoch 7 step 5600 loss 0.07339
INFO:name:epoch 7 step 5700 loss 0.05451
INFO:name:epoch 7 step 5800 loss 0.05874
INFO:name:epoch 7 step 5900 loss 0.06031
INFO:name:epoch 7 step 6000 loss 0.07543
INFO:name:epoch 7 step 6100 loss 0.06092
INFO:name:epoch 7 step 6200 loss 0.06891
INFO:name:epoch 7 step 6300 loss 0.05871
INFO:name:epoch 7 step 6400 loss 0.05685
INFO:name:epoch 7 step 6500 loss 0.06742
INFO:name:epoch 7 step 6600 loss 0.07035
INFO:name:epoch 7 step 6700 loss 0.07143
INFO:name:epoch 7 step 6800 loss 0.06246
INFO:name:epoch 7 step 6900 loss 0.07032
INFO:name:epoch 7 step 7000 loss 0.05946
INFO:name:epoch 7 step 7100 loss 0.06247
INFO:name:epoch 7 step 7200 loss 0.06238
INFO:name:epoch 7 step 7300 loss 0.0716
INFO:name:epoch 7 step 7400 loss 0.06766
INFO:name:epoch 7 step 7500 loss 0.0574
INFO:name:epoch 7 step 7600 loss 0.07807
INFO:name:epoch 7 step 7700 loss 0.06126
INFO:name:epoch 7 step 7800 loss 0.06412
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1827
INFO:name:epoch 8 step 100 loss 0.0633
INFO:name:epoch 8 step 200 loss 0.06116
INFO:name:epoch 8 step 300 loss 0.05361
INFO:name:epoch 8 step 400 loss 0.0607
INFO:name:epoch 8 step 500 loss 0.05478
INFO:name:epoch 8 step 600 loss 0.06115
INFO:name:epoch 8 step 700 loss 0.06902
INFO:name:epoch 8 step 800 loss 0.0619
INFO:name:epoch 8 step 900 loss 0.06309
INFO:name:epoch 8 step 1000 loss 0.06007
INFO:name:epoch 8 step 1100 loss 0.04618
INFO:name:epoch 8 step 1200 loss 0.05128
INFO:name:epoch 8 step 1300 loss 0.06611
INFO:name:epoch 8 step 1400 loss 0.05711
INFO:name:epoch 8 step 1500 loss 0.05854
INFO:name:epoch 8 step 1600 loss 0.05487
INFO:name:epoch 8 step 1700 loss 0.04706
INFO:name:epoch 8 step 1800 loss 0.05251
INFO:name:epoch 8 step 1900 loss 0.07033
INFO:name:epoch 8 step 2000 loss 0.06502
INFO:name:epoch 8 step 2100 loss 0.0646
INFO:name:epoch 8 step 2200 loss 0.0657
INFO:name:epoch 8 step 2300 loss 0.05847
INFO:name:epoch 8 step 2400 loss 0.06044
INFO:name:epoch 8 step 2500 loss 0.06694
INFO:name:epoch 8 step 2600 loss 0.04687
INFO:name:epoch 8 step 2700 loss 0.06158
INFO:name:epoch 8 step 2800 loss 0.05761
INFO:name:epoch 8 step 2900 loss 0.05238
INFO:name:epoch 8 step 3000 loss 0.05614
INFO:name:epoch 8 step 3100 loss 0.0721
INFO:name:epoch 8 step 3200 loss 0.06378
INFO:name:epoch 8 step 3300 loss 0.06633
INFO:name:epoch 8 step 3400 loss 0.05834
INFO:name:epoch 8 step 3500 loss 0.06791
INFO:name:epoch 8 step 3600 loss 0.06221
INFO:name:epoch 8 step 3700 loss 0.06576
INFO:name:epoch 8 step 3800 loss 0.05863
INFO:name:epoch 8 step 3900 loss 0.04708
INFO:name:epoch 8 step 4000 loss 0.05868
INFO:name:epoch 8 step 4100 loss 0.05623
INFO:name:epoch 8 step 4200 loss 0.06363
INFO:name:epoch 8 step 4300 loss 0.05604
INFO:name:epoch 8 step 4400 loss 0.04818
INFO:name:epoch 8 step 4500 loss 0.05319
INFO:name:epoch 8 step 4600 loss 0.05782
INFO:name:epoch 8 step 4700 loss 0.05201
INFO:name:epoch 8 step 4800 loss 0.05374
INFO:name:epoch 8 step 4900 loss 0.05407
INFO:name:epoch 8 step 5000 loss 0.06406
INFO:name:epoch 8 step 5100 loss 0.06758
INFO:name:epoch 8 step 5200 loss 0.05105
INFO:name:epoch 8 step 5300 loss 0.05581
INFO:name:epoch 8 step 5400 loss 0.06693
INFO:name:epoch 8 step 5500 loss 0.05855
INFO:name:epoch 8 step 5600 loss 0.05456
INFO:name:epoch 8 step 5700 loss 0.06295
INFO:name:epoch 8 step 5800 loss 0.05012
INFO:name:epoch 8 step 5900 loss 0.05248
INFO:name:epoch 8 step 6000 loss 0.06332
INFO:name:epoch 8 step 6100 loss 0.06738
INFO:name:epoch 8 step 6200 loss 0.05531
INFO:name:epoch 8 step 6300 loss 0.05334
INFO:name:epoch 8 step 6400 loss 0.05002
INFO:name:epoch 8 step 6500 loss 0.06366
INFO:name:epoch 8 step 6600 loss 0.05368
INFO:name:epoch 8 step 6700 loss 0.06326
INFO:name:epoch 8 step 6800 loss 0.05606
INFO:name:epoch 8 step 6900 loss 0.04967
INFO:name:epoch 8 step 7000 loss 0.05097
INFO:name:epoch 8 step 7100 loss 0.04662
INFO:name:epoch 8 step 7200 loss 0.06456
INFO:name:epoch 8 step 7300 loss 0.05774
INFO:name:epoch 8 step 7400 loss 0.0535
INFO:name:epoch 8 step 7500 loss 0.05595
INFO:name:epoch 8 step 7600 loss 0.06086
INFO:name:epoch 8 step 7700 loss 0.05563
INFO:name:epoch 8 step 7800 loss 0.06057
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1919
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1919
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1456
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 9 step 100 loss 0.05618
INFO:name:epoch 9 step 200 loss 0.05883
INFO:name:epoch 9 step 300 loss 0.05108
INFO:name:epoch 9 step 400 loss 0.04555
INFO:name:epoch 9 step 500 loss 0.05159
INFO:name:epoch 9 step 600 loss 0.05892
INFO:name:epoch 9 step 700 loss 0.05571
INFO:name:epoch 9 step 800 loss 0.05244
INFO:name:epoch 9 step 900 loss 0.04585
INFO:name:epoch 9 step 1000 loss 0.04411
INFO:name:epoch 9 step 1100 loss 0.04407
INFO:name:epoch 9 step 1200 loss 0.05676
INFO:name:epoch 9 step 1300 loss 0.05434
INFO:name:epoch 9 step 1400 loss 0.05789
INFO:name:epoch 9 step 1500 loss 0.03992
INFO:name:epoch 9 step 1600 loss 0.05742
INFO:name:epoch 9 step 1700 loss 0.05164
INFO:name:epoch 9 step 1800 loss 0.04738
INFO:name:epoch 9 step 1900 loss 0.05518
INFO:name:epoch 9 step 2000 loss 0.06031
INFO:name:epoch 9 step 2100 loss 0.05
INFO:name:epoch 9 step 2200 loss 0.04696
INFO:name:epoch 9 step 2300 loss 0.06124
INFO:name:epoch 9 step 2400 loss 0.05571
INFO:name:epoch 9 step 2500 loss 0.04757
INFO:name:epoch 9 step 2600 loss 0.04341
INFO:name:epoch 9 step 2700 loss 0.04733
INFO:name:epoch 9 step 2800 loss 0.05481
INFO:name:epoch 9 step 2900 loss 0.05549
INFO:name:epoch 9 step 3000 loss 0.05457
INFO:name:epoch 9 step 3100 loss 0.05541
INFO:name:epoch 9 step 3200 loss 0.05092
INFO:name:epoch 9 step 3300 loss 0.05583
INFO:name:epoch 9 step 3400 loss 0.05129
INFO:name:epoch 9 step 3500 loss 0.05799
INFO:name:epoch 9 step 3600 loss 0.05342
INFO:name:epoch 9 step 3700 loss 0.05545
INFO:name:epoch 9 step 3800 loss 0.05545
INFO:name:epoch 9 step 3900 loss 0.05572
INFO:name:epoch 9 step 4000 loss 0.04971
INFO:name:epoch 9 step 4100 loss 0.05198
INFO:name:epoch 9 step 4200 loss 0.04747
INFO:name:epoch 9 step 4300 loss 0.04989
INFO:name:epoch 9 step 4400 loss 0.04998
INFO:name:epoch 9 step 4500 loss 0.04589
INFO:name:epoch 9 step 4600 loss 0.05297
INFO:name:epoch 9 step 4700 loss 0.05513
INFO:name:epoch 9 step 4800 loss 0.05457
INFO:name:epoch 9 step 4900 loss 0.05864
INFO:name:epoch 9 step 5000 loss 0.05436
INFO:name:epoch 9 step 5100 loss 0.05813
INFO:name:epoch 9 step 5200 loss 0.05218
INFO:name:epoch 9 step 5300 loss 0.05639
INFO:name:epoch 9 step 5400 loss 0.04472
INFO:name:epoch 9 step 5500 loss 0.04869
INFO:name:epoch 9 step 5600 loss 0.04441
INFO:name:epoch 9 step 5700 loss 0.04119
INFO:name:epoch 9 step 5800 loss 0.05899
INFO:name:epoch 9 step 5900 loss 0.0619
INFO:name:epoch 9 step 6000 loss 0.04611
INFO:name:epoch 9 step 6100 loss 0.04627
INFO:name:epoch 9 step 6200 loss 0.05488
INFO:name:epoch 9 step 6300 loss 0.04567
INFO:name:epoch 9 step 6400 loss 0.05416
INFO:name:epoch 9 step 6500 loss 0.05476
INFO:name:epoch 9 step 6600 loss 0.04421
INFO:name:epoch 9 step 6700 loss 0.05183
INFO:name:epoch 9 step 6800 loss 0.04747
INFO:name:epoch 9 step 6900 loss 0.04888
INFO:name:epoch 9 step 7000 loss 0.04969
INFO:name:epoch 9 step 7100 loss 0.05262
INFO:name:epoch 9 step 7200 loss 0.04676
INFO:name:epoch 9 step 7300 loss 0.05534
INFO:name:epoch 9 step 7400 loss 0.05615
INFO:name:epoch 9 step 7500 loss 0.04727
INFO:name:epoch 9 step 7600 loss 0.05671
INFO:name:epoch 9 step 7700 loss 0.04669
INFO:name:epoch 9 step 7800 loss 0.04953
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2
INFO:name:  ********************
INFO:name:  Best eval mrr:0.2
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1521
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
train results ([1.0878273228517104, 0.2793440523434293, 0.18854870332015639, 0.14397650935919068, 0.11477199672914218, 0.09364729991359896, 0.07954044072137935, 0.06642908563522629, 0.05845538115583294, 0.051762497689328814], [0.0684532703496401, 0.09733343010236986, 0.13242952436727448, 0.15031993080262823, 0.1740279276248153, 0.18284659998719302, 0.18662733909258084, 0.18272385302783414, 0.19187850142377968, 0.20004936268772597])
