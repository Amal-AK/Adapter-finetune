/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
INFO:name:device: cuda:3, n_gpu: 1
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/config.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/vocab.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/merges.txt HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/tokenizer.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/added_tokens.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/special_tokens_map.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/model.safetensors HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/model.safetensors.index.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/pytorch_model.bin HTTP/1.1" 302 0
INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

root
├── shared (Embedding) weight:[32100, 768]
├── encoder (T5Stack)
│   ├── embed_tokens (Embedding) weight:[32100, 768]
│   ├── block (ModuleList)
│   │   ├── 0 (T5Block)
│   │   │   └── layer (ModuleList)
│   │   │       ├── 0 (T5LayerSelfAttention)
│   │   │       │   ├── SelfAttention (T5Attention)
│   │   │       │   │   ├── q,k,v,o(Linear) weight:[768, 768]
│   │   │       │   │   └── relative_attention_bias (Embedding) weight:[32, 12]
│   │   │       │   ├── layer_norm (T5LayerNorm) weight:[768]
│   │   │       │   └── reparams (ReparameterizeFunction) input_tokens:[6]
│   │   │       │       ├── module_list (ModuleList)
│   │   │       │       ├── wte (Embedding) weight:[6, 512]
│   │   │       │       └── control_trans (Sequential)
│   │   │       │           ├── 0 (Linear) weight:[512, 512] bias:[512]
│   │   │       │           └── 2 (Linear) weight:[36864, 512] bias:[36864]
│   │   │       └── 1 (T5LayerFF)
│   │   │           ├── DenseReluDense (T5DenseActDense)
│   │   │           │   ├── wi (Linear) weight:[3072, 768]
│   │   │           │   └── wo (Linear) weight:[768, 3072]
│   │   │           └── layer_norm (T5LayerNorm) weight:[768]
│   │   └── 1-11(T5Block)
│   │       └── layer (ModuleList)
│   │           ├── 0 (T5LayerSelfAttention)
│   │           │   ├── SelfAttention (T5Attention)
│   │           │   │   └── q,k,v,o(Linear) weight:[768, 768]
│   │           │   └── layer_norm (T5LayerNorm) weight:[768]
│   │           └── 1 (T5LayerFF)
│   │               ├── DenseReluDense (T5DenseActDense)
│   │               │   ├── wi (Linear) weight:[3072, 768]
│   │               │   └── wo (Linear) weight:[768, 3072]
│   │               └── layer_norm (T5LayerNorm) weight:[768]
│   └── final_layer_norm (T5LayerNorm) weight:[768]
└── decoder (T5Stack)
    ├── embed_tokens (Embedding) weight:[32100, 768]
    ├── block (ModuleList)
    │   ├── 0 (T5Block)
    │   │   └── layer (ModuleList)
    │   │       ├── 0 (T5LayerSelfAttention)
    │   │       │   ├── SelfAttention (T5Attention)
    │   │       │   │   ├── q,k,v,o(Linear) weight:[768, 768]
    │   │       │   │   └── relative_attention_bias (Embedding) weight:[32, 12]
    │   │       │   └── layer_norm (T5LayerNorm) weight:[768]
    │   │       ├── 1 (T5LayerCrossAttention)
    │   │       │   ├── EncDecAttention (T5Attention)
    │   │       │   │   └── q,k,v,o(Linear) weight:[768, 768]
    │   │       │   └── layer_norm (T5LayerNorm) weight:[768]
    │   │       └── 2 (T5LayerFF)
    │   │           ├── DenseReluDense (T5DenseActDense)
    │   │           │   ├── wi (Linear) weight:[3072, 768]
    │   │           │   └── wo (Linear) weight:[768, 3072]
    │   │           └── layer_norm (T5LayerNorm) weight:[768]
    │   └── 1-11(T5Block)
    │       └── layer (ModuleList)
    │           ├── 0 (T5LayerSelfAttention)
    │           │   ├── SelfAttention (T5Attention)
    │           │   │   └── q,k,v,o(Linear) weight:[768, 768]
    │           │   └── layer_norm (T5LayerNorm) weight:[768]
    │           ├── 1 (T5LayerCrossAttention)
    │           │   ├── EncDecAttention (T5Attention)
    │           │   │   └── q,k,v,o(Linear) weight:[768, 768]
    │           │   └── layer_norm (T5LayerNorm) weight:[768]
    │           └── 2 (T5LayerFF)
    │               ├── DenseReluDense (T5DenseActDense)
    │               │   ├── wi (Linear) weight:[3072, 768]
    │               │   └── wo (Linear) weight:[768, 3072]
    │               └── layer_norm (T5LayerNorm) weight:[768]
    └── final_layer_norm (T5LayerNorm) weight:[768]
[INFO|(OpenDelta)basemodel:700]2025-01-22 02:01:28,378 >> Trainable Ratio: 19176960/242059014=7.922432%
[INFO|(OpenDelta)basemodel:702]2025-01-22 02:01:28,378 >> Delta Parameter Ratio: 19176966/242059014=7.922434%
[INFO|(OpenDelta)basemodel:704]2025-01-22 02:01:28,378 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 10
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 78700
INFO:name:epoch 0 step 100 loss 3.70276
INFO:name:epoch 0 step 200 loss 3.37336
INFO:name:epoch 0 step 300 loss 1.61813
INFO:name:epoch 0 step 400 loss 0.74234
INFO:name:epoch 0 step 500 loss 0.55849
INFO:name:epoch 0 step 600 loss 0.4839
INFO:name:epoch 0 step 700 loss 0.43266
INFO:name:epoch 0 step 800 loss 0.4528
INFO:name:epoch 0 step 900 loss 0.37297
INFO:name:epoch 0 step 1000 loss 0.38522
INFO:name:epoch 0 step 1100 loss 0.36237
INFO:name:epoch 0 step 1200 loss 0.38332
INFO:name:epoch 0 step 1300 loss 0.32991
INFO:name:epoch 0 step 1400 loss 0.35342
INFO:name:epoch 0 step 1500 loss 0.33238
INFO:name:epoch 0 step 1600 loss 0.33658
INFO:name:epoch 0 step 1700 loss 0.3155
INFO:name:epoch 0 step 1800 loss 0.30404
INFO:name:epoch 0 step 1900 loss 0.30944
INFO:name:epoch 0 step 2000 loss 0.28658
INFO:name:epoch 0 step 2100 loss 0.2983
INFO:name:epoch 0 step 2200 loss 0.27224
INFO:name:epoch 0 step 2300 loss 0.28215
INFO:name:epoch 0 step 2400 loss 0.28745
INFO:name:epoch 0 step 2500 loss 0.30055
INFO:name:epoch 0 step 2600 loss 0.28424
INFO:name:epoch 0 step 2700 loss 0.28526
INFO:name:epoch 0 step 2800 loss 0.27759
INFO:name:epoch 0 step 2900 loss 0.27722
INFO:name:epoch 0 step 3000 loss 0.24774
INFO:name:epoch 0 step 3100 loss 0.27215
INFO:name:epoch 0 step 3200 loss 0.26937
INFO:name:epoch 0 step 3300 loss 0.28151
INFO:name:epoch 0 step 3400 loss 0.26454
INFO:name:epoch 0 step 3500 loss 0.28004
INFO:name:epoch 0 step 3600 loss 0.2655
INFO:name:epoch 0 step 3700 loss 0.25675
INFO:name:epoch 0 step 3800 loss 0.24905
INFO:name:epoch 0 step 3900 loss 0.24348
INFO:name:epoch 0 step 4000 loss 0.25843
INFO:name:epoch 0 step 4100 loss 0.26072
INFO:name:epoch 0 step 4200 loss 0.23246
INFO:name:epoch 0 step 4300 loss 0.24188
INFO:name:epoch 0 step 4400 loss 0.24704
INFO:name:epoch 0 step 4500 loss 0.24447
INFO:name:epoch 0 step 4600 loss 0.24007
INFO:name:epoch 0 step 4700 loss 0.23909
INFO:name:epoch 0 step 4800 loss 0.24007
INFO:name:epoch 0 step 4900 loss 0.23776
INFO:name:epoch 0 step 5000 loss 0.24947
INFO:name:epoch 0 step 5100 loss 0.2256
INFO:name:epoch 0 step 5200 loss 0.24177
INFO:name:epoch 0 step 5300 loss 0.22163
INFO:name:epoch 0 step 5400 loss 0.2365
INFO:name:epoch 0 step 5500 loss 0.22652
INFO:name:epoch 0 step 5600 loss 0.2266
INFO:name:epoch 0 step 5700 loss 0.23292
INFO:name:epoch 0 step 5800 loss 0.22663
INFO:name:epoch 0 step 5900 loss 0.22507
INFO:name:epoch 0 step 6000 loss 0.24093
INFO:name:epoch 0 step 6100 loss 0.22243
INFO:name:epoch 0 step 6200 loss 0.21861
INFO:name:epoch 0 step 6300 loss 0.20615
INFO:name:epoch 0 step 6400 loss 0.21208
INFO:name:epoch 0 step 6500 loss 0.22395
INFO:name:epoch 0 step 6600 loss 0.2072
INFO:name:epoch 0 step 6700 loss 0.23091
INFO:name:epoch 0 step 6800 loss 0.24005
INFO:name:epoch 0 step 6900 loss 0.21149
INFO:name:epoch 0 step 7000 loss 0.20833
INFO:name:epoch 0 step 7100 loss 0.20888
INFO:name:epoch 0 step 7200 loss 0.213
INFO:name:epoch 0 step 7300 loss 0.21782
INFO:name:epoch 0 step 7400 loss 0.22341
INFO:name:epoch 0 step 7500 loss 0.21515
INFO:name:epoch 0 step 7600 loss 0.21336
INFO:name:epoch 0 step 7700 loss 0.19582
INFO:name:epoch 0 step 7800 loss 0.21248
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1143
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1143
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.082
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 0.20655
INFO:name:epoch 1 step 200 loss 0.17927
INFO:name:epoch 1 step 300 loss 0.17054
INFO:name:epoch 1 step 400 loss 0.18364
INFO:name:epoch 1 step 500 loss 0.19994
INFO:name:epoch 1 step 600 loss 0.20478
INFO:name:epoch 1 step 700 loss 0.18872
INFO:name:epoch 1 step 800 loss 0.15494
INFO:name:epoch 1 step 900 loss 0.17044
INFO:name:epoch 1 step 1000 loss 0.17516
INFO:name:epoch 1 step 1100 loss 0.16879
INFO:name:epoch 1 step 1200 loss 0.17058
INFO:name:epoch 1 step 1300 loss 0.17354
INFO:name:epoch 1 step 1400 loss 0.17521
INFO:name:epoch 1 step 1500 loss 0.15592
INFO:name:epoch 1 step 1600 loss 0.16839
INFO:name:epoch 1 step 1700 loss 0.19162
INFO:name:epoch 1 step 1800 loss 0.17867
INFO:name:epoch 1 step 1900 loss 0.15488
INFO:name:epoch 1 step 2000 loss 0.15903
INFO:name:epoch 1 step 2100 loss 0.1648
INFO:name:epoch 1 step 2200 loss 0.15256
INFO:name:epoch 1 step 2300 loss 0.16127
INFO:name:epoch 1 step 2400 loss 0.14756
INFO:name:epoch 1 step 2500 loss 0.14927
INFO:name:epoch 1 step 2600 loss 0.16711
INFO:name:epoch 1 step 2700 loss 0.15575
INFO:name:epoch 1 step 2800 loss 0.15938
INFO:name:epoch 1 step 2900 loss 0.15646
INFO:name:epoch 1 step 3000 loss 0.14638
INFO:name:epoch 1 step 3100 loss 0.14556
INFO:name:epoch 1 step 3200 loss 0.16099
INFO:name:epoch 1 step 3300 loss 0.14273
INFO:name:epoch 1 step 3400 loss 0.17746
INFO:name:epoch 1 step 3500 loss 0.14933
INFO:name:epoch 1 step 3600 loss 0.17576
INFO:name:epoch 1 step 3700 loss 0.15405
INFO:name:epoch 1 step 3800 loss 0.15701
INFO:name:epoch 1 step 3900 loss 0.15529
INFO:name:epoch 1 step 4000 loss 0.15684
INFO:name:epoch 1 step 4100 loss 0.15092
INFO:name:epoch 1 step 4200 loss 0.16457
INFO:name:epoch 1 step 4300 loss 0.1597
INFO:name:epoch 1 step 4400 loss 0.13621
INFO:name:epoch 1 step 4500 loss 0.16477
INFO:name:epoch 1 step 4600 loss 0.15126
INFO:name:epoch 1 step 4700 loss 0.15521
INFO:name:epoch 1 step 4800 loss 0.14199
INFO:name:epoch 1 step 4900 loss 0.16118
INFO:name:epoch 1 step 5000 loss 0.15179
INFO:name:epoch 1 step 5100 loss 0.16307
INFO:name:epoch 1 step 5200 loss 0.16229
INFO:name:epoch 1 step 5300 loss 0.16203
INFO:name:epoch 1 step 5400 loss 0.16226
INFO:name:epoch 1 step 5500 loss 0.16281
INFO:name:epoch 1 step 5600 loss 0.16766
INFO:name:epoch 1 step 5700 loss 0.16368
INFO:name:epoch 1 step 5800 loss 0.13621
INFO:name:epoch 1 step 5900 loss 0.13346
INFO:name:epoch 1 step 6000 loss 0.14545
INFO:name:epoch 1 step 6100 loss 0.14095
INFO:name:epoch 1 step 6200 loss 0.1505
INFO:name:epoch 1 step 6300 loss 0.14681
INFO:name:epoch 1 step 6400 loss 0.16028
INFO:name:epoch 1 step 6500 loss 0.14622
INFO:name:epoch 1 step 6600 loss 0.12913
INFO:name:epoch 1 step 6700 loss 0.15184
INFO:name:epoch 1 step 6800 loss 0.15273
INFO:name:epoch 1 step 6900 loss 0.15732
INFO:name:epoch 1 step 7000 loss 0.1464
INFO:name:epoch 1 step 7100 loss 0.15012
INFO:name:epoch 1 step 7200 loss 0.15768
INFO:name:epoch 1 step 7300 loss 0.13257
INFO:name:epoch 1 step 7400 loss 0.13346
INFO:name:epoch 1 step 7500 loss 0.14608
INFO:name:epoch 1 step 7600 loss 0.12613
INFO:name:epoch 1 step 7700 loss 0.14237
INFO:name:epoch 1 step 7800 loss 0.15117
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2051
INFO:name:  ********************
INFO:name:  Best eval mrr:0.2051
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1614
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 2 step 100 loss 0.1418
INFO:name:epoch 2 step 200 loss 0.14849
INFO:name:epoch 2 step 300 loss 0.12938
INFO:name:epoch 2 step 400 loss 0.12721
INFO:name:epoch 2 step 500 loss 0.12064
INFO:name:epoch 2 step 600 loss 0.12813
INFO:name:epoch 2 step 700 loss 0.1492
INFO:name:epoch 2 step 800 loss 0.14094
INFO:name:epoch 2 step 900 loss 0.13284
INFO:name:epoch 2 step 1000 loss 0.13249
INFO:name:epoch 2 step 1100 loss 0.14393
INFO:name:epoch 2 step 1200 loss 0.12409
INFO:name:epoch 2 step 1300 loss 0.13143
INFO:name:epoch 2 step 1400 loss 0.13484
INFO:name:epoch 2 step 1500 loss 0.14225
INFO:name:epoch 2 step 1600 loss 0.12167
INFO:name:epoch 2 step 1700 loss 0.12327
INFO:name:epoch 2 step 1800 loss 0.1392
INFO:name:epoch 2 step 1900 loss 0.11919
INFO:name:epoch 2 step 2000 loss 0.12923
INFO:name:epoch 2 step 2100 loss 0.12575
INFO:name:epoch 2 step 2200 loss 0.11609
INFO:name:epoch 2 step 2300 loss 0.13604
INFO:name:epoch 2 step 2400 loss 0.12978
INFO:name:epoch 2 step 2500 loss 0.14691
INFO:name:epoch 2 step 2600 loss 0.14284
INFO:name:epoch 2 step 2700 loss 0.12795
INFO:name:epoch 2 step 2800 loss 0.12213
INFO:name:epoch 2 step 2900 loss 0.11656
INFO:name:epoch 2 step 3000 loss 0.1443
INFO:name:epoch 2 step 3100 loss 0.1154
INFO:name:epoch 2 step 3200 loss 0.12178
INFO:name:epoch 2 step 3300 loss 0.16291
INFO:name:epoch 2 step 3400 loss 0.11827
INFO:name:epoch 2 step 3500 loss 0.13408
INFO:name:epoch 2 step 3600 loss 0.1365
INFO:name:epoch 2 step 3700 loss 0.12615
INFO:name:epoch 2 step 3800 loss 0.14043
INFO:name:epoch 2 step 3900 loss 0.14033
INFO:name:epoch 2 step 4000 loss 0.14095
INFO:name:epoch 2 step 4100 loss 0.11284
INFO:name:epoch 2 step 4200 loss 0.12225
INFO:name:epoch 2 step 4300 loss 0.12623
INFO:name:epoch 2 step 4400 loss 0.13895
INFO:name:epoch 2 step 4500 loss 0.14009
INFO:name:epoch 2 step 4600 loss 0.12223
INFO:name:epoch 2 step 4700 loss 0.12952
INFO:name:epoch 2 step 4800 loss 0.13327
INFO:name:epoch 2 step 4900 loss 0.15735
INFO:name:epoch 2 step 5000 loss 0.13502
INFO:name:epoch 2 step 5100 loss 0.13147
INFO:name:epoch 2 step 5200 loss 0.1328
INFO:name:epoch 2 step 5300 loss 0.12199
INFO:name:epoch 2 step 5400 loss 0.12525
INFO:name:epoch 2 step 5500 loss 0.1436
INFO:name:epoch 2 step 5600 loss 0.11684
INFO:name:epoch 2 step 5700 loss 0.12118
INFO:name:epoch 2 step 5800 loss 0.10254
INFO:name:epoch 2 step 5900 loss 0.14633
INFO:name:epoch 2 step 6000 loss 0.13438
INFO:name:epoch 2 step 6100 loss 0.12737
INFO:name:epoch 2 step 6200 loss 0.11749
INFO:name:epoch 2 step 6300 loss 0.1378
INFO:name:epoch 2 step 6400 loss 0.12415
INFO:name:epoch 2 step 6500 loss 0.1376
INFO:name:epoch 2 step 6600 loss 0.11882
INFO:name:epoch 2 step 6700 loss 0.12626
INFO:name:epoch 2 step 6800 loss 0.11663
INFO:name:epoch 2 step 6900 loss 0.12903
INFO:name:epoch 2 step 7000 loss 0.12185
INFO:name:epoch 2 step 7100 loss 0.12841
INFO:name:epoch 2 step 7200 loss 0.12572
INFO:name:epoch 2 step 7300 loss 0.12961
INFO:name:epoch 2 step 7400 loss 0.13351
INFO:name:epoch 2 step 7500 loss 0.10852
INFO:name:epoch 2 step 7600 loss 0.12247
INFO:name:epoch 2 step 7700 loss 0.12529
INFO:name:epoch 2 step 7800 loss 0.13621
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2336
INFO:name:  ********************
INFO:name:  Best eval mrr:0.2336
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1843
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.11465
INFO:name:epoch 3 step 200 loss 0.11606
INFO:name:epoch 3 step 300 loss 0.11952
INFO:name:epoch 3 step 400 loss 0.11412
INFO:name:epoch 3 step 500 loss 0.12025
INFO:name:epoch 3 step 600 loss 0.09669
INFO:name:epoch 3 step 700 loss 0.10625
INFO:name:epoch 3 step 800 loss 0.10512
INFO:name:epoch 3 step 900 loss 0.10262
INFO:name:epoch 3 step 1000 loss 0.11806
INFO:name:epoch 3 step 1100 loss 0.12178
INFO:name:epoch 3 step 1200 loss 0.12476
INFO:name:epoch 3 step 1300 loss 0.12775
INFO:name:epoch 3 step 1400 loss 0.11899
INFO:name:epoch 3 step 1500 loss 0.12655
INFO:name:epoch 3 step 1600 loss 0.12352
INFO:name:epoch 3 step 1700 loss 0.13035
INFO:name:epoch 3 step 1800 loss 0.10165
INFO:name:epoch 3 step 1900 loss 0.11523
INFO:name:epoch 3 step 2000 loss 0.11115
INFO:name:epoch 3 step 2100 loss 0.11735
INFO:name:epoch 3 step 2200 loss 0.11878
INFO:name:epoch 3 step 2300 loss 0.12821
INFO:name:epoch 3 step 2400 loss 0.1157
INFO:name:epoch 3 step 2500 loss 0.12356
INFO:name:epoch 3 step 2600 loss 0.11806
INFO:name:epoch 3 step 2700 loss 0.1228
INFO:name:epoch 3 step 2800 loss 0.11069
INFO:name:epoch 3 step 2900 loss 0.11352
INFO:name:epoch 3 step 3000 loss 0.11713
INFO:name:epoch 3 step 3100 loss 0.11041
INFO:name:epoch 3 step 3200 loss 0.12238
INFO:name:epoch 3 step 3300 loss 0.12586
INFO:name:epoch 3 step 3400 loss 0.13097
INFO:name:epoch 3 step 3500 loss 0.10431
INFO:name:epoch 3 step 3600 loss 0.12504
INFO:name:epoch 3 step 3700 loss 0.12519
INFO:name:epoch 3 step 3800 loss 0.11103
INFO:name:epoch 3 step 3900 loss 0.11488
INFO:name:epoch 3 step 4000 loss 0.1176
INFO:name:epoch 3 step 4100 loss 0.11523
INFO:name:epoch 3 step 4200 loss 0.12636
INFO:name:epoch 3 step 4300 loss 0.12631
INFO:name:epoch 3 step 4400 loss 0.11362
INFO:name:epoch 3 step 4500 loss 0.10761
INFO:name:epoch 3 step 4600 loss 0.1051
INFO:name:epoch 3 step 4700 loss 0.10768
INFO:name:epoch 3 step 4800 loss 0.11563
INFO:name:epoch 3 step 4900 loss 0.11691
INFO:name:epoch 3 step 5000 loss 0.11557
INFO:name:epoch 3 step 5100 loss 0.10442
INFO:name:epoch 3 step 5200 loss 0.12117
INFO:name:epoch 3 step 5300 loss 0.104
INFO:name:epoch 3 step 5400 loss 0.1157
INFO:name:epoch 3 step 5500 loss 0.11725
INFO:name:epoch 3 step 5600 loss 0.12483
INFO:name:epoch 3 step 5700 loss 0.11965
INFO:name:epoch 3 step 5800 loss 0.11728
INFO:name:epoch 3 step 5900 loss 0.1213
INFO:name:epoch 3 step 6000 loss 0.11281
INFO:name:epoch 3 step 6100 loss 0.1154
INFO:name:epoch 3 step 6200 loss 0.1153
INFO:name:epoch 3 step 6300 loss 0.12253
INFO:name:epoch 3 step 6400 loss 0.10555
INFO:name:epoch 3 step 6500 loss 0.12372
INFO:name:epoch 3 step 6600 loss 0.11263
INFO:name:epoch 3 step 6700 loss 0.11746
INFO:name:epoch 3 step 6800 loss 0.122
INFO:name:epoch 3 step 6900 loss 0.11348
INFO:name:epoch 3 step 7000 loss 0.10113
INFO:name:epoch 3 step 7100 loss 0.11762
INFO:name:epoch 3 step 7200 loss 0.10684
INFO:name:epoch 3 step 7300 loss 0.1113
INFO:name:epoch 3 step 7400 loss 0.1309
INFO:name:epoch 3 step 7500 loss 0.1091
INFO:name:epoch 3 step 7600 loss 0.099
INFO:name:epoch 3 step 7700 loss 0.11634
INFO:name:epoch 3 step 7800 loss 0.10301
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2251
INFO:name:epoch 4 step 100 loss 0.11472
INFO:name:epoch 4 step 200 loss 0.08881
INFO:name:epoch 4 step 300 loss 0.10385
INFO:name:epoch 4 step 400 loss 0.09831
INFO:name:epoch 4 step 500 loss 0.10011
INFO:name:epoch 4 step 600 loss 0.11379
INFO:name:epoch 4 step 700 loss 0.10751
INFO:name:epoch 4 step 800 loss 0.09165
INFO:name:epoch 4 step 900 loss 0.10512
INFO:name:epoch 4 step 1000 loss 0.10276
INFO:name:epoch 4 step 1100 loss 0.11528
INFO:name:epoch 4 step 1200 loss 0.09711
INFO:name:epoch 4 step 1300 loss 0.1208
INFO:name:epoch 4 step 1400 loss 0.10465
INFO:name:epoch 4 step 1500 loss 0.10301
INFO:name:epoch 4 step 1600 loss 0.09674
INFO:name:epoch 4 step 1700 loss 0.1047
INFO:name:epoch 4 step 1800 loss 0.10663
INFO:name:epoch 4 step 1900 loss 0.09278
INFO:name:epoch 4 step 2000 loss 0.10494
INFO:name:epoch 4 step 2100 loss 0.09727
INFO:name:epoch 4 step 2200 loss 0.09533
INFO:name:epoch 4 step 2300 loss 0.12054
INFO:name:epoch 4 step 2400 loss 0.10595
INFO:name:epoch 4 step 2500 loss 0.11493
INFO:name:epoch 4 step 2600 loss 0.10553
INFO:name:epoch 4 step 2700 loss 0.09756
INFO:name:epoch 4 step 2800 loss 0.12565
INFO:name:epoch 4 step 2900 loss 0.1132
INFO:name:epoch 4 step 3000 loss 0.10159
INFO:name:epoch 4 step 3100 loss 0.0967
INFO:name:epoch 4 step 3200 loss 0.10715
INFO:name:epoch 4 step 3300 loss 0.11082
INFO:name:epoch 4 step 3400 loss 0.1127
INFO:name:epoch 4 step 3500 loss 0.10567
INFO:name:epoch 4 step 3600 loss 0.10222
INFO:name:epoch 4 step 3700 loss 0.11875
INFO:name:epoch 4 step 3800 loss 0.09858
INFO:name:epoch 4 step 3900 loss 0.09944
INFO:name:epoch 4 step 4000 loss 0.09275
INFO:name:epoch 4 step 4100 loss 0.1008
INFO:name:epoch 4 step 4200 loss 0.0923
INFO:name:epoch 4 step 4300 loss 0.10049
INFO:name:epoch 4 step 4400 loss 0.11312
INFO:name:epoch 4 step 4500 loss 0.1019
INFO:name:epoch 4 step 4600 loss 0.1063
INFO:name:epoch 4 step 4700 loss 0.09268
INFO:name:epoch 4 step 4800 loss 0.11331
INFO:name:epoch 4 step 4900 loss 0.10073
INFO:name:epoch 4 step 5000 loss 0.11357
INFO:name:epoch 4 step 5100 loss 0.10597
INFO:name:epoch 4 step 5200 loss 0.09818
INFO:name:epoch 4 step 5300 loss 0.10469
INFO:name:epoch 4 step 5400 loss 0.10059
INFO:name:epoch 4 step 5500 loss 0.10649
INFO:name:epoch 4 step 5600 loss 0.09119
INFO:name:epoch 4 step 5700 loss 0.1035
INFO:name:epoch 4 step 5800 loss 0.09757
INFO:name:epoch 4 step 5900 loss 0.10703
INFO:name:epoch 4 step 6000 loss 0.11282
INFO:name:epoch 4 step 6100 loss 0.10741
INFO:name:epoch 4 step 6200 loss 0.11037
INFO:name:epoch 4 step 6300 loss 0.10843
INFO:name:epoch 4 step 6400 loss 0.10733
INFO:name:epoch 4 step 6500 loss 0.10573
INFO:name:epoch 4 step 6600 loss 0.12594
INFO:name:epoch 4 step 6700 loss 0.09989
INFO:name:epoch 4 step 6800 loss 0.11743
INFO:name:epoch 4 step 6900 loss 0.0927
INFO:name:epoch 4 step 7000 loss 0.10873
INFO:name:epoch 4 step 7100 loss 0.10289
INFO:name:epoch 4 step 7200 loss 0.11834
INFO:name:epoch 4 step 7300 loss 0.10191
INFO:name:epoch 4 step 7400 loss 0.1008
INFO:name:epoch 4 step 7500 loss 0.0883
INFO:name:epoch 4 step 7600 loss 0.1218
INFO:name:epoch 4 step 7700 loss 0.10989
INFO:name:epoch 4 step 7800 loss 0.09389
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2496
INFO:name:  ********************
INFO:name:  Best eval mrr:0.2496
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1994
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 5 step 100 loss 0.09064
INFO:name:epoch 5 step 200 loss 0.08613
INFO:name:epoch 5 step 300 loss 0.08398
INFO:name:epoch 5 step 400 loss 0.09754
INFO:name:epoch 5 step 500 loss 0.1093
INFO:name:epoch 5 step 600 loss 0.10666
INFO:name:epoch 5 step 700 loss 0.09125
INFO:name:epoch 5 step 800 loss 0.08538
INFO:name:epoch 5 step 900 loss 0.10148
INFO:name:epoch 5 step 1000 loss 0.10447
INFO:name:epoch 5 step 1100 loss 0.09642
INFO:name:epoch 5 step 1200 loss 0.09248
INFO:name:epoch 5 step 1300 loss 0.0989
INFO:name:epoch 5 step 1400 loss 0.09821
INFO:name:epoch 5 step 1500 loss 0.08932
INFO:name:epoch 5 step 1600 loss 0.0809
INFO:name:epoch 5 step 1700 loss 0.10566
INFO:name:epoch 5 step 1800 loss 0.09997
INFO:name:epoch 5 step 1900 loss 0.09937
INFO:name:epoch 5 step 2000 loss 0.10837
INFO:name:epoch 5 step 2100 loss 0.10176
INFO:name:epoch 5 step 2200 loss 0.10216
INFO:name:epoch 5 step 2300 loss 0.09065
INFO:name:epoch 5 step 2400 loss 0.09333
INFO:name:epoch 5 step 2500 loss 0.08872
INFO:name:epoch 5 step 2600 loss 0.09165
INFO:name:epoch 5 step 2700 loss 0.09829
INFO:name:epoch 5 step 2800 loss 0.08307
INFO:name:epoch 5 step 2900 loss 0.09256
INFO:name:epoch 5 step 3000 loss 0.07947
INFO:name:epoch 5 step 3100 loss 0.09842
INFO:name:epoch 5 step 3200 loss 0.09657
INFO:name:epoch 5 step 3300 loss 0.09321
INFO:name:epoch 5 step 3400 loss 0.09471
INFO:name:epoch 5 step 3500 loss 0.07147
INFO:name:epoch 5 step 3600 loss 0.09659
INFO:name:epoch 5 step 3700 loss 0.08389
INFO:name:epoch 5 step 3800 loss 0.09464
INFO:name:epoch 5 step 3900 loss 0.09552
INFO:name:epoch 5 step 4000 loss 0.09941
INFO:name:epoch 5 step 4100 loss 0.10451
INFO:name:epoch 5 step 4200 loss 0.10482
INFO:name:epoch 5 step 4300 loss 0.09582
INFO:name:epoch 5 step 4400 loss 0.09472
INFO:name:epoch 5 step 4500 loss 0.09085
INFO:name:epoch 5 step 4600 loss 0.09513
INFO:name:epoch 5 step 4700 loss 0.08873
INFO:name:epoch 5 step 4800 loss 0.09305
INFO:name:epoch 5 step 4900 loss 0.1028
INFO:name:epoch 5 step 5000 loss 0.09814
INFO:name:epoch 5 step 5100 loss 0.09681
INFO:name:epoch 5 step 5200 loss 0.10069
INFO:name:epoch 5 step 5300 loss 0.0961
INFO:name:epoch 5 step 5400 loss 0.09151
INFO:name:epoch 5 step 5500 loss 0.0987
INFO:name:epoch 5 step 5600 loss 0.08585
INFO:name:epoch 5 step 5700 loss 0.09745
INFO:name:epoch 5 step 5800 loss 0.09843
INFO:name:epoch 5 step 5900 loss 0.0862
INFO:name:epoch 5 step 6000 loss 0.10194
INFO:name:epoch 5 step 6100 loss 0.10456
INFO:name:epoch 5 step 6200 loss 0.10544
INFO:name:epoch 5 step 6300 loss 0.0988
INFO:name:epoch 5 step 6400 loss 0.09503
INFO:name:epoch 5 step 6500 loss 0.08796
INFO:name:epoch 5 step 6600 loss 0.09216
INFO:name:epoch 5 step 6700 loss 0.08912
INFO:name:epoch 5 step 6800 loss 0.08906
INFO:name:epoch 5 step 6900 loss 0.08989
INFO:name:epoch 5 step 7000 loss 0.10328
INFO:name:epoch 5 step 7100 loss 0.09855
INFO:name:epoch 5 step 7200 loss 0.10787
INFO:name:epoch 5 step 7300 loss 0.0874
INFO:name:epoch 5 step 7400 loss 0.08257
INFO:name:epoch 5 step 7500 loss 0.08303
INFO:name:epoch 5 step 7600 loss 0.09478
INFO:name:epoch 5 step 7700 loss 0.11372
INFO:name:epoch 5 step 7800 loss 0.09549
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2285
INFO:name:epoch 6 step 100 loss 0.08682
INFO:name:epoch 6 step 200 loss 0.08854
INFO:name:epoch 6 step 300 loss 0.09767
INFO:name:epoch 6 step 400 loss 0.08295
INFO:name:epoch 6 step 500 loss 0.08733
INFO:name:epoch 6 step 600 loss 0.09088
INFO:name:epoch 6 step 700 loss 0.08124
INFO:name:epoch 6 step 800 loss 0.07841
INFO:name:epoch 6 step 900 loss 0.08317
INFO:name:epoch 6 step 1000 loss 0.08259
INFO:name:epoch 6 step 1100 loss 0.08934
INFO:name:epoch 6 step 1200 loss 0.10449
INFO:name:epoch 6 step 1300 loss 0.08982
INFO:name:epoch 6 step 1400 loss 0.09306
INFO:name:epoch 6 step 1500 loss 0.08405
INFO:name:epoch 6 step 1600 loss 0.08737
INFO:name:epoch 6 step 1700 loss 0.095
INFO:name:epoch 6 step 1800 loss 0.08737
INFO:name:epoch 6 step 1900 loss 0.07991
INFO:name:epoch 6 step 2000 loss 0.08242
INFO:name:epoch 6 step 2100 loss 0.08835
INFO:name:epoch 6 step 2200 loss 0.09216
INFO:name:epoch 6 step 2300 loss 0.08806
INFO:name:epoch 6 step 2400 loss 0.0824
INFO:name:epoch 6 step 2500 loss 0.09258
INFO:name:epoch 6 step 2600 loss 0.088
INFO:name:epoch 6 step 2700 loss 0.08921
INFO:name:epoch 6 step 2800 loss 0.0864
INFO:name:epoch 6 step 2900 loss 0.07848
INFO:name:epoch 6 step 3000 loss 0.08879
INFO:name:epoch 6 step 3100 loss 0.09282
INFO:name:epoch 6 step 3200 loss 0.08655
INFO:name:epoch 6 step 3300 loss 0.08358
INFO:name:epoch 6 step 3400 loss 0.08901
INFO:name:epoch 6 step 3500 loss 0.08572
INFO:name:epoch 6 step 3600 loss 0.08874
INFO:name:epoch 6 step 3700 loss 0.09528
INFO:name:epoch 6 step 3800 loss 0.08037
INFO:name:epoch 6 step 3900 loss 0.08943
INFO:name:epoch 6 step 4000 loss 0.11164
INFO:name:epoch 6 step 4100 loss 0.08956
INFO:name:epoch 6 step 4200 loss 0.08134
INFO:name:epoch 6 step 4300 loss 0.08137
INFO:name:epoch 6 step 4400 loss 0.09098
INFO:name:epoch 6 step 4500 loss 0.09091
INFO:name:epoch 6 step 4600 loss 0.09276
INFO:name:epoch 6 step 4700 loss 0.08212
INFO:name:epoch 6 step 4800 loss 0.09538
INFO:name:epoch 6 step 4900 loss 0.07956
INFO:name:epoch 6 step 5000 loss 0.08701
INFO:name:epoch 6 step 5100 loss 0.08374
INFO:name:epoch 6 step 5200 loss 0.07738
INFO:name:epoch 6 step 5300 loss 0.08679
INFO:name:epoch 6 step 5400 loss 0.08585
INFO:name:epoch 6 step 5500 loss 0.09191
INFO:name:epoch 6 step 5600 loss 0.08759
INFO:name:epoch 6 step 5700 loss 0.0873
INFO:name:epoch 6 step 5800 loss 0.07465
INFO:name:epoch 6 step 5900 loss 0.08511
INFO:name:epoch 6 step 6000 loss 0.08565
INFO:name:epoch 6 step 6100 loss 0.09434
INFO:name:epoch 6 step 6200 loss 0.0902
INFO:name:epoch 6 step 6300 loss 0.07549
INFO:name:epoch 6 step 6400 loss 0.08607
INFO:name:epoch 6 step 6500 loss 0.08995
INFO:name:epoch 6 step 6600 loss 0.08443
INFO:name:epoch 6 step 6700 loss 0.0785
INFO:name:epoch 6 step 6800 loss 0.08199
INFO:name:epoch 6 step 6900 loss 0.08643
INFO:name:epoch 6 step 7000 loss 0.08098
INFO:name:epoch 6 step 7100 loss 0.09017
INFO:name:epoch 6 step 7200 loss 0.07642
INFO:name:epoch 6 step 7300 loss 0.08941
INFO:name:epoch 6 step 7400 loss 0.08553
INFO:name:epoch 6 step 7500 loss 0.09011
INFO:name:epoch 6 step 7600 loss 0.09338
INFO:name:epoch 6 step 7700 loss 0.08641
INFO:name:epoch 6 step 7800 loss 0.08738
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2559
INFO:name:  ********************
INFO:name:  Best eval mrr:0.2559
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2057
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 7 step 100 loss 0.08488
INFO:name:epoch 7 step 200 loss 0.08524
INFO:name:epoch 7 step 300 loss 0.07327
INFO:name:epoch 7 step 400 loss 0.0783
INFO:name:epoch 7 step 500 loss 0.07937
INFO:name:epoch 7 step 600 loss 0.07428
INFO:name:epoch 7 step 700 loss 0.08295
INFO:name:epoch 7 step 800 loss 0.07179
INFO:name:epoch 7 step 900 loss 0.07514
INFO:name:epoch 7 step 1000 loss 0.08775
INFO:name:epoch 7 step 1100 loss 0.08199
INFO:name:epoch 7 step 1200 loss 0.07947
INFO:name:epoch 7 step 1300 loss 0.07859
INFO:name:epoch 7 step 1400 loss 0.07161
INFO:name:epoch 7 step 1500 loss 0.08925
INFO:name:epoch 7 step 1600 loss 0.08615
INFO:name:epoch 7 step 1700 loss 0.08323
INFO:name:epoch 7 step 1800 loss 0.07957
INFO:name:epoch 7 step 1900 loss 0.09653
INFO:name:epoch 7 step 2000 loss 0.07625
INFO:name:epoch 7 step 2100 loss 0.08121
INFO:name:epoch 7 step 2200 loss 0.07971
INFO:name:epoch 7 step 2300 loss 0.08384
INFO:name:epoch 7 step 2400 loss 0.08197
INFO:name:epoch 7 step 2500 loss 0.06428
INFO:name:epoch 7 step 2600 loss 0.0795
INFO:name:epoch 7 step 2700 loss 0.07852
INFO:name:epoch 7 step 2800 loss 0.07932
INFO:name:epoch 7 step 2900 loss 0.07776
INFO:name:epoch 7 step 3000 loss 0.08649
INFO:name:epoch 7 step 3100 loss 0.08169
INFO:name:epoch 7 step 3200 loss 0.07239
INFO:name:epoch 7 step 3300 loss 0.07798
INFO:name:epoch 7 step 3400 loss 0.07241
INFO:name:epoch 7 step 3500 loss 0.08427
INFO:name:epoch 7 step 3600 loss 0.07311
INFO:name:epoch 7 step 3700 loss 0.08996
INFO:name:epoch 7 step 3800 loss 0.07028
INFO:name:epoch 7 step 3900 loss 0.07579
INFO:name:epoch 7 step 4000 loss 0.08094
INFO:name:epoch 7 step 4100 loss 0.08999
INFO:name:epoch 7 step 4200 loss 0.06862
INFO:name:epoch 7 step 4300 loss 0.07915
INFO:name:epoch 7 step 4400 loss 0.08804
INFO:name:epoch 7 step 4500 loss 0.08139
INFO:name:epoch 7 step 4600 loss 0.07684
INFO:name:epoch 7 step 4700 loss 0.07059
INFO:name:epoch 7 step 4800 loss 0.07853
INFO:name:epoch 7 step 4900 loss 0.08885
INFO:name:epoch 7 step 5000 loss 0.08784
INFO:name:epoch 7 step 5100 loss 0.08811
INFO:name:epoch 7 step 5200 loss 0.08529
INFO:name:epoch 7 step 5300 loss 0.08353
INFO:name:epoch 7 step 5400 loss 0.08345
INFO:name:epoch 7 step 5500 loss 0.09591
INFO:name:epoch 7 step 5600 loss 0.08329
INFO:name:epoch 7 step 5700 loss 0.08764
INFO:name:epoch 7 step 5800 loss 0.08478
INFO:name:epoch 7 step 5900 loss 0.0933
INFO:name:epoch 7 step 6000 loss 0.07816
INFO:name:epoch 7 step 6100 loss 0.07727
INFO:name:epoch 7 step 6200 loss 0.05869
INFO:name:epoch 7 step 6300 loss 0.0874
INFO:name:epoch 7 step 6400 loss 0.07337
INFO:name:epoch 7 step 6500 loss 0.07432
INFO:name:epoch 7 step 6600 loss 0.07819
INFO:name:epoch 7 step 6700 loss 0.07477
INFO:name:epoch 7 step 6800 loss 0.06913
INFO:name:epoch 7 step 6900 loss 0.07504
INFO:name:epoch 7 step 7000 loss 0.07534
INFO:name:epoch 7 step 7100 loss 0.08033
INFO:name:epoch 7 step 7200 loss 0.08109
INFO:name:epoch 7 step 7300 loss 0.06596
INFO:name:epoch 7 step 7400 loss 0.08636
INFO:name:epoch 7 step 7500 loss 0.08688
INFO:name:epoch 7 step 7600 loss 0.08222
INFO:name:epoch 7 step 7700 loss 0.07105
INFO:name:epoch 7 step 7800 loss 0.07296
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2631
INFO:name:  ********************
INFO:name:  Best eval mrr:0.2631
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2115
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 8 step 100 loss 0.08116
INFO:name:epoch 8 step 200 loss 0.07514
INFO:name:epoch 8 step 300 loss 0.07524
INFO:name:epoch 8 step 400 loss 0.0685
INFO:name:epoch 8 step 500 loss 0.0716
INFO:name:epoch 8 step 600 loss 0.07313
INFO:name:epoch 8 step 700 loss 0.06043
INFO:name:epoch 8 step 800 loss 0.07761
INFO:name:epoch 8 step 900 loss 0.07635
INFO:name:epoch 8 step 1000 loss 0.06856
INFO:name:epoch 8 step 1100 loss 0.07133
INFO:name:epoch 8 step 1200 loss 0.07438
INFO:name:epoch 8 step 1300 loss 0.08667
INFO:name:epoch 8 step 1400 loss 0.07032
INFO:name:epoch 8 step 1500 loss 0.07216
INFO:name:epoch 8 step 1600 loss 0.07519
INFO:name:epoch 8 step 1700 loss 0.07672
INFO:name:epoch 8 step 1800 loss 0.06467
INFO:name:epoch 8 step 1900 loss 0.08032
INFO:name:epoch 8 step 2000 loss 0.07509
INFO:name:epoch 8 step 2100 loss 0.06601
INFO:name:epoch 8 step 2200 loss 0.06628
INFO:name:epoch 8 step 2300 loss 0.08592
INFO:name:epoch 8 step 2400 loss 0.07856
INFO:name:epoch 8 step 2500 loss 0.08215
INFO:name:epoch 8 step 2600 loss 0.07744
INFO:name:epoch 8 step 2700 loss 0.06712
INFO:name:epoch 8 step 2800 loss 0.07759
INFO:name:epoch 8 step 2900 loss 0.07589
INFO:name:epoch 8 step 3000 loss 0.07641
INFO:name:epoch 8 step 3100 loss 0.07358
INFO:name:epoch 8 step 3200 loss 0.07295
INFO:name:epoch 8 step 3300 loss 0.07164
INFO:name:epoch 8 step 3400 loss 0.0738
INFO:name:epoch 8 step 3500 loss 0.08129
INFO:name:epoch 8 step 3600 loss 0.08136
INFO:name:epoch 8 step 3700 loss 0.07485
INFO:name:epoch 8 step 3800 loss 0.07507
INFO:name:epoch 8 step 3900 loss 0.08123
INFO:name:epoch 8 step 4000 loss 0.07522
INFO:name:epoch 8 step 4100 loss 0.0703
INFO:name:epoch 8 step 4200 loss 0.07674
INFO:name:epoch 8 step 4300 loss 0.07642
INFO:name:epoch 8 step 4400 loss 0.06205
INFO:name:epoch 8 step 4500 loss 0.07756
INFO:name:epoch 8 step 4600 loss 0.07285
INFO:name:epoch 8 step 4700 loss 0.07146
INFO:name:epoch 8 step 4800 loss 0.07432
INFO:name:epoch 8 step 4900 loss 0.06886
INFO:name:epoch 8 step 5000 loss 0.07442
INFO:name:epoch 8 step 5100 loss 0.0677
INFO:name:epoch 8 step 5200 loss 0.07824
INFO:name:epoch 8 step 5300 loss 0.0718
INFO:name:epoch 8 step 5400 loss 0.07534
INFO:name:epoch 8 step 5500 loss 0.07165
INFO:name:epoch 8 step 5600 loss 0.07447
INFO:name:epoch 8 step 5700 loss 0.06646
INFO:name:epoch 8 step 5800 loss 0.07673
INFO:name:epoch 8 step 5900 loss 0.08159
INFO:name:epoch 8 step 6000 loss 0.08143
INFO:name:epoch 8 step 6100 loss 0.06955
INFO:name:epoch 8 step 6200 loss 0.07143
INFO:name:epoch 8 step 6300 loss 0.06234
INFO:name:epoch 8 step 6400 loss 0.0773
INFO:name:epoch 8 step 6500 loss 0.08168
INFO:name:epoch 8 step 6600 loss 0.07829
INFO:name:epoch 8 step 6700 loss 0.07206
INFO:name:epoch 8 step 6800 loss 0.06145
INFO:name:epoch 8 step 6900 loss 0.07901
INFO:name:epoch 8 step 7000 loss 0.063
INFO:name:epoch 8 step 7100 loss 0.06356
INFO:name:epoch 8 step 7200 loss 0.06402
INFO:name:epoch 8 step 7300 loss 0.07526
INFO:name:epoch 8 step 7400 loss 0.08293
INFO:name:epoch 8 step 7500 loss 0.06716
INFO:name:epoch 8 step 7600 loss 0.07604
INFO:name:epoch 8 step 7700 loss 0.07591
INFO:name:epoch 8 step 7800 loss 0.06287
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2585
INFO:name:epoch 9 step 100 loss 0.06387
INFO:name:epoch 9 step 200 loss 0.06969
INFO:name:epoch 9 step 300 loss 0.07624
INFO:name:epoch 9 step 400 loss 0.05889
INFO:name:epoch 9 step 500 loss 0.06949
INFO:name:epoch 9 step 600 loss 0.06328
INFO:name:epoch 9 step 700 loss 0.07043
INFO:name:epoch 9 step 800 loss 0.06456
INFO:name:epoch 9 step 900 loss 0.08346
INFO:name:epoch 9 step 1000 loss 0.0761
INFO:name:epoch 9 step 1100 loss 0.06909
INFO:name:epoch 9 step 1200 loss 0.06712
INFO:name:epoch 9 step 1300 loss 0.06354
INFO:name:epoch 9 step 1400 loss 0.07303
INFO:name:epoch 9 step 1500 loss 0.07089
INFO:name:epoch 9 step 1600 loss 0.07104
INFO:name:epoch 9 step 1700 loss 0.07126
INFO:name:epoch 9 step 1800 loss 0.06651
INFO:name:epoch 9 step 1900 loss 0.06566
INFO:name:epoch 9 step 2000 loss 0.06171
INFO:name:epoch 9 step 2100 loss 0.05788
INFO:name:epoch 9 step 2200 loss 0.07044
INFO:name:epoch 9 step 2300 loss 0.06532
INFO:name:epoch 9 step 2400 loss 0.08407
INFO:name:epoch 9 step 2500 loss 0.06584
INFO:name:epoch 9 step 2600 loss 0.06567
INFO:name:epoch 9 step 2700 loss 0.07054
INFO:name:epoch 9 step 2800 loss 0.06279
INFO:name:epoch 9 step 2900 loss 0.07032
INFO:name:epoch 9 step 3000 loss 0.06608
INFO:name:epoch 9 step 3100 loss 0.06962
INFO:name:epoch 9 step 3200 loss 0.06716
INFO:name:epoch 9 step 3300 loss 0.06071
INFO:name:epoch 9 step 3400 loss 0.06581
INFO:name:epoch 9 step 3500 loss 0.07046
INFO:name:epoch 9 step 3600 loss 0.06426
INFO:name:epoch 9 step 3700 loss 0.07156
INFO:name:epoch 9 step 3800 loss 0.07292
INFO:name:epoch 9 step 3900 loss 0.07545
INFO:name:epoch 9 step 4000 loss 0.06822
INFO:name:epoch 9 step 4100 loss 0.07648
INFO:name:epoch 9 step 4200 loss 0.07381
INFO:name:epoch 9 step 4300 loss 0.06506
INFO:name:epoch 9 step 4400 loss 0.06541
INFO:name:epoch 9 step 4500 loss 0.05909
INFO:name:epoch 9 step 4600 loss 0.05416
INFO:name:epoch 9 step 4700 loss 0.06758
INFO:name:epoch 9 step 4800 loss 0.06655
INFO:name:epoch 9 step 4900 loss 0.05958
INFO:name:epoch 9 step 5000 loss 0.06359
INFO:name:epoch 9 step 5100 loss 0.05713
INFO:name:epoch 9 step 5200 loss 0.06637
INFO:name:epoch 9 step 5300 loss 0.07254
INFO:name:epoch 9 step 5400 loss 0.0671
INFO:name:epoch 9 step 5500 loss 0.06522
INFO:name:epoch 9 step 5600 loss 0.06957
INFO:name:epoch 9 step 5700 loss 0.06562
INFO:name:epoch 9 step 5800 loss 0.06339
INFO:name:epoch 9 step 5900 loss 0.07312
INFO:name:epoch 9 step 6000 loss 0.06853
INFO:name:epoch 9 step 6100 loss 0.07616
INFO:name:epoch 9 step 6200 loss 0.06539
INFO:name:epoch 9 step 6300 loss 0.06115
INFO:name:epoch 9 step 6400 loss 0.0576
INFO:name:epoch 9 step 6500 loss 0.06217
INFO:name:epoch 9 step 6600 loss 0.06703
INFO:name:epoch 9 step 6700 loss 0.06131
INFO:name:epoch 9 step 6800 loss 0.06834
INFO:name:epoch 9 step 6900 loss 0.0741
INFO:name:epoch 9 step 7000 loss 0.05099
INFO:name:epoch 9 step 7100 loss 0.05907
INFO:name:epoch 9 step 7200 loss 0.0589
INFO:name:epoch 9 step 7300 loss 0.06193
INFO:name:epoch 9 step 7400 loss 0.06229
INFO:name:epoch 9 step 7500 loss 0.06265
INFO:name:epoch 9 step 7600 loss 0.06347
INFO:name:epoch 9 step 7700 loss 0.06058
INFO:name:epoch 9 step 7800 loss 0.06633
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2646
INFO:name:  ********************
INFO:name:  Best eval mrr:0.2646
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2125
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
train results ([0.37687486291743505, 0.1584992599842497, 0.1302053694361809, 0.1160031249123629, 0.1047001639296735, 0.0949191706315135, 0.08710683763513562, 0.07997927151357381, 0.07338691314361483, 0.06673490469254614], [0.1143283770553315, 0.20514395524834925, 0.23355240575459363, 0.22509738247503766, 0.24959839188983984, 0.228512044519114, 0.255855114727436, 0.2631246894126102, 0.2584957958918312, 0.26462407477002076])
