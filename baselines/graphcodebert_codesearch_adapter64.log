/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
INFO:name:device: cuda:3, n_gpu: 1
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/main/config.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

root
├── embeddings (RobertaEmbeddings)
│   ├── word_embeddings (Embedding) weight:[50265, 768]
│   ├── position_embeddings (Embedding) weight:[514, 768]
│   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
├── encoder (RobertaEncoder)
│   └── layer (ModuleList)
│       └── 0-11(RobertaLayer)
│           ├── attention (RobertaAttention)
│           │   ├── self (RobertaSelfAttention)
│           │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│           │   └── output (RobertaSelfOutput)
│           │       ├── dense (Linear) weight:[768, 768] bias:[768]
│           │       │   └── adapter (AdapterLayer)
│           │       │       └── modulelist (Sequential)
│           │       │           ├── down_proj (Linear) weight:[64, 768] bias:[64]
│           │       │           └── up_proj (Linear) weight:[768, 64] bias:[768]
│           │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│           ├── intermediate (RobertaIntermediate)
│           │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│           └── output (RobertaOutput)
│               ├── dense (Linear) weight:[768, 3072] bias:[768]
│               │   └── adapter (AdapterLayer)
│               │       └── modulelist (Sequential)
│               │           ├── down_proj (Linear) weight:[64, 768] bias:[64]
│               │           └── up_proj (Linear) weight:[768, 64] bias:[768]
│               └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── pooler (RobertaPooler)
    └── dense (Linear) weight:[768, 768] bias:[768]
[INFO|(OpenDelta)basemodel:700]2025-01-15 15:13:25,763 >> Trainable Ratio: 2379264/127024896=1.873069%
[INFO|(OpenDelta)basemodel:702]2025-01-15 15:13:25,763 >> Delta Parameter Ratio: 2379264/127024896=1.873069%
[INFO|(OpenDelta)basemodel:704]2025-01-15 15:13:25,763 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 15
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 118050
INFO:name:epoch 0 step 100 loss 3.53426
INFO:name:epoch 0 step 200 loss 3.46703
INFO:name:epoch 0 step 300 loss 3.46701
INFO:name:epoch 0 step 400 loss 3.46653
INFO:name:epoch 0 step 500 loss 3.46634
INFO:name:epoch 0 step 600 loss 3.46552
INFO:name:epoch 0 step 700 loss 3.31331
INFO:name:epoch 0 step 800 loss 3.10273
INFO:name:epoch 0 step 900 loss 3.04551
INFO:name:epoch 0 step 1000 loss 2.99535
INFO:name:epoch 0 step 1100 loss 2.92517
INFO:name:epoch 0 step 1200 loss 2.95114
INFO:name:epoch 0 step 1300 loss 2.95814
INFO:name:epoch 0 step 1400 loss 2.84969
INFO:name:epoch 0 step 1500 loss 2.78234
INFO:name:epoch 0 step 1600 loss 2.69238
INFO:name:epoch 0 step 1700 loss 2.64475
INFO:name:epoch 0 step 1800 loss 2.5498
INFO:name:epoch 0 step 1900 loss 2.52396
INFO:name:epoch 0 step 2000 loss 2.50587
INFO:name:epoch 0 step 2100 loss 2.41176
INFO:name:epoch 0 step 2200 loss 2.30909
INFO:name:epoch 0 step 2300 loss 2.27121
INFO:name:epoch 0 step 2400 loss 2.23504
INFO:name:epoch 0 step 2500 loss 2.15791
INFO:name:epoch 0 step 2600 loss 2.18648
INFO:name:epoch 0 step 2700 loss 2.17471
INFO:name:epoch 0 step 2800 loss 2.08289
INFO:name:epoch 0 step 2900 loss 2.04176
INFO:name:epoch 0 step 3000 loss 1.94646
INFO:name:epoch 0 step 3100 loss 1.95293
INFO:name:epoch 0 step 3200 loss 1.84215
INFO:name:epoch 0 step 3300 loss 1.79292
INFO:name:epoch 0 step 3400 loss 1.80976
INFO:name:epoch 0 step 3500 loss 1.67928
INFO:name:epoch 0 step 3600 loss 1.66643
INFO:name:epoch 0 step 3700 loss 1.59704
INFO:name:epoch 0 step 3800 loss 1.57697
INFO:name:epoch 0 step 3900 loss 1.47768
INFO:name:epoch 0 step 4000 loss 1.44962
INFO:name:epoch 0 step 4100 loss 1.41107
INFO:name:epoch 0 step 4200 loss 1.38116
INFO:name:epoch 0 step 4300 loss 1.30273
INFO:name:epoch 0 step 4400 loss 1.29977
INFO:name:epoch 0 step 4500 loss 1.22741
INFO:name:epoch 0 step 4600 loss 1.25292
INFO:name:epoch 0 step 4700 loss 1.22381
INFO:name:epoch 0 step 4800 loss 1.17185
INFO:name:epoch 0 step 4900 loss 1.11827
INFO:name:epoch 0 step 5000 loss 1.15274
INFO:name:epoch 0 step 5100 loss 1.05014
INFO:name:epoch 0 step 5200 loss 1.07704
INFO:name:epoch 0 step 5300 loss 1.04819
INFO:name:epoch 0 step 5400 loss 1.05711
INFO:name:epoch 0 step 5500 loss 1.02597
INFO:name:epoch 0 step 5600 loss 1.02613
INFO:name:epoch 0 step 5700 loss 0.97159
INFO:name:epoch 0 step 5800 loss 0.94003
INFO:name:epoch 0 step 5900 loss 0.99561
INFO:name:epoch 0 step 6000 loss 0.96041
INFO:name:epoch 0 step 6100 loss 0.90492
INFO:name:epoch 0 step 6200 loss 0.94192
INFO:name:epoch 0 step 6300 loss 0.88521
INFO:name:epoch 0 step 6400 loss 0.91133
INFO:name:epoch 0 step 6500 loss 0.88254
INFO:name:epoch 0 step 6600 loss 0.91624
INFO:name:epoch 0 step 6700 loss 0.88296
INFO:name:epoch 0 step 6800 loss 0.80492
INFO:name:epoch 0 step 6900 loss 0.87941
INFO:name:epoch 0 step 7000 loss 0.82913
INFO:name:epoch 0 step 7100 loss 0.84929
INFO:name:epoch 0 step 7200 loss 0.82314
INFO:name:epoch 0 step 7300 loss 0.79144
INFO:name:epoch 0 step 7400 loss 0.84399
INFO:name:epoch 0 step 7500 loss 0.74943
INFO:name:epoch 0 step 7600 loss 0.75917
INFO:name:epoch 0 step 7700 loss 0.74944
INFO:name:epoch 0 step 7800 loss 0.71634
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0497
INFO:name:  ********************
INFO:name:  Best eval mrr:0.0497
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0338
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 0.64863
INFO:name:epoch 1 step 200 loss 0.58951
INFO:name:epoch 1 step 300 loss 0.58184
INFO:name:epoch 1 step 400 loss 0.57885
INFO:name:epoch 1 step 500 loss 0.56971
INFO:name:epoch 1 step 600 loss 0.61064
INFO:name:epoch 1 step 700 loss 0.55401
INFO:name:epoch 1 step 800 loss 0.52587
INFO:name:epoch 1 step 900 loss 0.51343
INFO:name:epoch 1 step 1000 loss 0.54207
INFO:name:epoch 1 step 1100 loss 0.49737
INFO:name:epoch 1 step 1200 loss 0.54937
INFO:name:epoch 1 step 1300 loss 0.51684
INFO:name:epoch 1 step 1400 loss 0.5077
INFO:name:epoch 1 step 1500 loss 0.47245
INFO:name:epoch 1 step 1600 loss 0.49827
INFO:name:epoch 1 step 1700 loss 0.47615
INFO:name:epoch 1 step 1800 loss 0.44641
INFO:name:epoch 1 step 1900 loss 0.45273
INFO:name:epoch 1 step 2000 loss 0.47829
INFO:name:epoch 1 step 2100 loss 0.45893
INFO:name:epoch 1 step 2200 loss 0.48452
INFO:name:epoch 1 step 2300 loss 0.45516
INFO:name:epoch 1 step 2400 loss 0.4265
INFO:name:epoch 1 step 2500 loss 0.45754
INFO:name:epoch 1 step 2600 loss 0.43206
INFO:name:epoch 1 step 2700 loss 0.44233
INFO:name:epoch 1 step 2800 loss 0.43219
INFO:name:epoch 1 step 2900 loss 0.40914
INFO:name:epoch 1 step 3000 loss 0.42515
INFO:name:epoch 1 step 3100 loss 0.43916
INFO:name:epoch 1 step 3200 loss 0.43302
INFO:name:epoch 1 step 3300 loss 0.40955
INFO:name:epoch 1 step 3400 loss 0.42685
INFO:name:epoch 1 step 3500 loss 0.42789
INFO:name:epoch 1 step 3600 loss 0.35916
INFO:name:epoch 1 step 3700 loss 0.40953
INFO:name:epoch 1 step 3800 loss 0.38319
INFO:name:epoch 1 step 3900 loss 0.40348
INFO:name:epoch 1 step 4000 loss 0.3956
INFO:name:epoch 1 step 4100 loss 0.39569
INFO:name:epoch 1 step 4200 loss 0.39677
INFO:name:epoch 1 step 4300 loss 0.36956
INFO:name:epoch 1 step 4400 loss 0.36347
INFO:name:epoch 1 step 4500 loss 0.34163
INFO:name:epoch 1 step 4600 loss 0.3837
INFO:name:epoch 1 step 4700 loss 0.37061
INFO:name:epoch 1 step 4800 loss 0.39355
INFO:name:epoch 1 step 4900 loss 0.36754
INFO:name:epoch 1 step 5000 loss 0.35838
INFO:name:epoch 1 step 5100 loss 0.39336
INFO:name:epoch 1 step 5200 loss 0.36574
INFO:name:epoch 1 step 5300 loss 0.33037
INFO:name:epoch 1 step 5400 loss 0.38154
INFO:name:epoch 1 step 5500 loss 0.34938
INFO:name:epoch 1 step 5600 loss 0.3591
INFO:name:epoch 1 step 5700 loss 0.39177
INFO:name:epoch 1 step 5800 loss 0.37582
INFO:name:epoch 1 step 5900 loss 0.37566
INFO:name:epoch 1 step 6000 loss 0.33745
INFO:name:epoch 1 step 6100 loss 0.38995
INFO:name:epoch 1 step 6200 loss 0.34029
INFO:name:epoch 1 step 6300 loss 0.36106
INFO:name:epoch 1 step 6400 loss 0.39267
INFO:name:epoch 1 step 6500 loss 0.3214
INFO:name:epoch 1 step 6600 loss 0.34721
INFO:name:epoch 1 step 6700 loss 0.34426
INFO:name:epoch 1 step 6800 loss 0.33908
INFO:name:epoch 1 step 6900 loss 0.34953
INFO:name:epoch 1 step 7000 loss 0.32508
INFO:name:epoch 1 step 7100 loss 0.31554
INFO:name:epoch 1 step 7200 loss 0.32093
INFO:name:epoch 1 step 7300 loss 0.32345
INFO:name:epoch 1 step 7400 loss 0.31065
INFO:name:epoch 1 step 7500 loss 0.28988
INFO:name:epoch 1 step 7600 loss 0.2997
INFO:name:epoch 1 step 7700 loss 0.33563
INFO:name:epoch 1 step 7800 loss 0.34139
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0781
INFO:name:  ********************
INFO:name:  Best eval mrr:0.0781
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0516
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 2 step 100 loss 0.30008
INFO:name:epoch 2 step 200 loss 0.29846
INFO:name:epoch 2 step 300 loss 0.28197
INFO:name:epoch 2 step 400 loss 0.29509
INFO:name:epoch 2 step 500 loss 0.29059
INFO:name:epoch 2 step 600 loss 0.25303
INFO:name:epoch 2 step 700 loss 0.27637
INFO:name:epoch 2 step 800 loss 0.29225
INFO:name:epoch 2 step 900 loss 0.30819
INFO:name:epoch 2 step 1000 loss 0.26488
INFO:name:epoch 2 step 1100 loss 0.2584
INFO:name:epoch 2 step 1200 loss 0.29167
INFO:name:epoch 2 step 1300 loss 0.28419
INFO:name:epoch 2 step 1400 loss 0.29977
INFO:name:epoch 2 step 1500 loss 0.25929
INFO:name:epoch 2 step 1600 loss 0.29583
INFO:name:epoch 2 step 1700 loss 0.29092
INFO:name:epoch 2 step 1800 loss 0.2875
INFO:name:epoch 2 step 1900 loss 0.26882
INFO:name:epoch 2 step 2000 loss 0.28997
INFO:name:epoch 2 step 2100 loss 0.27811
INFO:name:epoch 2 step 2200 loss 0.27176
INFO:name:epoch 2 step 2300 loss 0.28884
INFO:name:epoch 2 step 2400 loss 0.26479
INFO:name:epoch 2 step 2500 loss 0.25413
INFO:name:epoch 2 step 2600 loss 0.27786
INFO:name:epoch 2 step 2700 loss 0.26576
INFO:name:epoch 2 step 2800 loss 0.2624
INFO:name:epoch 2 step 2900 loss 0.24053
INFO:name:epoch 2 step 3000 loss 0.27234
INFO:name:epoch 2 step 3100 loss 0.28669
INFO:name:epoch 2 step 3200 loss 0.26751
INFO:name:epoch 2 step 3300 loss 0.26076
INFO:name:epoch 2 step 3400 loss 0.26188
INFO:name:epoch 2 step 3500 loss 0.27253
INFO:name:epoch 2 step 3600 loss 0.26598
INFO:name:epoch 2 step 3700 loss 0.25661
INFO:name:epoch 2 step 3800 loss 0.25226
INFO:name:epoch 2 step 3900 loss 0.25652
INFO:name:epoch 2 step 4000 loss 0.25909
INFO:name:epoch 2 step 4100 loss 0.24862
INFO:name:epoch 2 step 4200 loss 0.251
INFO:name:epoch 2 step 4300 loss 0.24449
INFO:name:epoch 2 step 4400 loss 0.25609
INFO:name:epoch 2 step 4500 loss 0.22032
INFO:name:epoch 2 step 4600 loss 0.24678
INFO:name:epoch 2 step 4700 loss 0.23731
INFO:name:epoch 2 step 4800 loss 0.26865
INFO:name:epoch 2 step 4900 loss 0.25168
INFO:name:epoch 2 step 5000 loss 0.24836
INFO:name:epoch 2 step 5100 loss 0.27082
INFO:name:epoch 2 step 5200 loss 0.23055
INFO:name:epoch 2 step 5300 loss 0.25908
INFO:name:epoch 2 step 5400 loss 0.25911
INFO:name:epoch 2 step 5500 loss 0.22224
INFO:name:epoch 2 step 5600 loss 0.24046
INFO:name:epoch 2 step 5700 loss 0.23856
INFO:name:epoch 2 step 5800 loss 0.26286
INFO:name:epoch 2 step 5900 loss 0.21282
INFO:name:epoch 2 step 6000 loss 0.22461
INFO:name:epoch 2 step 6100 loss 0.24273
INFO:name:epoch 2 step 6200 loss 0.23221
INFO:name:epoch 2 step 6300 loss 0.25136
INFO:name:epoch 2 step 6400 loss 0.23449
INFO:name:epoch 2 step 6500 loss 0.2413
INFO:name:epoch 2 step 6600 loss 0.23073
INFO:name:epoch 2 step 6700 loss 0.22517
INFO:name:epoch 2 step 6800 loss 0.22474
INFO:name:epoch 2 step 6900 loss 0.21975
INFO:name:epoch 2 step 7000 loss 0.20485
INFO:name:epoch 2 step 7100 loss 0.24449
INFO:name:epoch 2 step 7200 loss 0.26893
INFO:name:epoch 2 step 7300 loss 0.21655
INFO:name:epoch 2 step 7400 loss 0.23814
INFO:name:epoch 2 step 7500 loss 0.24137
INFO:name:epoch 2 step 7600 loss 0.23593
INFO:name:epoch 2 step 7700 loss 0.21388
INFO:name:epoch 2 step 7800 loss 0.21494
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1189
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1189
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0884
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.19503
INFO:name:epoch 3 step 200 loss 0.19927
INFO:name:epoch 3 step 300 loss 0.20037
INFO:name:epoch 3 step 400 loss 0.20293
INFO:name:epoch 3 step 500 loss 0.20826
INFO:name:epoch 3 step 600 loss 0.2069
INFO:name:epoch 3 step 700 loss 0.19143
INFO:name:epoch 3 step 800 loss 0.21151
INFO:name:epoch 3 step 900 loss 0.20973
INFO:name:epoch 3 step 1000 loss 0.18985
INFO:name:epoch 3 step 1100 loss 0.19773
INFO:name:epoch 3 step 1200 loss 0.19592
INFO:name:epoch 3 step 1300 loss 0.19677
INFO:name:epoch 3 step 1400 loss 0.18102
INFO:name:epoch 3 step 1500 loss 0.19272
INFO:name:epoch 3 step 1600 loss 0.212
INFO:name:epoch 3 step 1700 loss 0.18289
INFO:name:epoch 3 step 1800 loss 0.17998
INFO:name:epoch 3 step 1900 loss 0.20736
INFO:name:epoch 3 step 2000 loss 0.1827
INFO:name:epoch 3 step 2100 loss 0.1875
INFO:name:epoch 3 step 2200 loss 0.21664
INFO:name:epoch 3 step 2300 loss 0.21194
INFO:name:epoch 3 step 2400 loss 0.19625
INFO:name:epoch 3 step 2500 loss 0.18041
INFO:name:epoch 3 step 2600 loss 0.19827
INFO:name:epoch 3 step 2700 loss 0.18052
INFO:name:epoch 3 step 2800 loss 0.21418
INFO:name:epoch 3 step 2900 loss 0.19755
INFO:name:epoch 3 step 3000 loss 0.18773
INFO:name:epoch 3 step 3100 loss 0.22016
INFO:name:epoch 3 step 3200 loss 0.19823
INFO:name:epoch 3 step 3300 loss 0.19502
INFO:name:epoch 3 step 3400 loss 0.19722
INFO:name:epoch 3 step 3500 loss 0.2116
INFO:name:epoch 3 step 3600 loss 0.18953
INFO:name:epoch 3 step 3700 loss 0.22278
INFO:name:epoch 3 step 3800 loss 0.18293
INFO:name:epoch 3 step 3900 loss 0.18982
INFO:name:epoch 3 step 4000 loss 0.18932
INFO:name:epoch 3 step 4100 loss 0.18161
INFO:name:epoch 3 step 4200 loss 0.18738
INFO:name:epoch 3 step 4300 loss 0.18508
INFO:name:epoch 3 step 4400 loss 0.19191
INFO:name:epoch 3 step 4500 loss 0.17594
INFO:name:epoch 3 step 4600 loss 0.17266
INFO:name:epoch 3 step 4700 loss 0.19357
INFO:name:epoch 3 step 4800 loss 0.19214
INFO:name:epoch 3 step 4900 loss 0.1746
INFO:name:epoch 3 step 5000 loss 0.1645
INFO:name:epoch 3 step 5100 loss 0.18293
INFO:name:epoch 3 step 5200 loss 0.1746
INFO:name:epoch 3 step 5300 loss 0.17459
INFO:name:epoch 3 step 5400 loss 0.18316
INFO:name:epoch 3 step 5500 loss 0.194
INFO:name:epoch 3 step 5600 loss 0.18596
INFO:name:epoch 3 step 5700 loss 0.15696
INFO:name:epoch 3 step 5800 loss 0.19032
INFO:name:epoch 3 step 5900 loss 0.2018
INFO:name:epoch 3 step 6000 loss 0.17934
INFO:name:epoch 3 step 6100 loss 0.18316
INFO:name:epoch 3 step 6200 loss 0.18764
INFO:name:epoch 3 step 6300 loss 0.19216
INFO:name:epoch 3 step 6400 loss 0.16326
INFO:name:epoch 3 step 6500 loss 0.19442
INFO:name:epoch 3 step 6600 loss 0.18285
INFO:name:epoch 3 step 6700 loss 0.20069
INFO:name:epoch 3 step 6800 loss 0.18229
INFO:name:epoch 3 step 6900 loss 0.17593
INFO:name:epoch 3 step 7000 loss 0.18286
INFO:name:epoch 3 step 7100 loss 0.1802
INFO:name:epoch 3 step 7200 loss 0.16824
INFO:name:epoch 3 step 7300 loss 0.17222
INFO:name:epoch 3 step 7400 loss 0.16791
INFO:name:epoch 3 step 7500 loss 0.18042
INFO:name:epoch 3 step 7600 loss 0.1871
INFO:name:epoch 3 step 7700 loss 0.18446
INFO:name:epoch 3 step 7800 loss 0.16901
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1311
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1311
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.098
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 4 step 100 loss 0.1548
INFO:name:epoch 4 step 200 loss 0.16272
INFO:name:epoch 4 step 300 loss 0.15161
INFO:name:epoch 4 step 400 loss 0.13267
INFO:name:epoch 4 step 500 loss 0.13702
INFO:name:epoch 4 step 600 loss 0.15342
INFO:name:epoch 4 step 700 loss 0.14616
INFO:name:epoch 4 step 800 loss 0.15941
INFO:name:epoch 4 step 900 loss 0.14252
INFO:name:epoch 4 step 1000 loss 0.14889
INFO:name:epoch 4 step 1100 loss 0.14175
INFO:name:epoch 4 step 1200 loss 0.14348
INFO:name:epoch 4 step 1300 loss 0.15146
INFO:name:epoch 4 step 1400 loss 0.1819
INFO:name:epoch 4 step 1500 loss 0.13638
INFO:name:epoch 4 step 1600 loss 0.17888
INFO:name:epoch 4 step 1700 loss 0.13705
INFO:name:epoch 4 step 1800 loss 0.15815
INFO:name:epoch 4 step 1900 loss 0.14433
INFO:name:epoch 4 step 2000 loss 0.15916
INFO:name:epoch 4 step 2100 loss 0.15049
INFO:name:epoch 4 step 2200 loss 0.1411
INFO:name:epoch 4 step 2300 loss 0.15266
INFO:name:epoch 4 step 2400 loss 0.14669
INFO:name:epoch 4 step 2500 loss 0.16834
INFO:name:epoch 4 step 2600 loss 0.15526
INFO:name:epoch 4 step 2700 loss 0.15872
INFO:name:epoch 4 step 2800 loss 0.15781
INFO:name:epoch 4 step 2900 loss 0.14004
INFO:name:epoch 4 step 3000 loss 0.15388
INFO:name:epoch 4 step 3100 loss 0.15068
INFO:name:epoch 4 step 3200 loss 0.13898
INFO:name:epoch 4 step 3300 loss 0.14194
INFO:name:epoch 4 step 3400 loss 0.13285
INFO:name:epoch 4 step 3500 loss 0.14002
INFO:name:epoch 4 step 3600 loss 0.17804
INFO:name:epoch 4 step 3700 loss 0.14884
INFO:name:epoch 4 step 3800 loss 0.15197
INFO:name:epoch 4 step 3900 loss 0.14916
INFO:name:epoch 4 step 4000 loss 0.14469
INFO:name:epoch 4 step 4100 loss 0.1444
INFO:name:epoch 4 step 4200 loss 0.14822
INFO:name:epoch 4 step 4300 loss 0.14481
INFO:name:epoch 4 step 4400 loss 0.14797
INFO:name:epoch 4 step 4500 loss 0.16013
INFO:name:epoch 4 step 4600 loss 0.12012
INFO:name:epoch 4 step 4700 loss 0.12414
INFO:name:epoch 4 step 4800 loss 0.15595
INFO:name:epoch 4 step 4900 loss 0.15658
INFO:name:epoch 4 step 5000 loss 0.15898
INFO:name:epoch 4 step 5100 loss 0.14766
INFO:name:epoch 4 step 5200 loss 0.13954
INFO:name:epoch 4 step 5300 loss 0.14041
INFO:name:epoch 4 step 5400 loss 0.15966
INFO:name:epoch 4 step 5500 loss 0.14934
INFO:name:epoch 4 step 5600 loss 0.13071
INFO:name:epoch 4 step 5700 loss 0.13461
INFO:name:epoch 4 step 5800 loss 0.14799
INFO:name:epoch 4 step 5900 loss 0.13842
INFO:name:epoch 4 step 6000 loss 0.14444
INFO:name:epoch 4 step 6100 loss 0.15717
INFO:name:epoch 4 step 6200 loss 0.14635
INFO:name:epoch 4 step 6300 loss 0.1585
INFO:name:epoch 4 step 6400 loss 0.14928
INFO:name:epoch 4 step 6500 loss 0.14891
INFO:name:epoch 4 step 6600 loss 0.14749
INFO:name:epoch 4 step 6700 loss 0.14145
INFO:name:epoch 4 step 6800 loss 0.14288
INFO:name:epoch 4 step 6900 loss 0.15187
INFO:name:epoch 4 step 7000 loss 0.15313
INFO:name:epoch 4 step 7100 loss 0.14354
INFO:name:epoch 4 step 7200 loss 0.13273
INFO:name:epoch 4 step 7300 loss 0.13027
INFO:name:epoch 4 step 7400 loss 0.1559
INFO:name:epoch 4 step 7500 loss 0.13332
INFO:name:epoch 4 step 7600 loss 0.14024
INFO:name:epoch 4 step 7700 loss 0.12685
INFO:name:epoch 4 step 7800 loss 0.15157
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1462
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1462
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1121
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 5 step 100 loss 0.12285
INFO:name:epoch 5 step 200 loss 0.10745
INFO:name:epoch 5 step 300 loss 0.11996
INFO:name:epoch 5 step 400 loss 0.12196
INFO:name:epoch 5 step 500 loss 0.11039
INFO:name:epoch 5 step 600 loss 0.12975
INFO:name:epoch 5 step 700 loss 0.11768
INFO:name:epoch 5 step 800 loss 0.11344
INFO:name:epoch 5 step 900 loss 0.12567
INFO:name:epoch 5 step 1000 loss 0.12634
INFO:name:epoch 5 step 1100 loss 0.12611
INFO:name:epoch 5 step 1200 loss 0.10842
INFO:name:epoch 5 step 1300 loss 0.12595
INFO:name:epoch 5 step 1400 loss 0.13409
INFO:name:epoch 5 step 1500 loss 0.11747
INFO:name:epoch 5 step 1600 loss 0.12227
INFO:name:epoch 5 step 1700 loss 0.11531
INFO:name:epoch 5 step 1800 loss 0.11631
INFO:name:epoch 5 step 1900 loss 0.13844
INFO:name:epoch 5 step 2000 loss 0.09584
INFO:name:epoch 5 step 2100 loss 0.11772
INFO:name:epoch 5 step 2200 loss 0.11934
INFO:name:epoch 5 step 2300 loss 0.1148
INFO:name:epoch 5 step 2400 loss 0.12761
INFO:name:epoch 5 step 2500 loss 0.13117
INFO:name:epoch 5 step 2600 loss 0.12924
INFO:name:epoch 5 step 2700 loss 0.11761
INFO:name:epoch 5 step 2800 loss 0.13977
INFO:name:epoch 5 step 2900 loss 0.12216
INFO:name:epoch 5 step 3000 loss 0.11377
INFO:name:epoch 5 step 3100 loss 0.12142
INFO:name:epoch 5 step 3200 loss 0.10449
INFO:name:epoch 5 step 3300 loss 0.12361
INFO:name:epoch 5 step 3400 loss 0.11247
INFO:name:epoch 5 step 3500 loss 0.13452
INFO:name:epoch 5 step 3600 loss 0.11172
INFO:name:epoch 5 step 3700 loss 0.10036
INFO:name:epoch 5 step 3800 loss 0.10988
INFO:name:epoch 5 step 3900 loss 0.13464
INFO:name:epoch 5 step 4000 loss 0.12771
INFO:name:epoch 5 step 4100 loss 0.12155
INFO:name:epoch 5 step 4200 loss 0.11234
INFO:name:epoch 5 step 4300 loss 0.11125
INFO:name:epoch 5 step 4400 loss 0.11779
INFO:name:epoch 5 step 4500 loss 0.13257
INFO:name:epoch 5 step 4600 loss 0.12944
INFO:name:epoch 5 step 4700 loss 0.10639
INFO:name:epoch 5 step 4800 loss 0.11596
INFO:name:epoch 5 step 4900 loss 0.1187
INFO:name:epoch 5 step 5000 loss 0.12819
INFO:name:epoch 5 step 5100 loss 0.10601
INFO:name:epoch 5 step 5200 loss 0.12068
INFO:name:epoch 5 step 5300 loss 0.11362
INFO:name:epoch 5 step 5400 loss 0.13236
INFO:name:epoch 5 step 5500 loss 0.10882
INFO:name:epoch 5 step 5600 loss 0.12012
INFO:name:epoch 5 step 5700 loss 0.11726
INFO:name:epoch 5 step 5800 loss 0.11494
INFO:name:epoch 5 step 5900 loss 0.11804
INFO:name:epoch 5 step 6000 loss 0.12357
INFO:name:epoch 5 step 6100 loss 0.10917
INFO:name:epoch 5 step 6200 loss 0.12206
INFO:name:epoch 5 step 6300 loss 0.1126
INFO:name:epoch 5 step 6400 loss 0.11343
INFO:name:epoch 5 step 6500 loss 0.12221
INFO:name:epoch 5 step 6600 loss 0.11482
INFO:name:epoch 5 step 6700 loss 0.12741
INFO:name:epoch 5 step 6800 loss 0.10877
INFO:name:epoch 5 step 6900 loss 0.11483
INFO:name:epoch 5 step 7000 loss 0.13007
INFO:name:epoch 5 step 7100 loss 0.10922
INFO:name:epoch 5 step 7200 loss 0.12015
INFO:name:epoch 5 step 7300 loss 0.12746
INFO:name:epoch 5 step 7400 loss 0.12484
INFO:name:epoch 5 step 7500 loss 0.10757
INFO:name:epoch 5 step 7600 loss 0.10803
INFO:name:epoch 5 step 7700 loss 0.11905
INFO:name:epoch 5 step 7800 loss 0.11166
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1577
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1577
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1192
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 6 step 100 loss 0.11098
INFO:name:epoch 6 step 200 loss 0.1036
INFO:name:epoch 6 step 300 loss 0.09124
INFO:name:epoch 6 step 400 loss 0.10548
INFO:name:epoch 6 step 500 loss 0.09349
INFO:name:epoch 6 step 600 loss 0.10256
INFO:name:epoch 6 step 700 loss 0.10201
INFO:name:epoch 6 step 800 loss 0.09521
INFO:name:epoch 6 step 900 loss 0.10198
INFO:name:epoch 6 step 1000 loss 0.098
INFO:name:epoch 6 step 1100 loss 0.10228
INFO:name:epoch 6 step 1200 loss 0.09071
INFO:name:epoch 6 step 1300 loss 0.09629
INFO:name:epoch 6 step 1400 loss 0.09673
INFO:name:epoch 6 step 1500 loss 0.08808
INFO:name:epoch 6 step 1600 loss 0.1119
INFO:name:epoch 6 step 1700 loss 0.09754
INFO:name:epoch 6 step 1800 loss 0.08866
INFO:name:epoch 6 step 1900 loss 0.09172
INFO:name:epoch 6 step 2000 loss 0.1006
INFO:name:epoch 6 step 2100 loss 0.10219
INFO:name:epoch 6 step 2200 loss 0.09846
INFO:name:epoch 6 step 2300 loss 0.09994
INFO:name:epoch 6 step 2400 loss 0.10591
INFO:name:epoch 6 step 2500 loss 0.0957
INFO:name:epoch 6 step 2600 loss 0.0888
INFO:name:epoch 6 step 2700 loss 0.10496
INFO:name:epoch 6 step 2800 loss 0.08576
INFO:name:epoch 6 step 2900 loss 0.09303
INFO:name:epoch 6 step 3000 loss 0.08743
INFO:name:epoch 6 step 3100 loss 0.10398
INFO:name:epoch 6 step 3200 loss 0.09462
INFO:name:epoch 6 step 3300 loss 0.12079
INFO:name:epoch 6 step 3400 loss 0.08709
INFO:name:epoch 6 step 3500 loss 0.08119
INFO:name:epoch 6 step 3600 loss 0.094
INFO:name:epoch 6 step 3700 loss 0.08815
INFO:name:epoch 6 step 3800 loss 0.09003
INFO:name:epoch 6 step 3900 loss 0.09716
INFO:name:epoch 6 step 4000 loss 0.09225
INFO:name:epoch 6 step 4100 loss 0.09476
INFO:name:epoch 6 step 4200 loss 0.08567
INFO:name:epoch 6 step 4300 loss 0.09765
INFO:name:epoch 6 step 4400 loss 0.11104
INFO:name:epoch 6 step 4500 loss 0.09864
INFO:name:epoch 6 step 4600 loss 0.10233
INFO:name:epoch 6 step 4700 loss 0.08178
INFO:name:epoch 6 step 4800 loss 0.10156
INFO:name:epoch 6 step 4900 loss 0.0834
INFO:name:epoch 6 step 5000 loss 0.10231
INFO:name:epoch 6 step 5100 loss 0.09255
INFO:name:epoch 6 step 5200 loss 0.09246
INFO:name:epoch 6 step 5300 loss 0.08963
INFO:name:epoch 6 step 5400 loss 0.0938
INFO:name:epoch 6 step 5500 loss 0.07462
INFO:name:epoch 6 step 5600 loss 0.10997
INFO:name:epoch 6 step 5700 loss 0.10724
INFO:name:epoch 6 step 5800 loss 0.09602
INFO:name:epoch 6 step 5900 loss 0.09224
INFO:name:epoch 6 step 6000 loss 0.10017
INFO:name:epoch 6 step 6100 loss 0.11046
INFO:name:epoch 6 step 6200 loss 0.10056
INFO:name:epoch 6 step 6300 loss 0.09519
INFO:name:epoch 6 step 6400 loss 0.10373
INFO:name:epoch 6 step 6500 loss 0.11087
INFO:name:epoch 6 step 6600 loss 0.10838
INFO:name:epoch 6 step 6700 loss 0.1106
INFO:name:epoch 6 step 6800 loss 0.10902
INFO:name:epoch 6 step 6900 loss 0.09017
INFO:name:epoch 6 step 7000 loss 0.10084
INFO:name:epoch 6 step 7100 loss 0.10439
INFO:name:epoch 6 step 7200 loss 0.09641
INFO:name:epoch 6 step 7300 loss 0.09252
INFO:name:epoch 6 step 7400 loss 0.09296
INFO:name:epoch 6 step 7500 loss 0.10385
INFO:name:epoch 6 step 7600 loss 0.10038
INFO:name:epoch 6 step 7700 loss 0.09927
INFO:name:epoch 6 step 7800 loss 0.07382
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.171
INFO:name:  ********************
INFO:name:  Best eval mrr:0.171
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1314
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 7 step 100 loss 0.07849
INFO:name:epoch 7 step 200 loss 0.07374
INFO:name:epoch 7 step 300 loss 0.07771
INFO:name:epoch 7 step 400 loss 0.0854
INFO:name:epoch 7 step 500 loss 0.08451
INFO:name:epoch 7 step 600 loss 0.08969
INFO:name:epoch 7 step 700 loss 0.07686
INFO:name:epoch 7 step 800 loss 0.07329
INFO:name:epoch 7 step 900 loss 0.07059
INFO:name:epoch 7 step 1000 loss 0.08166
INFO:name:epoch 7 step 1100 loss 0.07729
INFO:name:epoch 7 step 1200 loss 0.07097
INFO:name:epoch 7 step 1300 loss 0.08369
INFO:name:epoch 7 step 1400 loss 0.06945
INFO:name:epoch 7 step 1500 loss 0.08599
INFO:name:epoch 7 step 1600 loss 0.08468
INFO:name:epoch 7 step 1700 loss 0.07747
INFO:name:epoch 7 step 1800 loss 0.07209
INFO:name:epoch 7 step 1900 loss 0.08517
INFO:name:epoch 7 step 2000 loss 0.0777
INFO:name:epoch 7 step 2100 loss 0.08474
INFO:name:epoch 7 step 2200 loss 0.07664
INFO:name:epoch 7 step 2300 loss 0.08248
INFO:name:epoch 7 step 2400 loss 0.07675
INFO:name:epoch 7 step 2500 loss 0.06338
INFO:name:epoch 7 step 2600 loss 0.08781
INFO:name:epoch 7 step 2700 loss 0.08643
INFO:name:epoch 7 step 2800 loss 0.09356
INFO:name:epoch 7 step 2900 loss 0.08168
INFO:name:epoch 7 step 3000 loss 0.07985
INFO:name:epoch 7 step 3100 loss 0.08533
INFO:name:epoch 7 step 3200 loss 0.07861
INFO:name:epoch 7 step 3300 loss 0.07611
INFO:name:epoch 7 step 3400 loss 0.1004
INFO:name:epoch 7 step 3500 loss 0.08579
INFO:name:epoch 7 step 3600 loss 0.07641
INFO:name:epoch 7 step 3700 loss 0.08584
INFO:name:epoch 7 step 3800 loss 0.08653
INFO:name:epoch 7 step 3900 loss 0.07882
INFO:name:epoch 7 step 4000 loss 0.08587
INFO:name:epoch 7 step 4100 loss 0.07353
INFO:name:epoch 7 step 4200 loss 0.08142
INFO:name:epoch 7 step 4300 loss 0.08354
INFO:name:epoch 7 step 4400 loss 0.08597
INFO:name:epoch 7 step 4500 loss 0.07522
INFO:name:epoch 7 step 4600 loss 0.08637
INFO:name:epoch 7 step 4700 loss 0.07874
INFO:name:epoch 7 step 4800 loss 0.08754
INFO:name:epoch 7 step 4900 loss 0.0854
INFO:name:epoch 7 step 5000 loss 0.08912
INFO:name:epoch 7 step 5100 loss 0.08319
INFO:name:epoch 7 step 5200 loss 0.08255
INFO:name:epoch 7 step 5300 loss 0.09859
INFO:name:epoch 7 step 5400 loss 0.08337
INFO:name:epoch 7 step 5500 loss 0.091
INFO:name:epoch 7 step 5600 loss 0.07849
INFO:name:epoch 7 step 5700 loss 0.08585
INFO:name:epoch 7 step 5800 loss 0.08823
INFO:name:epoch 7 step 5900 loss 0.09112
INFO:name:epoch 7 step 6000 loss 0.08195
INFO:name:epoch 7 step 6100 loss 0.08617
INFO:name:epoch 7 step 6200 loss 0.08991
INFO:name:epoch 7 step 6300 loss 0.0736
INFO:name:epoch 7 step 6400 loss 0.0841
INFO:name:epoch 7 step 6500 loss 0.07612
INFO:name:epoch 7 step 6600 loss 0.07616
INFO:name:epoch 7 step 6700 loss 0.08382
INFO:name:epoch 7 step 6800 loss 0.08421
INFO:name:epoch 7 step 6900 loss 0.08475
INFO:name:epoch 7 step 7000 loss 0.0727
INFO:name:epoch 7 step 7100 loss 0.08145
INFO:name:epoch 7 step 7200 loss 0.07588
INFO:name:epoch 7 step 7300 loss 0.07735
INFO:name:epoch 7 step 7400 loss 0.08221
INFO:name:epoch 7 step 7500 loss 0.07864
INFO:name:epoch 7 step 7600 loss 0.08485
INFO:name:epoch 7 step 7700 loss 0.09561
INFO:name:epoch 7 step 7800 loss 0.07205
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1737
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1737
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1321
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 8 step 100 loss 0.07712
INFO:name:epoch 8 step 200 loss 0.06441
INFO:name:epoch 8 step 300 loss 0.06331
INFO:name:epoch 8 step 400 loss 0.07121
INFO:name:epoch 8 step 500 loss 0.07581
INFO:name:epoch 8 step 600 loss 0.06814
INFO:name:epoch 8 step 700 loss 0.07555
INFO:name:epoch 8 step 800 loss 0.07384
INFO:name:epoch 8 step 900 loss 0.07416
INFO:name:epoch 8 step 1000 loss 0.05629
INFO:name:epoch 8 step 1100 loss 0.06551
INFO:name:epoch 8 step 1200 loss 0.07538
INFO:name:epoch 8 step 1300 loss 0.05583
INFO:name:epoch 8 step 1400 loss 0.074
INFO:name:epoch 8 step 1500 loss 0.07999
INFO:name:epoch 8 step 1600 loss 0.07621
INFO:name:epoch 8 step 1700 loss 0.06966
INFO:name:epoch 8 step 1800 loss 0.07922
INFO:name:epoch 8 step 1900 loss 0.07231
INFO:name:epoch 8 step 2000 loss 0.06774
INFO:name:epoch 8 step 2100 loss 0.07305
INFO:name:epoch 8 step 2200 loss 0.07349
INFO:name:epoch 8 step 2300 loss 0.07099
INFO:name:epoch 8 step 2400 loss 0.05612
INFO:name:epoch 8 step 2500 loss 0.0787
INFO:name:epoch 8 step 2600 loss 0.07747
INFO:name:epoch 8 step 2700 loss 0.06741
INFO:name:epoch 8 step 2800 loss 0.08063
INFO:name:epoch 8 step 2900 loss 0.06268
INFO:name:epoch 8 step 3000 loss 0.07037
INFO:name:epoch 8 step 3100 loss 0.06134
INFO:name:epoch 8 step 3200 loss 0.08881
INFO:name:epoch 8 step 3300 loss 0.06835
INFO:name:epoch 8 step 3400 loss 0.06657
INFO:name:epoch 8 step 3500 loss 0.07765
INFO:name:epoch 8 step 3600 loss 0.07637
INFO:name:epoch 8 step 3700 loss 0.07054
INFO:name:epoch 8 step 3800 loss 0.0646
INFO:name:epoch 8 step 3900 loss 0.06509
INFO:name:epoch 8 step 4000 loss 0.0697
INFO:name:epoch 8 step 4100 loss 0.0644
INFO:name:epoch 8 step 4200 loss 0.08202
INFO:name:epoch 8 step 4300 loss 0.08412
INFO:name:epoch 8 step 4400 loss 0.06604
INFO:name:epoch 8 step 4500 loss 0.06612
INFO:name:epoch 8 step 4600 loss 0.06691
INFO:name:epoch 8 step 4700 loss 0.06288
INFO:name:epoch 8 step 4800 loss 0.06267
INFO:name:epoch 8 step 4900 loss 0.06585
INFO:name:epoch 8 step 5000 loss 0.07737
INFO:name:epoch 8 step 5100 loss 0.06999
INFO:name:epoch 8 step 5200 loss 0.07408
INFO:name:epoch 8 step 5300 loss 0.06612
INFO:name:epoch 8 step 5400 loss 0.06132
INFO:name:epoch 8 step 5500 loss 0.05676
INFO:name:epoch 8 step 5600 loss 0.0753
INFO:name:epoch 8 step 5700 loss 0.07566
INFO:name:epoch 8 step 5800 loss 0.079
INFO:name:epoch 8 step 5900 loss 0.06262
INFO:name:epoch 8 step 6000 loss 0.07378
INFO:name:epoch 8 step 6100 loss 0.06808
INFO:name:epoch 8 step 6200 loss 0.07003
INFO:name:epoch 8 step 6300 loss 0.06657
INFO:name:epoch 8 step 6400 loss 0.06726
INFO:name:epoch 8 step 6500 loss 0.07054
INFO:name:epoch 8 step 6600 loss 0.07171
INFO:name:epoch 8 step 6700 loss 0.06415
INFO:name:epoch 8 step 6800 loss 0.06356
INFO:name:epoch 8 step 6900 loss 0.07012
INFO:name:epoch 8 step 7000 loss 0.08275
INFO:name:epoch 8 step 7100 loss 0.06679
INFO:name:epoch 8 step 7200 loss 0.06127
INFO:name:epoch 8 step 7300 loss 0.06637
INFO:name:epoch 8 step 7400 loss 0.07056
INFO:name:epoch 8 step 7500 loss 0.06374
INFO:name:epoch 8 step 7600 loss 0.06659
INFO:name:epoch 8 step 7700 loss 0.07456
INFO:name:epoch 8 step 7800 loss 0.06476
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1677
INFO:name:epoch 9 step 100 loss 0.05708
INFO:name:epoch 9 step 200 loss 0.05756
INFO:name:epoch 9 step 300 loss 0.05147
INFO:name:epoch 9 step 400 loss 0.0532
INFO:name:epoch 9 step 500 loss 0.05425
INFO:name:epoch 9 step 600 loss 0.05627
INFO:name:epoch 9 step 700 loss 0.04657
INFO:name:epoch 9 step 800 loss 0.05113
INFO:name:epoch 9 step 900 loss 0.05878
INFO:name:epoch 9 step 1000 loss 0.05159
INFO:name:epoch 9 step 1100 loss 0.05103
INFO:name:epoch 9 step 1200 loss 0.06078
INFO:name:epoch 9 step 1300 loss 0.05782
INFO:name:epoch 9 step 1400 loss 0.05196
INFO:name:epoch 9 step 1500 loss 0.06131
INFO:name:epoch 9 step 1600 loss 0.06651
INFO:name:epoch 9 step 1700 loss 0.05182
INFO:name:epoch 9 step 1800 loss 0.05566
INFO:name:epoch 9 step 1900 loss 0.05517
INFO:name:epoch 9 step 2000 loss 0.05677
INFO:name:epoch 9 step 2100 loss 0.06048
INFO:name:epoch 9 step 2200 loss 0.05692
INFO:name:epoch 9 step 2300 loss 0.06301
INFO:name:epoch 9 step 2400 loss 0.05237
INFO:name:epoch 9 step 2500 loss 0.05814
INFO:name:epoch 9 step 2600 loss 0.05447
INFO:name:epoch 9 step 2700 loss 0.06348
INFO:name:epoch 9 step 2800 loss 0.05721
INFO:name:epoch 9 step 2900 loss 0.05916
INFO:name:epoch 9 step 3000 loss 0.05697
INFO:name:epoch 9 step 3100 loss 0.05963
INFO:name:epoch 9 step 3200 loss 0.06353
INFO:name:epoch 9 step 3300 loss 0.06096
INFO:name:epoch 9 step 3400 loss 0.05562
INFO:name:epoch 9 step 3500 loss 0.04934
INFO:name:epoch 9 step 3600 loss 0.05208
INFO:name:epoch 9 step 3700 loss 0.05367
INFO:name:epoch 9 step 3800 loss 0.05884
INFO:name:epoch 9 step 3900 loss 0.05982
INFO:name:epoch 9 step 4000 loss 0.05946
INFO:name:epoch 9 step 4100 loss 0.06198
INFO:name:epoch 9 step 4200 loss 0.05497
INFO:name:epoch 9 step 4300 loss 0.06059
INFO:name:epoch 9 step 4400 loss 0.06244
INFO:name:epoch 9 step 4500 loss 0.05943
INFO:name:epoch 9 step 4600 loss 0.07101
INFO:name:epoch 9 step 4700 loss 0.0664
INFO:name:epoch 9 step 4800 loss 0.05378
INFO:name:epoch 9 step 4900 loss 0.05664
INFO:name:epoch 9 step 5000 loss 0.05921
INFO:name:epoch 9 step 5100 loss 0.06434
INFO:name:epoch 9 step 5200 loss 0.06572
INFO:name:epoch 9 step 5300 loss 0.06416
INFO:name:epoch 9 step 5400 loss 0.06421
INFO:name:epoch 9 step 5500 loss 0.05546
INFO:name:epoch 9 step 5600 loss 0.0517
INFO:name:epoch 9 step 5700 loss 0.06722
INFO:name:epoch 9 step 5800 loss 0.06168
INFO:name:epoch 9 step 5900 loss 0.05705
INFO:name:epoch 9 step 6000 loss 0.05395
INFO:name:epoch 9 step 6100 loss 0.0612
INFO:name:epoch 9 step 6200 loss 0.06078
INFO:name:epoch 9 step 6300 loss 0.0639
INFO:name:epoch 9 step 6400 loss 0.05689
INFO:name:epoch 9 step 6500 loss 0.06207
INFO:name:epoch 9 step 6600 loss 0.05644
INFO:name:epoch 9 step 6700 loss 0.06247
INFO:name:epoch 9 step 6800 loss 0.05707
INFO:name:epoch 9 step 6900 loss 0.06027
INFO:name:epoch 9 step 7000 loss 0.06166
INFO:name:epoch 9 step 7100 loss 0.06079
INFO:name:epoch 9 step 7200 loss 0.06218
INFO:name:epoch 9 step 7300 loss 0.06629
INFO:name:epoch 9 step 7400 loss 0.06661
INFO:name:epoch 9 step 7500 loss 0.05678
INFO:name:epoch 9 step 7600 loss 0.05809
INFO:name:epoch 9 step 7700 loss 0.05634
INFO:name:epoch 9 step 7800 loss 0.05725
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1847
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1847
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.142
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 10 step 100 loss 0.04606
INFO:name:epoch 10 step 200 loss 0.05262
INFO:name:epoch 10 step 300 loss 0.05679
INFO:name:epoch 10 step 400 loss 0.04756
INFO:name:epoch 10 step 500 loss 0.04158
INFO:name:epoch 10 step 600 loss 0.05033
INFO:name:epoch 10 step 700 loss 0.04969
INFO:name:epoch 10 step 800 loss 0.05841
INFO:name:epoch 10 step 900 loss 0.04916
INFO:name:epoch 10 step 1000 loss 0.05184
INFO:name:epoch 10 step 1100 loss 0.04001
INFO:name:epoch 10 step 1200 loss 0.05557
INFO:name:epoch 10 step 1300 loss 0.04785
INFO:name:epoch 10 step 1400 loss 0.05507
INFO:name:epoch 10 step 1500 loss 0.04476
INFO:name:epoch 10 step 1600 loss 0.0379
INFO:name:epoch 10 step 1700 loss 0.03899
INFO:name:epoch 10 step 1800 loss 0.04906
INFO:name:epoch 10 step 1900 loss 0.05061
INFO:name:epoch 10 step 2000 loss 0.05012
INFO:name:epoch 10 step 2100 loss 0.04759
INFO:name:epoch 10 step 2200 loss 0.05776
INFO:name:epoch 10 step 2300 loss 0.04506
INFO:name:epoch 10 step 2400 loss 0.04888
INFO:name:epoch 10 step 2500 loss 0.04862
INFO:name:epoch 10 step 2600 loss 0.04299
INFO:name:epoch 10 step 2700 loss 0.04729
INFO:name:epoch 10 step 2800 loss 0.05772
INFO:name:epoch 10 step 2900 loss 0.06046
INFO:name:epoch 10 step 3000 loss 0.04868
INFO:name:epoch 10 step 3100 loss 0.04698
INFO:name:epoch 10 step 3200 loss 0.04626
INFO:name:epoch 10 step 3300 loss 0.04685
INFO:name:epoch 10 step 3400 loss 0.0556
INFO:name:epoch 10 step 3500 loss 0.04327
INFO:name:epoch 10 step 3600 loss 0.04895
INFO:name:epoch 10 step 3700 loss 0.04303
INFO:name:epoch 10 step 3800 loss 0.04907
INFO:name:epoch 10 step 3900 loss 0.04971
INFO:name:epoch 10 step 4000 loss 0.04911
INFO:name:epoch 10 step 4100 loss 0.05225
INFO:name:epoch 10 step 4200 loss 0.05101
INFO:name:epoch 10 step 4300 loss 0.04615
INFO:name:epoch 10 step 4400 loss 0.05742
INFO:name:epoch 10 step 4500 loss 0.04305
INFO:name:epoch 10 step 4600 loss 0.04359
INFO:name:epoch 10 step 4700 loss 0.05049
INFO:name:epoch 10 step 4800 loss 0.05047
INFO:name:epoch 10 step 4900 loss 0.04899
INFO:name:epoch 10 step 5000 loss 0.05026
INFO:name:epoch 10 step 5100 loss 0.05364
INFO:name:epoch 10 step 5200 loss 0.04696
INFO:name:epoch 10 step 5300 loss 0.05157
INFO:name:epoch 10 step 5400 loss 0.04861
INFO:name:epoch 10 step 5500 loss 0.04268
INFO:name:epoch 10 step 5600 loss 0.05397
INFO:name:epoch 10 step 5700 loss 0.04702
INFO:name:epoch 10 step 5800 loss 0.04721
INFO:name:epoch 10 step 5900 loss 0.04771
INFO:name:epoch 10 step 6000 loss 0.05595
INFO:name:epoch 10 step 6100 loss 0.05217
INFO:name:epoch 10 step 6200 loss 0.05158
INFO:name:epoch 10 step 6300 loss 0.04708
INFO:name:epoch 10 step 6400 loss 0.05217
INFO:name:epoch 10 step 6500 loss 0.0582
INFO:name:epoch 10 step 6600 loss 0.05302
INFO:name:epoch 10 step 6700 loss 0.05596
INFO:name:epoch 10 step 6800 loss 0.05368
INFO:name:epoch 10 step 6900 loss 0.05322
INFO:name:epoch 10 step 7000 loss 0.05333
INFO:name:epoch 10 step 7100 loss 0.03662
INFO:name:epoch 10 step 7200 loss 0.05251
INFO:name:epoch 10 step 7300 loss 0.04829
INFO:name:epoch 10 step 7400 loss 0.05363
INFO:name:epoch 10 step 7500 loss 0.05298
INFO:name:epoch 10 step 7600 loss 0.0533
INFO:name:epoch 10 step 7700 loss 0.04752
INFO:name:epoch 10 step 7800 loss 0.05169
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.191
INFO:name:  ********************
INFO:name:  Best eval mrr:0.191
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1458
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 11 step 100 loss 0.05221
INFO:name:epoch 11 step 200 loss 0.05
INFO:name:epoch 11 step 300 loss 0.05001
INFO:name:epoch 11 step 400 loss 0.04156
INFO:name:epoch 11 step 500 loss 0.04028
INFO:name:epoch 11 step 600 loss 0.04674
INFO:name:epoch 11 step 700 loss 0.03844
INFO:name:epoch 11 step 800 loss 0.05122
INFO:name:epoch 11 step 900 loss 0.0386
INFO:name:epoch 11 step 1000 loss 0.04714
INFO:name:epoch 11 step 1100 loss 0.04866
INFO:name:epoch 11 step 1200 loss 0.03906
INFO:name:epoch 11 step 1300 loss 0.05083
INFO:name:epoch 11 step 1400 loss 0.04036
INFO:name:epoch 11 step 1500 loss 0.04437
INFO:name:epoch 11 step 1600 loss 0.03911
INFO:name:epoch 11 step 1700 loss 0.04096
INFO:name:epoch 11 step 1800 loss 0.04092
INFO:name:epoch 11 step 1900 loss 0.04225
INFO:name:epoch 11 step 2000 loss 0.04156
INFO:name:epoch 11 step 2100 loss 0.04845
INFO:name:epoch 11 step 2200 loss 0.04347
INFO:name:epoch 11 step 2300 loss 0.0383
INFO:name:epoch 11 step 2400 loss 0.047
INFO:name:epoch 11 step 2500 loss 0.03967
INFO:name:epoch 11 step 2600 loss 0.05103
INFO:name:epoch 11 step 2700 loss 0.03777
INFO:name:epoch 11 step 2800 loss 0.03612
INFO:name:epoch 11 step 2900 loss 0.04442
INFO:name:epoch 11 step 3000 loss 0.04763
INFO:name:epoch 11 step 3100 loss 0.03809
INFO:name:epoch 11 step 3200 loss 0.03946
INFO:name:epoch 11 step 3300 loss 0.04463
INFO:name:epoch 11 step 3400 loss 0.04762
INFO:name:epoch 11 step 3500 loss 0.04381
INFO:name:epoch 11 step 3600 loss 0.04476
INFO:name:epoch 11 step 3700 loss 0.0427
INFO:name:epoch 11 step 3800 loss 0.04021
INFO:name:epoch 11 step 3900 loss 0.03911
INFO:name:epoch 11 step 4000 loss 0.03716
INFO:name:epoch 11 step 4100 loss 0.0484
INFO:name:epoch 11 step 4200 loss 0.04491
INFO:name:epoch 11 step 4300 loss 0.04275
INFO:name:epoch 11 step 4400 loss 0.04807
INFO:name:epoch 11 step 4500 loss 0.04896
INFO:name:epoch 11 step 4600 loss 0.04518
INFO:name:epoch 11 step 4700 loss 0.04794
INFO:name:epoch 11 step 4800 loss 0.05107
INFO:name:epoch 11 step 4900 loss 0.04677
INFO:name:epoch 11 step 5000 loss 0.04362
INFO:name:epoch 11 step 5100 loss 0.0459
INFO:name:epoch 11 step 5200 loss 0.04447
INFO:name:epoch 11 step 5300 loss 0.04615
INFO:name:epoch 11 step 5400 loss 0.04792
INFO:name:epoch 11 step 5500 loss 0.04207
INFO:name:epoch 11 step 5600 loss 0.04386
INFO:name:epoch 11 step 5700 loss 0.05141
INFO:name:epoch 11 step 5800 loss 0.04665
INFO:name:epoch 11 step 5900 loss 0.04026
INFO:name:epoch 11 step 6000 loss 0.04526
INFO:name:epoch 11 step 6100 loss 0.04611
INFO:name:epoch 11 step 6200 loss 0.0404
INFO:name:epoch 11 step 6300 loss 0.04166
INFO:name:epoch 11 step 6400 loss 0.04935
INFO:name:epoch 11 step 6500 loss 0.04238
INFO:name:epoch 11 step 6600 loss 0.048
INFO:name:epoch 11 step 6700 loss 0.04398
INFO:name:epoch 11 step 6800 loss 0.03742
INFO:name:epoch 11 step 6900 loss 0.04246
INFO:name:epoch 11 step 7000 loss 0.04573
INFO:name:epoch 11 step 7100 loss 0.04544
INFO:name:epoch 11 step 7200 loss 0.03594
INFO:name:epoch 11 step 7300 loss 0.05233
INFO:name:epoch 11 step 7400 loss 0.04951
INFO:name:epoch 11 step 7500 loss 0.04566
INFO:name:epoch 11 step 7600 loss 0.05067
INFO:name:epoch 11 step 7700 loss 0.04533
INFO:name:epoch 11 step 7800 loss 0.04743
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.195
INFO:name:  ********************
INFO:name:  Best eval mrr:0.195
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1507
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 12 step 100 loss 0.04195
INFO:name:epoch 12 step 200 loss 0.03142
INFO:name:epoch 12 step 300 loss 0.03871
INFO:name:epoch 12 step 400 loss 0.04037
INFO:name:epoch 12 step 500 loss 0.03721
INFO:name:epoch 12 step 600 loss 0.04341
INFO:name:epoch 12 step 700 loss 0.0394
INFO:name:epoch 12 step 800 loss 0.04286
INFO:name:epoch 12 step 900 loss 0.03754
INFO:name:epoch 12 step 1000 loss 0.03657
INFO:name:epoch 12 step 1100 loss 0.03264
INFO:name:epoch 12 step 1200 loss 0.0381
INFO:name:epoch 12 step 1300 loss 0.03715
INFO:name:epoch 12 step 1400 loss 0.03626
INFO:name:epoch 12 step 1500 loss 0.03406
INFO:name:epoch 12 step 1600 loss 0.03981
INFO:name:epoch 12 step 1700 loss 0.04
INFO:name:epoch 12 step 1800 loss 0.03807
INFO:name:epoch 12 step 1900 loss 0.03977
INFO:name:epoch 12 step 2000 loss 0.04375
INFO:name:epoch 12 step 2100 loss 0.03886
INFO:name:epoch 12 step 2200 loss 0.03795
INFO:name:epoch 12 step 2300 loss 0.04382
INFO:name:epoch 12 step 2400 loss 0.04148
INFO:name:epoch 12 step 2500 loss 0.03557
INFO:name:epoch 12 step 2600 loss 0.04497
INFO:name:epoch 12 step 2700 loss 0.03325
INFO:name:epoch 12 step 2800 loss 0.03928
INFO:name:epoch 12 step 2900 loss 0.04
INFO:name:epoch 12 step 3000 loss 0.03768
INFO:name:epoch 12 step 3100 loss 0.04731
INFO:name:epoch 12 step 3200 loss 0.03998
INFO:name:epoch 12 step 3300 loss 0.0435
INFO:name:epoch 12 step 3400 loss 0.03479
INFO:name:epoch 12 step 3500 loss 0.04413
INFO:name:epoch 12 step 3600 loss 0.03277
INFO:name:epoch 12 step 3700 loss 0.03655
INFO:name:epoch 12 step 3800 loss 0.03171
INFO:name:epoch 12 step 3900 loss 0.04063
INFO:name:epoch 12 step 4000 loss 0.04004
INFO:name:epoch 12 step 4100 loss 0.04041
INFO:name:epoch 12 step 4200 loss 0.04
INFO:name:epoch 12 step 4300 loss 0.03887
INFO:name:epoch 12 step 4400 loss 0.03802
INFO:name:epoch 12 step 4500 loss 0.03682
INFO:name:epoch 12 step 4600 loss 0.0429
INFO:name:epoch 12 step 4700 loss 0.04306
INFO:name:epoch 12 step 4800 loss 0.03929
INFO:name:epoch 12 step 4900 loss 0.04155
INFO:name:epoch 12 step 5000 loss 0.0381
INFO:name:epoch 12 step 5100 loss 0.03599
INFO:name:epoch 12 step 5200 loss 0.03438
INFO:name:epoch 12 step 5300 loss 0.03841
INFO:name:epoch 12 step 5400 loss 0.04264
INFO:name:epoch 12 step 5500 loss 0.03938
INFO:name:epoch 12 step 5600 loss 0.03853
INFO:name:epoch 12 step 5700 loss 0.03311
INFO:name:epoch 12 step 5800 loss 0.04272
INFO:name:epoch 12 step 5900 loss 0.03976
INFO:name:epoch 12 step 6000 loss 0.03316
INFO:name:epoch 12 step 6100 loss 0.03889
INFO:name:epoch 12 step 6200 loss 0.03662
INFO:name:epoch 12 step 6300 loss 0.04232
INFO:name:epoch 12 step 6400 loss 0.04653
INFO:name:epoch 12 step 6500 loss 0.03929
INFO:name:epoch 12 step 6600 loss 0.0343
INFO:name:epoch 12 step 6700 loss 0.04276
INFO:name:epoch 12 step 6800 loss 0.04018
INFO:name:epoch 12 step 6900 loss 0.04242
INFO:name:epoch 12 step 7000 loss 0.03369
INFO:name:epoch 12 step 7100 loss 0.04693
INFO:name:epoch 12 step 7200 loss 0.03797
INFO:name:epoch 12 step 7300 loss 0.03551
INFO:name:epoch 12 step 7400 loss 0.04322
INFO:name:epoch 12 step 7500 loss 0.04365
INFO:name:epoch 12 step 7600 loss 0.03673
INFO:name:epoch 12 step 7700 loss 0.03919
INFO:name:epoch 12 step 7800 loss 0.03847
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.197
INFO:name:  ********************
INFO:name:  Best eval mrr:0.197
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1529
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 13 step 100 loss 0.03523
INFO:name:epoch 13 step 200 loss 0.03443
INFO:name:epoch 13 step 300 loss 0.03365
INFO:name:epoch 13 step 400 loss 0.03412
INFO:name:epoch 13 step 500 loss 0.03371
INFO:name:epoch 13 step 600 loss 0.03662
INFO:name:epoch 13 step 700 loss 0.03792
INFO:name:epoch 13 step 800 loss 0.03225
INFO:name:epoch 13 step 900 loss 0.03423
INFO:name:epoch 13 step 1000 loss 0.0305
INFO:name:epoch 13 step 1100 loss 0.03294
INFO:name:epoch 13 step 1200 loss 0.03184
INFO:name:epoch 13 step 1300 loss 0.03386
INFO:name:epoch 13 step 1400 loss 0.03756
INFO:name:epoch 13 step 1500 loss 0.02991
INFO:name:epoch 13 step 1600 loss 0.03101
INFO:name:epoch 13 step 1700 loss 0.0348
INFO:name:epoch 13 step 1800 loss 0.0323
INFO:name:epoch 13 step 1900 loss 0.03436
INFO:name:epoch 13 step 2000 loss 0.03859
INFO:name:epoch 13 step 2100 loss 0.0383
INFO:name:epoch 13 step 2200 loss 0.03473
INFO:name:epoch 13 step 2300 loss 0.03173
INFO:name:epoch 13 step 2400 loss 0.0332
INFO:name:epoch 13 step 2500 loss 0.03697
INFO:name:epoch 13 step 2600 loss 0.03263
INFO:name:epoch 13 step 2700 loss 0.03449
INFO:name:epoch 13 step 2800 loss 0.03671
INFO:name:epoch 13 step 2900 loss 0.03674
INFO:name:epoch 13 step 3000 loss 0.0325
INFO:name:epoch 13 step 3100 loss 0.03411
INFO:name:epoch 13 step 3200 loss 0.03109
INFO:name:epoch 13 step 3300 loss 0.03213
INFO:name:epoch 13 step 3400 loss 0.0383
INFO:name:epoch 13 step 3500 loss 0.03658
INFO:name:epoch 13 step 3600 loss 0.03362
INFO:name:epoch 13 step 3700 loss 0.0339
INFO:name:epoch 13 step 3800 loss 0.03964
INFO:name:epoch 13 step 3900 loss 0.02843
INFO:name:epoch 13 step 4000 loss 0.02987
INFO:name:epoch 13 step 4100 loss 0.03757
INFO:name:epoch 13 step 4200 loss 0.03502
INFO:name:epoch 13 step 4300 loss 0.03909
INFO:name:epoch 13 step 4400 loss 0.03512
INFO:name:epoch 13 step 4500 loss 0.03588
INFO:name:epoch 13 step 4600 loss 0.03305
INFO:name:epoch 13 step 4700 loss 0.038
INFO:name:epoch 13 step 4800 loss 0.03725
INFO:name:epoch 13 step 4900 loss 0.03398
INFO:name:epoch 13 step 5000 loss 0.02949
INFO:name:epoch 13 step 5100 loss 0.03747
INFO:name:epoch 13 step 5200 loss 0.04026
INFO:name:epoch 13 step 5300 loss 0.03533
INFO:name:epoch 13 step 5400 loss 0.03558
INFO:name:epoch 13 step 5500 loss 0.03057
INFO:name:epoch 13 step 5600 loss 0.03221
INFO:name:epoch 13 step 5700 loss 0.03595
INFO:name:epoch 13 step 5800 loss 0.02836
INFO:name:epoch 13 step 5900 loss 0.03095
INFO:name:epoch 13 step 6000 loss 0.03321
INFO:name:epoch 13 step 6100 loss 0.03562
INFO:name:epoch 13 step 6200 loss 0.03836
INFO:name:epoch 13 step 6300 loss 0.03173
INFO:name:epoch 13 step 6400 loss 0.03259
INFO:name:epoch 13 step 6500 loss 0.03822
INFO:name:epoch 13 step 6600 loss 0.03052
INFO:name:epoch 13 step 6700 loss 0.04357
INFO:name:epoch 13 step 6800 loss 0.03151
INFO:name:epoch 13 step 6900 loss 0.0314
INFO:name:epoch 13 step 7000 loss 0.02995
INFO:name:epoch 13 step 7100 loss 0.03828
INFO:name:epoch 13 step 7200 loss 0.03427
INFO:name:epoch 13 step 7300 loss 0.03491
INFO:name:epoch 13 step 7400 loss 0.03029
INFO:name:epoch 13 step 7500 loss 0.03918
INFO:name:epoch 13 step 7600 loss 0.04226
INFO:name:epoch 13 step 7700 loss 0.03422
INFO:name:epoch 13 step 7800 loss 0.03126
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.202
INFO:name:  ********************
INFO:name:  Best eval mrr:0.202
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1567
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 14 step 100 loss 0.03182
INFO:name:epoch 14 step 200 loss 0.03893
INFO:name:epoch 14 step 300 loss 0.03493
INFO:name:epoch 14 step 400 loss 0.02839
INFO:name:epoch 14 step 500 loss 0.0329
INFO:name:epoch 14 step 600 loss 0.03021
INFO:name:epoch 14 step 700 loss 0.03147
INFO:name:epoch 14 step 800 loss 0.02894
INFO:name:epoch 14 step 900 loss 0.02964
INFO:name:epoch 14 step 1000 loss 0.03234
INFO:name:epoch 14 step 1100 loss 0.03035
INFO:name:epoch 14 step 1200 loss 0.03121
INFO:name:epoch 14 step 1300 loss 0.03317
INFO:name:epoch 14 step 1400 loss 0.02923
INFO:name:epoch 14 step 1500 loss 0.03128
INFO:name:epoch 14 step 1600 loss 0.02977
INFO:name:epoch 14 step 1700 loss 0.03063
INFO:name:epoch 14 step 1800 loss 0.02666
INFO:name:epoch 14 step 1900 loss 0.03403
INFO:name:epoch 14 step 2000 loss 0.03531
INFO:name:epoch 14 step 2100 loss 0.0345
INFO:name:epoch 14 step 2200 loss 0.03496
INFO:name:epoch 14 step 2300 loss 0.03477
INFO:name:epoch 14 step 2400 loss 0.03242
INFO:name:epoch 14 step 2500 loss 0.0317
INFO:name:epoch 14 step 2600 loss 0.03233
INFO:name:epoch 14 step 2700 loss 0.0305
INFO:name:epoch 14 step 2800 loss 0.03236
INFO:name:epoch 14 step 2900 loss 0.02954
INFO:name:epoch 14 step 3000 loss 0.03496
INFO:name:epoch 14 step 3100 loss 0.02663
INFO:name:epoch 14 step 3200 loss 0.03235
INFO:name:epoch 14 step 3300 loss 0.02992
INFO:name:epoch 14 step 3400 loss 0.03446
INFO:name:epoch 14 step 3500 loss 0.03217
INFO:name:epoch 14 step 3600 loss 0.03474
INFO:name:epoch 14 step 3700 loss 0.03649
INFO:name:epoch 14 step 3800 loss 0.03148
INFO:name:epoch 14 step 3900 loss 0.03791
INFO:name:epoch 14 step 4000 loss 0.02631
INFO:name:epoch 14 step 4100 loss 0.03008
INFO:name:epoch 14 step 4200 loss 0.02733
INFO:name:epoch 14 step 4300 loss 0.03802
INFO:name:epoch 14 step 4400 loss 0.03047
INFO:name:epoch 14 step 4500 loss 0.03342
INFO:name:epoch 14 step 4600 loss 0.03584
INFO:name:epoch 14 step 4700 loss 0.03858
INFO:name:epoch 14 step 4800 loss 0.03163
INFO:name:epoch 14 step 4900 loss 0.0305
INFO:name:epoch 14 step 5000 loss 0.03593
INFO:name:epoch 14 step 5100 loss 0.02841
INFO:name:epoch 14 step 5200 loss 0.0327
INFO:name:epoch 14 step 5300 loss 0.03125
INFO:name:epoch 14 step 5400 loss 0.03291
INFO:name:epoch 14 step 5500 loss 0.03292
INFO:name:epoch 14 step 5600 loss 0.03972
INFO:name:epoch 14 step 5700 loss 0.03272
INFO:name:epoch 14 step 5800 loss 0.02879
INFO:name:epoch 14 step 5900 loss 0.02913
INFO:name:epoch 14 step 6000 loss 0.0318
INFO:name:epoch 14 step 6100 loss 0.03219
INFO:name:epoch 14 step 6200 loss 0.03366
INFO:name:epoch 14 step 6300 loss 0.03449
INFO:name:epoch 14 step 6400 loss 0.03512
INFO:name:epoch 14 step 6500 loss 0.03998
INFO:name:epoch 14 step 6600 loss 0.03367
INFO:name:epoch 14 step 6700 loss 0.03206
INFO:name:epoch 14 step 6800 loss 0.03434
INFO:name:epoch 14 step 6900 loss 0.03152
INFO:name:epoch 14 step 7000 loss 0.0313
INFO:name:epoch 14 step 7100 loss 0.03295
INFO:name:epoch 14 step 7200 loss 0.03486
INFO:name:epoch 14 step 7300 loss 0.02875
INFO:name:epoch 14 step 7400 loss 0.02879
INFO:name:epoch 14 step 7500 loss 0.03328
INFO:name:epoch 14 step 7600 loss 0.03284
INFO:name:epoch 14 step 7700 loss 0.03656
INFO:name:epoch 14 step 7800 loss 0.03377
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1973
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
train results ([1.749663939477528, 0.41579265756250905, 0.2568539513762581, 0.18963212124110476, 0.14757653302974583, 0.11902343139379004, 0.09706973174996343, 0.08184649987655705, 0.06972609533466223, 0.05835400395143571, 0.04968052251688104, 0.0444465693310341, 0.03909420059911618, 0.03439968357309687, 0.03242545171766252], [0.04974873919140913, 0.07810515565421275, 0.1188923450802975, 0.13111440976191996, 0.1461881322795718, 0.1576708815503159, 0.1710319344483331, 0.17370264109802888, 0.16773409124003935, 0.1847040402906907, 0.19097497981214825, 0.19499661100227694, 0.19704603146204036, 0.20204833994263358, 0.19731948450880024])
