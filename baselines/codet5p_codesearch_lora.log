/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
INFO:name:device: cuda:0, n_gpu: 1
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/config.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /Salesforce/codet5p-220m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

root
├── shared (Embedding) weight:[32100, 768]
├── encoder (T5Stack)
│   ├── embed_tokens (Embedding) weight:[32100, 768]
│   ├── block (ModuleList)
│   │   ├── 0 (T5Block)
│   │   │   └── layer (ModuleList)
│   │   │       ├── 0 (T5LayerSelfAttention)
│   │   │       │   ├── SelfAttention (T5Attention)
│   │   │       │   │   ├── q,v(Linear) weight:[768, 768]
│   │   │       │   │   │   └── lora (LowRankLinear) lora_A:[8, 768] lora_B:[768, 8]
│   │   │       │   │   ├── k,o(Linear) weight:[768, 768]
│   │   │       │   │   └── relative_attention_bias (Embedding) weight:[32, 12]
│   │   │       │   └── layer_norm (T5LayerNorm) weight:[768]
│   │   │       └── 1 (T5LayerFF)
│   │   │           ├── DenseReluDense (T5DenseActDense)
│   │   │           │   ├── wi (Linear) weight:[3072, 768]
│   │   │           │   └── wo (Linear) weight:[768, 3072]
│   │   │           └── layer_norm (T5LayerNorm) weight:[768]
│   │   └── 1-11(T5Block)
│   │       └── layer (ModuleList)
│   │           ├── 0 (T5LayerSelfAttention)
│   │           │   ├── SelfAttention (T5Attention)
│   │           │   │   ├── q,v(Linear) weight:[768, 768]
│   │           │   │   │   └── lora (LowRankLinear) lora_A:[8, 768] lora_B:[768, 8]
│   │           │   │   └── k,o(Linear) weight:[768, 768]
│   │           │   └── layer_norm (T5LayerNorm) weight:[768]
│   │           └── 1 (T5LayerFF)
│   │               ├── DenseReluDense (T5DenseActDense)
│   │               │   ├── wi (Linear) weight:[3072, 768]
│   │               │   └── wo (Linear) weight:[768, 3072]
│   │               └── layer_norm (T5LayerNorm) weight:[768]
│   └── final_layer_norm (T5LayerNorm) weight:[768]
└── decoder (T5Stack)
    ├── embed_tokens (Embedding) weight:[32100, 768]
    ├── block (ModuleList)
    │   ├── 0 (T5Block)
    │   │   └── layer (ModuleList)
    │   │       ├── 0 (T5LayerSelfAttention)
    │   │       │   ├── SelfAttention (T5Attention)
    │   │       │   │   ├── q,v(Linear) weight:[768, 768]
    │   │       │   │   │   └── lora (LowRankLinear) lora_A:[8, 768] lora_B:[768, 8]
    │   │       │   │   ├── k,o(Linear) weight:[768, 768]
    │   │       │   │   └── relative_attention_bias (Embedding) weight:[32, 12]
    │   │       │   └── layer_norm (T5LayerNorm) weight:[768]
    │   │       ├── 1 (T5LayerCrossAttention)
    │   │       │   ├── EncDecAttention (T5Attention)
    │   │       │   │   └── q,k,v,o(Linear) weight:[768, 768]
    │   │       │   └── layer_norm (T5LayerNorm) weight:[768]
    │   │       └── 2 (T5LayerFF)
    │   │           ├── DenseReluDense (T5DenseActDense)
    │   │           │   ├── wi (Linear) weight:[3072, 768]
    │   │           │   └── wo (Linear) weight:[768, 3072]
    │   │           └── layer_norm (T5LayerNorm) weight:[768]
    │   └── 1-11(T5Block)
    │       └── layer (ModuleList)
    │           ├── 0 (T5LayerSelfAttention)
    │           │   ├── SelfAttention (T5Attention)
    │           │   │   ├── q,v(Linear) weight:[768, 768]
    │           │   │   │   └── lora (LowRankLinear) lora_A:[8, 768] lora_B:[768, 8]
    │           │   │   └── k,o(Linear) weight:[768, 768]
    │           │   └── layer_norm (T5LayerNorm) weight:[768]
    │           ├── 1 (T5LayerCrossAttention)
    │           │   ├── EncDecAttention (T5Attention)
    │           │   │   └── q,k,v,o(Linear) weight:[768, 768]
    │           │   └── layer_norm (T5LayerNorm) weight:[768]
    │           └── 2 (T5LayerFF)
    │               ├── DenseReluDense (T5DenseActDense)
    │               │   ├── wi (Linear) weight:[3072, 768]
    │               │   └── wo (Linear) weight:[768, 3072]
    │               └── layer_norm (T5LayerNorm) weight:[768]
    └── final_layer_norm (T5LayerNorm) weight:[768]
[INFO|(OpenDelta)basemodel:700]2025-01-20 01:09:19,782 >> Trainable Ratio: 589824/223471872=0.263937%
[INFO|(OpenDelta)basemodel:702]2025-01-20 01:09:19,783 >> Delta Parameter Ratio: 589824/223471872=0.263937%
[INFO|(OpenDelta)basemodel:704]2025-01-20 01:09:19,783 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 10
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 78700
INFO:name:epoch 0 step 100 loss 3.98897
INFO:name:epoch 0 step 200 loss 2.94112
INFO:name:epoch 0 step 300 loss 1.39808
INFO:name:epoch 0 step 400 loss 0.80679
INFO:name:epoch 0 step 500 loss 0.57095
INFO:name:epoch 0 step 600 loss 0.44611
INFO:name:epoch 0 step 700 loss 0.37324
INFO:name:epoch 0 step 800 loss 0.34181
INFO:name:epoch 0 step 900 loss 0.32199
INFO:name:epoch 0 step 1000 loss 0.28423
INFO:name:epoch 0 step 1100 loss 0.27205
INFO:name:epoch 0 step 1200 loss 0.2337
INFO:name:epoch 0 step 1300 loss 0.26463
INFO:name:epoch 0 step 1400 loss 0.23959
INFO:name:epoch 0 step 1500 loss 0.21519
INFO:name:epoch 0 step 1600 loss 0.23839
INFO:name:epoch 0 step 1700 loss 0.24365
INFO:name:epoch 0 step 1800 loss 0.23372
INFO:name:epoch 0 step 1900 loss 0.1934
INFO:name:epoch 0 step 2000 loss 0.23461
INFO:name:epoch 0 step 2100 loss 0.19814
INFO:name:epoch 0 step 2200 loss 0.20679
INFO:name:epoch 0 step 2300 loss 0.21304
INFO:name:epoch 0 step 2400 loss 0.19419
INFO:name:epoch 0 step 2500 loss 0.19318
INFO:name:epoch 0 step 2600 loss 0.18245
INFO:name:epoch 0 step 2700 loss 0.20334
INFO:name:epoch 0 step 2800 loss 0.1875
INFO:name:epoch 0 step 2900 loss 0.19326
INFO:name:epoch 0 step 3000 loss 0.17101
INFO:name:epoch 0 step 3100 loss 0.18739
INFO:name:epoch 0 step 3200 loss 0.17717
INFO:name:epoch 0 step 3300 loss 0.16516
INFO:name:epoch 0 step 3400 loss 0.15943
INFO:name:epoch 0 step 3500 loss 0.1774
INFO:name:epoch 0 step 3600 loss 0.19602
INFO:name:epoch 0 step 3700 loss 0.15459
INFO:name:epoch 0 step 3800 loss 0.17538
INFO:name:epoch 0 step 3900 loss 0.15752
INFO:name:epoch 0 step 4000 loss 0.17057
INFO:name:epoch 0 step 4100 loss 0.16161
INFO:name:epoch 0 step 4200 loss 0.16146
INFO:name:epoch 0 step 4300 loss 0.13756
INFO:name:epoch 0 step 4400 loss 0.16877
INFO:name:epoch 0 step 4500 loss 0.17615
INFO:name:epoch 0 step 4600 loss 0.17129
INFO:name:epoch 0 step 4700 loss 0.15743
INFO:name:epoch 0 step 4800 loss 0.15581
INFO:name:epoch 0 step 4900 loss 0.13415
INFO:name:epoch 0 step 5000 loss 0.16218
INFO:name:epoch 0 step 5100 loss 0.1431
INFO:name:epoch 0 step 5200 loss 0.15077
INFO:name:epoch 0 step 5300 loss 0.15462
INFO:name:epoch 0 step 5400 loss 0.14191
INFO:name:epoch 0 step 5500 loss 0.15748
INFO:name:epoch 0 step 5600 loss 0.14891
INFO:name:epoch 0 step 5700 loss 0.15724
INFO:name:epoch 0 step 5800 loss 0.15021
INFO:name:epoch 0 step 5900 loss 0.15371
INFO:name:epoch 0 step 6000 loss 0.15835
INFO:name:epoch 0 step 6100 loss 0.14898
INFO:name:epoch 0 step 6200 loss 0.14465
INFO:name:epoch 0 step 6300 loss 0.13925
INFO:name:epoch 0 step 6400 loss 0.15737
INFO:name:epoch 0 step 6500 loss 0.14822
INFO:name:epoch 0 step 6600 loss 0.16091
INFO:name:epoch 0 step 6700 loss 0.14418
INFO:name:epoch 0 step 6800 loss 0.14413
INFO:name:epoch 0 step 6900 loss 0.13953
INFO:name:epoch 0 step 7000 loss 0.13928
INFO:name:epoch 0 step 7100 loss 0.14042
INFO:name:epoch 0 step 7200 loss 0.14892
INFO:name:epoch 0 step 7300 loss 0.142
INFO:name:epoch 0 step 7400 loss 0.14228
INFO:name:epoch 0 step 7500 loss 0.12666
INFO:name:epoch 0 step 7600 loss 0.12138
INFO:name:epoch 0 step 7700 loss 0.143
INFO:name:epoch 0 step 7800 loss 0.15732
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2389
INFO:name:  ********************
INFO:name:  Best eval mrr:0.2389
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1905
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 0.12752
INFO:name:epoch 1 step 200 loss 0.12142
INFO:name:epoch 1 step 300 loss 0.12482
INFO:name:epoch 1 step 400 loss 0.13516
INFO:name:epoch 1 step 500 loss 0.12299
INFO:name:epoch 1 step 600 loss 0.11253
INFO:name:epoch 1 step 700 loss 0.11041
INFO:name:epoch 1 step 800 loss 0.11971
INFO:name:epoch 1 step 900 loss 0.11304
INFO:name:epoch 1 step 1000 loss 0.11901
INFO:name:epoch 1 step 1100 loss 0.11696
INFO:name:epoch 1 step 1200 loss 0.12412
INFO:name:epoch 1 step 1300 loss 0.11165
INFO:name:epoch 1 step 1400 loss 0.09317
INFO:name:epoch 1 step 1500 loss 0.11411
INFO:name:epoch 1 step 1600 loss 0.10166
INFO:name:epoch 1 step 1700 loss 0.11255
INFO:name:epoch 1 step 1800 loss 0.10567
INFO:name:epoch 1 step 1900 loss 0.109
INFO:name:epoch 1 step 2000 loss 0.10164
INFO:name:epoch 1 step 2100 loss 0.11847
INFO:name:epoch 1 step 2200 loss 0.10774
INFO:name:epoch 1 step 2300 loss 0.10338
INFO:name:epoch 1 step 2400 loss 0.11462
INFO:name:epoch 1 step 2500 loss 0.09867
INFO:name:epoch 1 step 2600 loss 0.10011
INFO:name:epoch 1 step 2700 loss 0.08567
INFO:name:epoch 1 step 2800 loss 0.10591
INFO:name:epoch 1 step 2900 loss 0.09363
INFO:name:epoch 1 step 3000 loss 0.1111
INFO:name:epoch 1 step 3100 loss 0.10826
INFO:name:epoch 1 step 3200 loss 0.10043
INFO:name:epoch 1 step 3300 loss 0.10695
INFO:name:epoch 1 step 3400 loss 0.09957
INFO:name:epoch 1 step 3500 loss 0.10315
INFO:name:epoch 1 step 3600 loss 0.09698
INFO:name:epoch 1 step 3700 loss 0.10169
INFO:name:epoch 1 step 3800 loss 0.09312
INFO:name:epoch 1 step 3900 loss 0.12153
INFO:name:epoch 1 step 4000 loss 0.10038
INFO:name:epoch 1 step 4100 loss 0.09667
INFO:name:epoch 1 step 4200 loss 0.10002
INFO:name:epoch 1 step 4300 loss 0.09839
INFO:name:epoch 1 step 4400 loss 0.0875
INFO:name:epoch 1 step 4500 loss 0.09348
INFO:name:epoch 1 step 4600 loss 0.11501
INFO:name:epoch 1 step 4700 loss 0.09722
INFO:name:epoch 1 step 4800 loss 0.10872
INFO:name:epoch 1 step 4900 loss 0.1115
INFO:name:epoch 1 step 5000 loss 0.09248
INFO:name:epoch 1 step 5100 loss 0.08858
INFO:name:epoch 1 step 5200 loss 0.11408
INFO:name:epoch 1 step 5300 loss 0.0941
INFO:name:epoch 1 step 5400 loss 0.11484
INFO:name:epoch 1 step 5500 loss 0.09284
INFO:name:epoch 1 step 5600 loss 0.12478
INFO:name:epoch 1 step 5700 loss 0.10059
INFO:name:epoch 1 step 5800 loss 0.10229
INFO:name:epoch 1 step 5900 loss 0.10009
INFO:name:epoch 1 step 6000 loss 0.08465
INFO:name:epoch 1 step 6100 loss 0.1009
INFO:name:epoch 1 step 6200 loss 0.10047
INFO:name:epoch 1 step 6300 loss 0.09147
INFO:name:epoch 1 step 6400 loss 0.10454
INFO:name:epoch 1 step 6500 loss 0.10837
INFO:name:epoch 1 step 6600 loss 0.10992
INFO:name:epoch 1 step 6700 loss 0.08331
INFO:name:epoch 1 step 6800 loss 0.09539
INFO:name:epoch 1 step 6900 loss 0.09321
INFO:name:epoch 1 step 7000 loss 0.0956
INFO:name:epoch 1 step 7100 loss 0.09804
INFO:name:epoch 1 step 7200 loss 0.09178
INFO:name:epoch 1 step 7300 loss 0.10106
INFO:name:epoch 1 step 7400 loss 0.09456
INFO:name:epoch 1 step 7500 loss 0.09777
INFO:name:epoch 1 step 7600 loss 0.07796
INFO:name:epoch 1 step 7700 loss 0.0948
INFO:name:epoch 1 step 7800 loss 0.10848
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3114
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3114
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2522
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 2 step 100 loss 0.09036
INFO:name:epoch 2 step 200 loss 0.08646
INFO:name:epoch 2 step 300 loss 0.0955
INFO:name:epoch 2 step 400 loss 0.09112
INFO:name:epoch 2 step 500 loss 0.09838
INFO:name:epoch 2 step 600 loss 0.09246
INFO:name:epoch 2 step 700 loss 0.0871
INFO:name:epoch 2 step 800 loss 0.08522
INFO:name:epoch 2 step 900 loss 0.08773
INFO:name:epoch 2 step 1000 loss 0.08912
INFO:name:epoch 2 step 1100 loss 0.08686
INFO:name:epoch 2 step 1200 loss 0.08574
INFO:name:epoch 2 step 1300 loss 0.10107
INFO:name:epoch 2 step 1400 loss 0.09516
INFO:name:epoch 2 step 1500 loss 0.09406
INFO:name:epoch 2 step 1600 loss 0.08556
INFO:name:epoch 2 step 1700 loss 0.0823
INFO:name:epoch 2 step 1800 loss 0.08183
INFO:name:epoch 2 step 1900 loss 0.08885
INFO:name:epoch 2 step 2000 loss 0.08813
INFO:name:epoch 2 step 2100 loss 0.07944
INFO:name:epoch 2 step 2200 loss 0.0906
INFO:name:epoch 2 step 2300 loss 0.08733
INFO:name:epoch 2 step 2400 loss 0.10126
INFO:name:epoch 2 step 2500 loss 0.09511
INFO:name:epoch 2 step 2600 loss 0.08586
INFO:name:epoch 2 step 2700 loss 0.09333
INFO:name:epoch 2 step 2800 loss 0.09133
INFO:name:epoch 2 step 2900 loss 0.08494
INFO:name:epoch 2 step 3000 loss 0.08028
INFO:name:epoch 2 step 3100 loss 0.07358
INFO:name:epoch 2 step 3200 loss 0.09328
INFO:name:epoch 2 step 3300 loss 0.0861
INFO:name:epoch 2 step 3400 loss 0.09645
INFO:name:epoch 2 step 3500 loss 0.08378
INFO:name:epoch 2 step 3600 loss 0.08882
INFO:name:epoch 2 step 3700 loss 0.08035
INFO:name:epoch 2 step 3800 loss 0.0879
INFO:name:epoch 2 step 3900 loss 0.07299
INFO:name:epoch 2 step 4000 loss 0.08697
INFO:name:epoch 2 step 4100 loss 0.09218
INFO:name:epoch 2 step 4200 loss 0.08029
INFO:name:epoch 2 step 4300 loss 0.09466
INFO:name:epoch 2 step 4400 loss 0.08338
INFO:name:epoch 2 step 4500 loss 0.09609
INFO:name:epoch 2 step 4600 loss 0.08587
INFO:name:epoch 2 step 4700 loss 0.09164
INFO:name:epoch 2 step 4800 loss 0.08331
INFO:name:epoch 2 step 4900 loss 0.08484
INFO:name:epoch 2 step 5000 loss 0.08036
INFO:name:epoch 2 step 5100 loss 0.07655
INFO:name:epoch 2 step 5200 loss 0.09536
INFO:name:epoch 2 step 5300 loss 0.10625
INFO:name:epoch 2 step 5400 loss 0.08477
INFO:name:epoch 2 step 5500 loss 0.08078
INFO:name:epoch 2 step 5600 loss 0.08711
INFO:name:epoch 2 step 5700 loss 0.0945
INFO:name:epoch 2 step 5800 loss 0.09029
INFO:name:epoch 2 step 5900 loss 0.07867
INFO:name:epoch 2 step 6000 loss 0.08442
INFO:name:epoch 2 step 6100 loss 0.08671
INFO:name:epoch 2 step 6200 loss 0.07772
INFO:name:epoch 2 step 6300 loss 0.08186
INFO:name:epoch 2 step 6400 loss 0.09676
INFO:name:epoch 2 step 6500 loss 0.08339
INFO:name:epoch 2 step 6600 loss 0.08554
INFO:name:epoch 2 step 6700 loss 0.09691
INFO:name:epoch 2 step 6800 loss 0.07068
INFO:name:epoch 2 step 6900 loss 0.08331
INFO:name:epoch 2 step 7000 loss 0.0937
INFO:name:epoch 2 step 7100 loss 0.07696
INFO:name:epoch 2 step 7200 loss 0.09072
INFO:name:epoch 2 step 7300 loss 0.0894
INFO:name:epoch 2 step 7400 loss 0.08551
INFO:name:epoch 2 step 7500 loss 0.07193
INFO:name:epoch 2 step 7600 loss 0.07606
INFO:name:epoch 2 step 7700 loss 0.08058
INFO:name:epoch 2 step 7800 loss 0.0958
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3137
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3137
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2528
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.07534
INFO:name:epoch 3 step 200 loss 0.08082
INFO:name:epoch 3 step 300 loss 0.07804
INFO:name:epoch 3 step 400 loss 0.08193
INFO:name:epoch 3 step 500 loss 0.07883
INFO:name:epoch 3 step 600 loss 0.06584
INFO:name:epoch 3 step 700 loss 0.08274
INFO:name:epoch 3 step 800 loss 0.07615
INFO:name:epoch 3 step 900 loss 0.07358
INFO:name:epoch 3 step 1000 loss 0.07722
INFO:name:epoch 3 step 1100 loss 0.10122
INFO:name:epoch 3 step 1200 loss 0.06709
INFO:name:epoch 3 step 1300 loss 0.08615
INFO:name:epoch 3 step 1400 loss 0.08141
INFO:name:epoch 3 step 1500 loss 0.07596
INFO:name:epoch 3 step 1600 loss 0.07666
INFO:name:epoch 3 step 1700 loss 0.088
INFO:name:epoch 3 step 1800 loss 0.07535
INFO:name:epoch 3 step 1900 loss 0.06355
INFO:name:epoch 3 step 2000 loss 0.08824
INFO:name:epoch 3 step 2100 loss 0.08434
INFO:name:epoch 3 step 2200 loss 0.08456
INFO:name:epoch 3 step 2300 loss 0.08007
INFO:name:epoch 3 step 2400 loss 0.07994
INFO:name:epoch 3 step 2500 loss 0.08114
INFO:name:epoch 3 step 2600 loss 0.07887
INFO:name:epoch 3 step 2700 loss 0.06931
INFO:name:epoch 3 step 2800 loss 0.0652
INFO:name:epoch 3 step 2900 loss 0.07041
INFO:name:epoch 3 step 3000 loss 0.08689
INFO:name:epoch 3 step 3100 loss 0.08351
INFO:name:epoch 3 step 3200 loss 0.08144
INFO:name:epoch 3 step 3300 loss 0.07551
INFO:name:epoch 3 step 3400 loss 0.08944
INFO:name:epoch 3 step 3500 loss 0.07774
INFO:name:epoch 3 step 3600 loss 0.06902
INFO:name:epoch 3 step 3700 loss 0.06607
INFO:name:epoch 3 step 3800 loss 0.07739
INFO:name:epoch 3 step 3900 loss 0.08262
INFO:name:epoch 3 step 4000 loss 0.07205
INFO:name:epoch 3 step 4100 loss 0.0689
INFO:name:epoch 3 step 4200 loss 0.07015
INFO:name:epoch 3 step 4300 loss 0.07936
INFO:name:epoch 3 step 4400 loss 0.07518
INFO:name:epoch 3 step 4500 loss 0.07237
INFO:name:epoch 3 step 4600 loss 0.08694
INFO:name:epoch 3 step 4700 loss 0.07676
INFO:name:epoch 3 step 4800 loss 0.06964
INFO:name:epoch 3 step 4900 loss 0.07089
INFO:name:epoch 3 step 5000 loss 0.08371
INFO:name:epoch 3 step 5100 loss 0.07876
INFO:name:epoch 3 step 5200 loss 0.06554
INFO:name:epoch 3 step 5300 loss 0.07721
INFO:name:epoch 3 step 5400 loss 0.06881
INFO:name:epoch 3 step 5500 loss 0.07506
INFO:name:epoch 3 step 5600 loss 0.07613
INFO:name:epoch 3 step 5700 loss 0.07434
INFO:name:epoch 3 step 5800 loss 0.07664
INFO:name:epoch 3 step 5900 loss 0.06599
INFO:name:epoch 3 step 6000 loss 0.07359
INFO:name:epoch 3 step 6100 loss 0.0699
INFO:name:epoch 3 step 6200 loss 0.0815
INFO:name:epoch 3 step 6300 loss 0.08031
INFO:name:epoch 3 step 6400 loss 0.07746
INFO:name:epoch 3 step 6500 loss 0.06476
INFO:name:epoch 3 step 6600 loss 0.0735
INFO:name:epoch 3 step 6700 loss 0.07687
INFO:name:epoch 3 step 6800 loss 0.08375
INFO:name:epoch 3 step 6900 loss 0.07934
INFO:name:epoch 3 step 7000 loss 0.08381
INFO:name:epoch 3 step 7100 loss 0.06302
INFO:name:epoch 3 step 7200 loss 0.08161
INFO:name:epoch 3 step 7300 loss 0.08195
INFO:name:epoch 3 step 7400 loss 0.07609
INFO:name:epoch 3 step 7500 loss 0.07974
INFO:name:epoch 3 step 7600 loss 0.07611
INFO:name:epoch 3 step 7700 loss 0.09453
INFO:name:epoch 3 step 7800 loss 0.06959
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3338
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3338
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2722
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 4 step 100 loss 0.07355
INFO:name:epoch 4 step 200 loss 0.06604
INFO:name:epoch 4 step 300 loss 0.06574
INFO:name:epoch 4 step 400 loss 0.06799
INFO:name:epoch 4 step 500 loss 0.06978
INFO:name:epoch 4 step 600 loss 0.06144
INFO:name:epoch 4 step 700 loss 0.06994
INFO:name:epoch 4 step 800 loss 0.07797
INFO:name:epoch 4 step 900 loss 0.07258
INFO:name:epoch 4 step 1000 loss 0.06748
INFO:name:epoch 4 step 1100 loss 0.06727
INFO:name:epoch 4 step 1200 loss 0.06054
INFO:name:epoch 4 step 1300 loss 0.06661
INFO:name:epoch 4 step 1400 loss 0.07108
INFO:name:epoch 4 step 1500 loss 0.06475
INFO:name:epoch 4 step 1600 loss 0.06323
INFO:name:epoch 4 step 1700 loss 0.0719
INFO:name:epoch 4 step 1800 loss 0.07464
INFO:name:epoch 4 step 1900 loss 0.0679
INFO:name:epoch 4 step 2000 loss 0.06988
INFO:name:epoch 4 step 2100 loss 0.07222
INFO:name:epoch 4 step 2200 loss 0.08183
INFO:name:epoch 4 step 2300 loss 0.06785
INFO:name:epoch 4 step 2400 loss 0.06795
INFO:name:epoch 4 step 2500 loss 0.06333
INFO:name:epoch 4 step 2600 loss 0.06854
INFO:name:epoch 4 step 2700 loss 0.07411
INFO:name:epoch 4 step 2800 loss 0.06984
INFO:name:epoch 4 step 2900 loss 0.06748
INFO:name:epoch 4 step 3000 loss 0.07173
INFO:name:epoch 4 step 3100 loss 0.0665
INFO:name:epoch 4 step 3200 loss 0.08318
INFO:name:epoch 4 step 3300 loss 0.0609
INFO:name:epoch 4 step 3400 loss 0.0657
INFO:name:epoch 4 step 3500 loss 0.06396
INFO:name:epoch 4 step 3600 loss 0.07595
INFO:name:epoch 4 step 3700 loss 0.0799
INFO:name:epoch 4 step 3800 loss 0.07317
INFO:name:epoch 4 step 3900 loss 0.07441
INFO:name:epoch 4 step 4000 loss 0.06432
INFO:name:epoch 4 step 4100 loss 0.07108
INFO:name:epoch 4 step 4200 loss 0.06701
INFO:name:epoch 4 step 4300 loss 0.07704
INFO:name:epoch 4 step 4400 loss 0.07247
INFO:name:epoch 4 step 4500 loss 0.07321
INFO:name:epoch 4 step 4600 loss 0.07949
INFO:name:epoch 4 step 4700 loss 0.07176
INFO:name:epoch 4 step 4800 loss 0.06655
INFO:name:epoch 4 step 4900 loss 0.06635
INFO:name:epoch 4 step 5000 loss 0.0842
INFO:name:epoch 4 step 5100 loss 0.07789
INFO:name:epoch 4 step 5200 loss 0.07118
INFO:name:epoch 4 step 5300 loss 0.06865
INFO:name:epoch 4 step 5400 loss 0.07525
INFO:name:epoch 4 step 5500 loss 0.06247
INFO:name:epoch 4 step 5600 loss 0.0682
INFO:name:epoch 4 step 5700 loss 0.07129
INFO:name:epoch 4 step 5800 loss 0.07793
INFO:name:epoch 4 step 5900 loss 0.07815
INFO:name:epoch 4 step 6000 loss 0.0726
INFO:name:epoch 4 step 6100 loss 0.0655
INFO:name:epoch 4 step 6200 loss 0.07948
INFO:name:epoch 4 step 6300 loss 0.06422
INFO:name:epoch 4 step 6400 loss 0.0819
INFO:name:epoch 4 step 6500 loss 0.06119
INFO:name:epoch 4 step 6600 loss 0.0655
INFO:name:epoch 4 step 6700 loss 0.06549
INFO:name:epoch 4 step 6800 loss 0.0666
INFO:name:epoch 4 step 6900 loss 0.0741
INFO:name:epoch 4 step 7000 loss 0.07173
INFO:name:epoch 4 step 7100 loss 0.07478
INFO:name:epoch 4 step 7200 loss 0.06139
INFO:name:epoch 4 step 7300 loss 0.06969
INFO:name:epoch 4 step 7400 loss 0.07195
INFO:name:epoch 4 step 7500 loss 0.05555
INFO:name:epoch 4 step 7600 loss 0.08225
INFO:name:epoch 4 step 7700 loss 0.06375
INFO:name:epoch 4 step 7800 loss 0.07
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3269
INFO:name:epoch 5 step 100 loss 0.07
INFO:name:epoch 5 step 200 loss 0.06987
INFO:name:epoch 5 step 300 loss 0.07131
INFO:name:epoch 5 step 400 loss 0.07286
INFO:name:epoch 5 step 500 loss 0.06593
INFO:name:epoch 5 step 600 loss 0.06658
INFO:name:epoch 5 step 700 loss 0.06013
INFO:name:epoch 5 step 800 loss 0.07096
INFO:name:epoch 5 step 900 loss 0.05974
INFO:name:epoch 5 step 1000 loss 0.06753
INFO:name:epoch 5 step 1100 loss 0.0671
INFO:name:epoch 5 step 1200 loss 0.06591
INFO:name:epoch 5 step 1300 loss 0.07162
INFO:name:epoch 5 step 1400 loss 0.07059
INFO:name:epoch 5 step 1500 loss 0.0679
INFO:name:epoch 5 step 1600 loss 0.0658
INFO:name:epoch 5 step 1700 loss 0.06797
INFO:name:epoch 5 step 1800 loss 0.06657
INFO:name:epoch 5 step 1900 loss 0.07575
INFO:name:epoch 5 step 2000 loss 0.06355
INFO:name:epoch 5 step 2100 loss 0.0635
INFO:name:epoch 5 step 2200 loss 0.06813
INFO:name:epoch 5 step 2300 loss 0.07009
INFO:name:epoch 5 step 2400 loss 0.06886
INFO:name:epoch 5 step 2500 loss 0.06459
INFO:name:epoch 5 step 2600 loss 0.06282
INFO:name:epoch 5 step 2700 loss 0.06698
INFO:name:epoch 5 step 2800 loss 0.06738
INFO:name:epoch 5 step 2900 loss 0.06347
INFO:name:epoch 5 step 3000 loss 0.05304
INFO:name:epoch 5 step 3100 loss 0.06781
INFO:name:epoch 5 step 3200 loss 0.06571
INFO:name:epoch 5 step 3300 loss 0.05756
INFO:name:epoch 5 step 3400 loss 0.06363
INFO:name:epoch 5 step 3500 loss 0.06424
INFO:name:epoch 5 step 3600 loss 0.0692
INFO:name:epoch 5 step 3700 loss 0.06571
INFO:name:epoch 5 step 3800 loss 0.0628
INFO:name:epoch 5 step 3900 loss 0.06059
INFO:name:epoch 5 step 4000 loss 0.07189
INFO:name:epoch 5 step 4100 loss 0.06508
INFO:name:epoch 5 step 4200 loss 0.06465
INFO:name:epoch 5 step 4300 loss 0.0605
INFO:name:epoch 5 step 4400 loss 0.06174
INFO:name:epoch 5 step 4500 loss 0.06877
INFO:name:epoch 5 step 4600 loss 0.06523
INFO:name:epoch 5 step 4700 loss 0.05709
INFO:name:epoch 5 step 4800 loss 0.06016
INFO:name:epoch 5 step 4900 loss 0.07446
INFO:name:epoch 5 step 5000 loss 0.06897
INFO:name:epoch 5 step 5100 loss 0.0614
INFO:name:epoch 5 step 5200 loss 0.06018
INFO:name:epoch 5 step 5300 loss 0.06084
INFO:name:epoch 5 step 5400 loss 0.05421
INFO:name:epoch 5 step 5500 loss 0.06478
INFO:name:epoch 5 step 5600 loss 0.06757
INFO:name:epoch 5 step 5700 loss 0.0695
INFO:name:epoch 5 step 5800 loss 0.06403
INFO:name:epoch 5 step 5900 loss 0.06641
INFO:name:epoch 5 step 6000 loss 0.05286
INFO:name:epoch 5 step 6100 loss 0.06718
INFO:name:epoch 5 step 6200 loss 0.06199
INFO:name:epoch 5 step 6300 loss 0.05444
INFO:name:epoch 5 step 6400 loss 0.06177
INFO:name:epoch 5 step 6500 loss 0.06608
INFO:name:epoch 5 step 6600 loss 0.07328
INFO:name:epoch 5 step 6700 loss 0.06352
INFO:name:epoch 5 step 6800 loss 0.06294
INFO:name:epoch 5 step 6900 loss 0.07387
INFO:name:epoch 5 step 7000 loss 0.06293
INFO:name:epoch 5 step 7100 loss 0.06954
INFO:name:epoch 5 step 7200 loss 0.06563
INFO:name:epoch 5 step 7300 loss 0.06618
INFO:name:epoch 5 step 7400 loss 0.06296
INFO:name:epoch 5 step 7500 loss 0.06568
INFO:name:epoch 5 step 7600 loss 0.06684
INFO:name:epoch 5 step 7700 loss 0.06798
INFO:name:epoch 5 step 7800 loss 0.07075
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3422
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3422
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2821
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 6 step 100 loss 0.05956
INFO:name:epoch 6 step 200 loss 0.05843
INFO:name:epoch 6 step 300 loss 0.05841
INFO:name:epoch 6 step 400 loss 0.05839
INFO:name:epoch 6 step 500 loss 0.05924
INFO:name:epoch 6 step 600 loss 0.05302
INFO:name:epoch 6 step 700 loss 0.05772
INFO:name:epoch 6 step 800 loss 0.06186
INFO:name:epoch 6 step 900 loss 0.05806
INFO:name:epoch 6 step 1000 loss 0.07532
INFO:name:epoch 6 step 1100 loss 0.0695
INFO:name:epoch 6 step 1200 loss 0.07588
INFO:name:epoch 6 step 1300 loss 0.07912
INFO:name:epoch 6 step 1400 loss 0.06441
INFO:name:epoch 6 step 1500 loss 0.06309
INFO:name:epoch 6 step 1600 loss 0.05714
INFO:name:epoch 6 step 1700 loss 0.0627
INFO:name:epoch 6 step 1800 loss 0.07049
INFO:name:epoch 6 step 1900 loss 0.06779
INFO:name:epoch 6 step 2000 loss 0.05848
INFO:name:epoch 6 step 2100 loss 0.06665
INFO:name:epoch 6 step 2200 loss 0.06566
INFO:name:epoch 6 step 2300 loss 0.05003
INFO:name:epoch 6 step 2400 loss 0.05514
INFO:name:epoch 6 step 2500 loss 0.05963
INFO:name:epoch 6 step 2600 loss 0.07335
INFO:name:epoch 6 step 2700 loss 0.0658
INFO:name:epoch 6 step 2800 loss 0.05873
INFO:name:epoch 6 step 2900 loss 0.06553
INFO:name:epoch 6 step 3000 loss 0.05265
INFO:name:epoch 6 step 3100 loss 0.06829
INFO:name:epoch 6 step 3200 loss 0.05398
INFO:name:epoch 6 step 3300 loss 0.07363
INFO:name:epoch 6 step 3400 loss 0.06089
INFO:name:epoch 6 step 3500 loss 0.05686
INFO:name:epoch 6 step 3600 loss 0.05488
INFO:name:epoch 6 step 3700 loss 0.06486
INFO:name:epoch 6 step 3800 loss 0.07446
INFO:name:epoch 6 step 3900 loss 0.06128
INFO:name:epoch 6 step 4000 loss 0.06812
INFO:name:epoch 6 step 4100 loss 0.05789
INFO:name:epoch 6 step 4200 loss 0.05755
INFO:name:epoch 6 step 4300 loss 0.05787
INFO:name:epoch 6 step 4400 loss 0.06014
INFO:name:epoch 6 step 4500 loss 0.06233
INFO:name:epoch 6 step 4600 loss 0.06104
INFO:name:epoch 6 step 4700 loss 0.06637
INFO:name:epoch 6 step 4800 loss 0.06184
INFO:name:epoch 6 step 4900 loss 0.05732
INFO:name:epoch 6 step 5000 loss 0.06238
INFO:name:epoch 6 step 5100 loss 0.05698
INFO:name:epoch 6 step 5200 loss 0.05704
INFO:name:epoch 6 step 5300 loss 0.04753
INFO:name:epoch 6 step 5400 loss 0.06191
INFO:name:epoch 6 step 5500 loss 0.06042
INFO:name:epoch 6 step 5600 loss 0.05971
INFO:name:epoch 6 step 5700 loss 0.05729
INFO:name:epoch 6 step 5800 loss 0.05972
INFO:name:epoch 6 step 5900 loss 0.05086
INFO:name:epoch 6 step 6000 loss 0.06017
INFO:name:epoch 6 step 6100 loss 0.06487
INFO:name:epoch 6 step 6200 loss 0.07161
INFO:name:epoch 6 step 6300 loss 0.06382
INFO:name:epoch 6 step 6400 loss 0.05997
INFO:name:epoch 6 step 6500 loss 0.05839
INFO:name:epoch 6 step 6600 loss 0.05735
INFO:name:epoch 6 step 6700 loss 0.0678
INFO:name:epoch 6 step 6800 loss 0.05947
INFO:name:epoch 6 step 6900 loss 0.06478
INFO:name:epoch 6 step 7000 loss 0.05889
INFO:name:epoch 6 step 7100 loss 0.06067
INFO:name:epoch 6 step 7200 loss 0.05128
INFO:name:epoch 6 step 7300 loss 0.06182
INFO:name:epoch 6 step 7400 loss 0.0665
INFO:name:epoch 6 step 7500 loss 0.06533
INFO:name:epoch 6 step 7600 loss 0.06367
INFO:name:epoch 6 step 7700 loss 0.0566
INFO:name:epoch 6 step 7800 loss 0.05468
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3432
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3432
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2823
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 7 step 100 loss 0.05854
INFO:name:epoch 7 step 200 loss 0.06348
INFO:name:epoch 7 step 300 loss 0.05482
INFO:name:epoch 7 step 400 loss 0.06292
INFO:name:epoch 7 step 500 loss 0.06999
INFO:name:epoch 7 step 600 loss 0.054
INFO:name:epoch 7 step 700 loss 0.04813
INFO:name:epoch 7 step 800 loss 0.06203
INFO:name:epoch 7 step 900 loss 0.06592
INFO:name:epoch 7 step 1000 loss 0.0571
INFO:name:epoch 7 step 1100 loss 0.05156
INFO:name:epoch 7 step 1200 loss 0.05318
INFO:name:epoch 7 step 1300 loss 0.05173
INFO:name:epoch 7 step 1400 loss 0.0645
INFO:name:epoch 7 step 1500 loss 0.04608
INFO:name:epoch 7 step 1600 loss 0.05339
INFO:name:epoch 7 step 1700 loss 0.04854
INFO:name:epoch 7 step 1800 loss 0.05813
INFO:name:epoch 7 step 1900 loss 0.05266
INFO:name:epoch 7 step 2000 loss 0.06416
INFO:name:epoch 7 step 2100 loss 0.06742
INFO:name:epoch 7 step 2200 loss 0.06028
INFO:name:epoch 7 step 2300 loss 0.04952
INFO:name:epoch 7 step 2400 loss 0.06302
INFO:name:epoch 7 step 2500 loss 0.06032
INFO:name:epoch 7 step 2600 loss 0.05813
INFO:name:epoch 7 step 2700 loss 0.0599
INFO:name:epoch 7 step 2800 loss 0.06231
INFO:name:epoch 7 step 2900 loss 0.06411
INFO:name:epoch 7 step 3000 loss 0.07031
INFO:name:epoch 7 step 3100 loss 0.06155
INFO:name:epoch 7 step 3200 loss 0.06361
INFO:name:epoch 7 step 3300 loss 0.06204
INFO:name:epoch 7 step 3400 loss 0.06423
INFO:name:epoch 7 step 3500 loss 0.05514
INFO:name:epoch 7 step 3600 loss 0.05568
INFO:name:epoch 7 step 3700 loss 0.0571
INFO:name:epoch 7 step 3800 loss 0.05679
INFO:name:epoch 7 step 3900 loss 0.06624
INFO:name:epoch 7 step 4000 loss 0.05019
INFO:name:epoch 7 step 4100 loss 0.06273
INFO:name:epoch 7 step 4200 loss 0.06645
INFO:name:epoch 7 step 4300 loss 0.06383
INFO:name:epoch 7 step 4400 loss 0.0649
INFO:name:epoch 7 step 4500 loss 0.06088
INFO:name:epoch 7 step 4600 loss 0.05406
INFO:name:epoch 7 step 4700 loss 0.05705
INFO:name:epoch 7 step 4800 loss 0.05863
INFO:name:epoch 7 step 4900 loss 0.05804
INFO:name:epoch 7 step 5000 loss 0.05504
INFO:name:epoch 7 step 5100 loss 0.06529
INFO:name:epoch 7 step 5200 loss 0.06095
INFO:name:epoch 7 step 5300 loss 0.05689
INFO:name:epoch 7 step 5400 loss 0.05716
INFO:name:epoch 7 step 5500 loss 0.0502
INFO:name:epoch 7 step 5600 loss 0.05262
INFO:name:epoch 7 step 5700 loss 0.05365
INFO:name:epoch 7 step 5800 loss 0.06435
INFO:name:epoch 7 step 5900 loss 0.04881
INFO:name:epoch 7 step 6000 loss 0.0646
INFO:name:epoch 7 step 6100 loss 0.05283
INFO:name:epoch 7 step 6200 loss 0.05388
INFO:name:epoch 7 step 6300 loss 0.05716
INFO:name:epoch 7 step 6400 loss 0.06431
INFO:name:epoch 7 step 6500 loss 0.05941
INFO:name:epoch 7 step 6600 loss 0.05535
INFO:name:epoch 7 step 6700 loss 0.0768
INFO:name:epoch 7 step 6800 loss 0.07191
INFO:name:epoch 7 step 6900 loss 0.06424
INFO:name:epoch 7 step 7000 loss 0.05708
INFO:name:epoch 7 step 7100 loss 0.05808
INFO:name:epoch 7 step 7200 loss 0.06095
INFO:name:epoch 7 step 7300 loss 0.05138
INFO:name:epoch 7 step 7400 loss 0.05497
INFO:name:epoch 7 step 7500 loss 0.0582
INFO:name:epoch 7 step 7600 loss 0.05935
INFO:name:epoch 7 step 7700 loss 0.05148
INFO:name:epoch 7 step 7800 loss 0.05831
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3452
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3452
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2829
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 8 step 100 loss 0.05365
INFO:name:epoch 8 step 200 loss 0.06528
INFO:name:epoch 8 step 300 loss 0.05366
INFO:name:epoch 8 step 400 loss 0.0607
INFO:name:epoch 8 step 500 loss 0.05922
INFO:name:epoch 8 step 600 loss 0.05777
INFO:name:epoch 8 step 700 loss 0.05696
INFO:name:epoch 8 step 800 loss 0.0525
INFO:name:epoch 8 step 900 loss 0.05742
INFO:name:epoch 8 step 1000 loss 0.04678
INFO:name:epoch 8 step 1100 loss 0.05674
INFO:name:epoch 8 step 1200 loss 0.05784
INFO:name:epoch 8 step 1300 loss 0.05857
INFO:name:epoch 8 step 1400 loss 0.05458
INFO:name:epoch 8 step 1500 loss 0.05461
INFO:name:epoch 8 step 1600 loss 0.05839
INFO:name:epoch 8 step 1700 loss 0.05682
INFO:name:epoch 8 step 1800 loss 0.05448
INFO:name:epoch 8 step 1900 loss 0.05768
INFO:name:epoch 8 step 2000 loss 0.05168
INFO:name:epoch 8 step 2100 loss 0.0516
INFO:name:epoch 8 step 2200 loss 0.06116
INFO:name:epoch 8 step 2300 loss 0.04144
INFO:name:epoch 8 step 2400 loss 0.0577
INFO:name:epoch 8 step 2500 loss 0.05431
INFO:name:epoch 8 step 2600 loss 0.06032
INFO:name:epoch 8 step 2700 loss 0.05708
INFO:name:epoch 8 step 2800 loss 0.05294
INFO:name:epoch 8 step 2900 loss 0.05955
INFO:name:epoch 8 step 3000 loss 0.06241
INFO:name:epoch 8 step 3100 loss 0.05199
INFO:name:epoch 8 step 3200 loss 0.05902
INFO:name:epoch 8 step 3300 loss 0.05492
INFO:name:epoch 8 step 3400 loss 0.05322
INFO:name:epoch 8 step 3500 loss 0.05161
INFO:name:epoch 8 step 3600 loss 0.06451
INFO:name:epoch 8 step 3700 loss 0.05859
INFO:name:epoch 8 step 3800 loss 0.05783
INFO:name:epoch 8 step 3900 loss 0.04489
INFO:name:epoch 8 step 4000 loss 0.0508
INFO:name:epoch 8 step 4100 loss 0.0579
INFO:name:epoch 8 step 4200 loss 0.05643
INFO:name:epoch 8 step 4300 loss 0.05659
INFO:name:epoch 8 step 4400 loss 0.05566
INFO:name:epoch 8 step 4500 loss 0.06085
INFO:name:epoch 8 step 4600 loss 0.05258
INFO:name:epoch 8 step 4700 loss 0.06423
INFO:name:epoch 8 step 4800 loss 0.05504
INFO:name:epoch 8 step 4900 loss 0.05055
INFO:name:epoch 8 step 5000 loss 0.06419
INFO:name:epoch 8 step 5100 loss 0.06558
INFO:name:epoch 8 step 5200 loss 0.05653
INFO:name:epoch 8 step 5300 loss 0.06041
INFO:name:epoch 8 step 5400 loss 0.06369
INFO:name:epoch 8 step 5500 loss 0.06145
INFO:name:epoch 8 step 5600 loss 0.05305
INFO:name:epoch 8 step 5700 loss 0.06317
INFO:name:epoch 8 step 5800 loss 0.0595
INFO:name:epoch 8 step 5900 loss 0.06174
INFO:name:epoch 8 step 6000 loss 0.05502
INFO:name:epoch 8 step 6100 loss 0.05242
INFO:name:epoch 8 step 6200 loss 0.04729
INFO:name:epoch 8 step 6300 loss 0.05203
INFO:name:epoch 8 step 6400 loss 0.05652
INFO:name:epoch 8 step 6500 loss 0.06267
INFO:name:epoch 8 step 6600 loss 0.05212
INFO:name:epoch 8 step 6700 loss 0.04823
INFO:name:epoch 8 step 6800 loss 0.04569
INFO:name:epoch 8 step 6900 loss 0.05169
INFO:name:epoch 8 step 7000 loss 0.05819
INFO:name:epoch 8 step 7100 loss 0.05298
INFO:name:epoch 8 step 7200 loss 0.04477
INFO:name:epoch 8 step 7300 loss 0.05566
INFO:name:epoch 8 step 7400 loss 0.05575
INFO:name:epoch 8 step 7500 loss 0.05162
INFO:name:epoch 8 step 7600 loss 0.06197
INFO:name:epoch 8 step 7700 loss 0.06041
INFO:name:epoch 8 step 7800 loss 0.04783
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3417
INFO:name:epoch 9 step 100 loss 0.0574
INFO:name:epoch 9 step 200 loss 0.04724
INFO:name:epoch 9 step 300 loss 0.05644
INFO:name:epoch 9 step 400 loss 0.07013
INFO:name:epoch 9 step 500 loss 0.05765
INFO:name:epoch 9 step 600 loss 0.05401
INFO:name:epoch 9 step 700 loss 0.05157
INFO:name:epoch 9 step 800 loss 0.05149
INFO:name:epoch 9 step 900 loss 0.0579
INFO:name:epoch 9 step 1000 loss 0.05926
INFO:name:epoch 9 step 1100 loss 0.04454
INFO:name:epoch 9 step 1200 loss 0.05797
INFO:name:epoch 9 step 1300 loss 0.06876
INFO:name:epoch 9 step 1400 loss 0.05366
INFO:name:epoch 9 step 1500 loss 0.0576
INFO:name:epoch 9 step 1600 loss 0.05001
INFO:name:epoch 9 step 1700 loss 0.06743
INFO:name:epoch 9 step 1800 loss 0.05887
INFO:name:epoch 9 step 1900 loss 0.06408
INFO:name:epoch 9 step 2000 loss 0.04589
INFO:name:epoch 9 step 2100 loss 0.05791
INFO:name:epoch 9 step 2200 loss 0.05556
INFO:name:epoch 9 step 2300 loss 0.06308
INFO:name:epoch 9 step 2400 loss 0.05874
INFO:name:epoch 9 step 2500 loss 0.05916
INFO:name:epoch 9 step 2600 loss 0.05311
INFO:name:epoch 9 step 2700 loss 0.0521
INFO:name:epoch 9 step 2800 loss 0.04972
INFO:name:epoch 9 step 2900 loss 0.05814
INFO:name:epoch 9 step 3000 loss 0.05579
INFO:name:epoch 9 step 3100 loss 0.05533
INFO:name:epoch 9 step 3200 loss 0.05284
INFO:name:epoch 9 step 3300 loss 0.05174
INFO:name:epoch 9 step 3400 loss 0.06084
INFO:name:epoch 9 step 3500 loss 0.04908
INFO:name:epoch 9 step 3600 loss 0.05763
INFO:name:epoch 9 step 3700 loss 0.05389
INFO:name:epoch 9 step 3800 loss 0.0517
INFO:name:epoch 9 step 3900 loss 0.04715
INFO:name:epoch 9 step 4000 loss 0.04561
INFO:name:epoch 9 step 4100 loss 0.05134
INFO:name:epoch 9 step 4200 loss 0.04132
INFO:name:epoch 9 step 4300 loss 0.05256
INFO:name:epoch 9 step 4400 loss 0.05344
INFO:name:epoch 9 step 4500 loss 0.04732
INFO:name:epoch 9 step 4600 loss 0.05278
INFO:name:epoch 9 step 4700 loss 0.04646
INFO:name:epoch 9 step 4800 loss 0.05643
INFO:name:epoch 9 step 4900 loss 0.05578
INFO:name:epoch 9 step 5000 loss 0.0566
INFO:name:epoch 9 step 5100 loss 0.05694
INFO:name:epoch 9 step 5200 loss 0.06545
INFO:name:epoch 9 step 5300 loss 0.05321
INFO:name:epoch 9 step 5400 loss 0.04568
INFO:name:epoch 9 step 5500 loss 0.06266
INFO:name:epoch 9 step 5600 loss 0.05159
INFO:name:epoch 9 step 5700 loss 0.05239
INFO:name:epoch 9 step 5800 loss 0.04756
INFO:name:epoch 9 step 5900 loss 0.05571
INFO:name:epoch 9 step 6000 loss 0.05853
INFO:name:epoch 9 step 6100 loss 0.05121
INFO:name:epoch 9 step 6200 loss 0.04872
INFO:name:epoch 9 step 6300 loss 0.05628
INFO:name:epoch 9 step 6400 loss 0.04836
INFO:name:epoch 9 step 6500 loss 0.04985
INFO:name:epoch 9 step 6600 loss 0.04408
INFO:name:epoch 9 step 6700 loss 0.05532
INFO:name:epoch 9 step 6800 loss 0.05895
INFO:name:epoch 9 step 6900 loss 0.05608
INFO:name:epoch 9 step 7000 loss 0.04668
INFO:name:epoch 9 step 7100 loss 0.05577
INFO:name:epoch 9 step 7200 loss 0.06262
INFO:name:epoch 9 step 7300 loss 0.04887
INFO:name:epoch 9 step 7400 loss 0.05758
INFO:name:epoch 9 step 7500 loss 0.04523
INFO:name:epoch 9 step 7600 loss 0.05563
INFO:name:epoch 9 step 7700 loss 0.05645
INFO:name:epoch 9 step 7800 loss 0.04944
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3424
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
train results ([0.29587908698714205, 0.10434182266358007, 0.08710156856607669, 0.07709036964093, 0.07012773269046041, 0.06538069738796114, 0.06163126630684908, 0.05870905161108656, 0.05595635996319074, 0.054197556060665494], [0.23885062654797837, 0.3113961599887201, 0.3137042137549874, 0.33382731011109323, 0.32692885159121476, 0.3422259560783275, 0.34323237316362637, 0.3451787485462742, 0.34174138376009805, 0.34241716071666667])
