/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
INFO:name:device: cuda:0, n_gpu: 1
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/main/config.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

root
├── embeddings (RobertaEmbeddings)
│   ├── word_embeddings (Embedding) weight:[50265, 768]
│   ├── position_embeddings (Embedding) weight:[514, 768]
│   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
├── encoder (RobertaEncoder)
│   └── layer (ModuleList)
│       └── 0-11(RobertaLayer)
│           ├── attention (RobertaAttention)
│           │   ├── self (RobertaSelfAttention)
│           │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│           │   └── output (RobertaSelfOutput)
│           │       ├── dense (Linear) weight:[768, 768] bias:[768]
│           │       │   └── adapter (AdapterLayer)
│           │       │       └── modulelist (Sequential)
│           │       │           ├── down_proj (Linear) weight:[24, 768] bias:[24]
│           │       │           └── up_proj (Linear) weight:[768, 24] bias:[768]
│           │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│           ├── intermediate (RobertaIntermediate)
│           │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│           └── output (RobertaOutput)
│               ├── dense (Linear) weight:[768, 3072] bias:[768]
│               │   └── adapter (AdapterLayer)
│               │       └── modulelist (Sequential)
│               │           ├── down_proj (Linear) weight:[24, 768] bias:[24]
│               │           └── up_proj (Linear) weight:[768, 24] bias:[768]
│               └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── pooler (RobertaPooler)
    └── dense (Linear) weight:[768, 768] bias:[768]
[INFO|(OpenDelta)basemodel:700]2025-01-16 00:04:57,669 >> Trainable Ratio: 903744/125549376=0.719832%
[INFO|(OpenDelta)basemodel:702]2025-01-16 00:04:57,669 >> Delta Parameter Ratio: 903744/125549376=0.719832%
[INFO|(OpenDelta)basemodel:704]2025-01-16 00:04:57,669 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 10
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 78700
INFO:name:epoch 0 step 100 loss 3.73422
INFO:name:epoch 0 step 200 loss 3.46666
INFO:name:epoch 0 step 300 loss 3.46614
INFO:name:epoch 0 step 400 loss 3.46625
INFO:name:epoch 0 step 500 loss 3.45207
INFO:name:epoch 0 step 600 loss 3.21431
INFO:name:epoch 0 step 700 loss 3.06526
INFO:name:epoch 0 step 800 loss 3.03077
INFO:name:epoch 0 step 900 loss 3.00185
INFO:name:epoch 0 step 1000 loss 2.93179
INFO:name:epoch 0 step 1100 loss 2.94886
INFO:name:epoch 0 step 1200 loss 2.86454
INFO:name:epoch 0 step 1300 loss 2.78557
INFO:name:epoch 0 step 1400 loss 2.77284
INFO:name:epoch 0 step 1500 loss 2.71839
INFO:name:epoch 0 step 1600 loss 2.66414
INFO:name:epoch 0 step 1700 loss 2.64098
INFO:name:epoch 0 step 1800 loss 2.62199
INFO:name:epoch 0 step 1900 loss 2.58849
INFO:name:epoch 0 step 2000 loss 2.55013
INFO:name:epoch 0 step 2100 loss 2.50835
INFO:name:epoch 0 step 2200 loss 2.44523
INFO:name:epoch 0 step 2300 loss 2.38797
INFO:name:epoch 0 step 2400 loss 2.3225
INFO:name:epoch 0 step 2500 loss 2.29101
INFO:name:epoch 0 step 2600 loss 2.26229
INFO:name:epoch 0 step 2700 loss 2.1915
INFO:name:epoch 0 step 2800 loss 2.10849
INFO:name:epoch 0 step 2900 loss 2.10699
INFO:name:epoch 0 step 3000 loss 2.09428
INFO:name:epoch 0 step 3100 loss 2.07482
INFO:name:epoch 0 step 3200 loss 2.09339
INFO:name:epoch 0 step 3300 loss 2.02915
INFO:name:epoch 0 step 3400 loss 1.95229
INFO:name:epoch 0 step 3500 loss 1.94652
INFO:name:epoch 0 step 3600 loss 1.89323
INFO:name:epoch 0 step 3700 loss 1.88014
INFO:name:epoch 0 step 3800 loss 1.86809
INFO:name:epoch 0 step 3900 loss 1.83818
INFO:name:epoch 0 step 4000 loss 1.72447
INFO:name:epoch 0 step 4100 loss 1.74926
INFO:name:epoch 0 step 4200 loss 1.74602
INFO:name:epoch 0 step 4300 loss 1.69672
INFO:name:epoch 0 step 4400 loss 1.69236
INFO:name:epoch 0 step 4500 loss 1.71618
INFO:name:epoch 0 step 4600 loss 1.64669
INFO:name:epoch 0 step 4700 loss 1.62671
INFO:name:epoch 0 step 4800 loss 1.62408
INFO:name:epoch 0 step 4900 loss 1.53901
INFO:name:epoch 0 step 5000 loss 1.59192
INFO:name:epoch 0 step 5100 loss 1.56448
INFO:name:epoch 0 step 5200 loss 1.55609
INFO:name:epoch 0 step 5300 loss 1.50964
INFO:name:epoch 0 step 5400 loss 1.47379
INFO:name:epoch 0 step 5500 loss 1.49133
INFO:name:epoch 0 step 5600 loss 1.44938
INFO:name:epoch 0 step 5700 loss 1.4607
INFO:name:epoch 0 step 5800 loss 1.46367
INFO:name:epoch 0 step 5900 loss 1.37343
INFO:name:epoch 0 step 6000 loss 1.4437
INFO:name:epoch 0 step 6100 loss 1.43298
INFO:name:epoch 0 step 6200 loss 1.36168
INFO:name:epoch 0 step 6300 loss 1.36259
INFO:name:epoch 0 step 6400 loss 1.32893
INFO:name:epoch 0 step 6500 loss 1.34547
INFO:name:epoch 0 step 6600 loss 1.37533
INFO:name:epoch 0 step 6700 loss 1.28104
INFO:name:epoch 0 step 6800 loss 1.29616
INFO:name:epoch 0 step 6900 loss 1.28411
INFO:name:epoch 0 step 7000 loss 1.27851
INFO:name:epoch 0 step 7100 loss 1.25077
INFO:name:epoch 0 step 7200 loss 1.27314
INFO:name:epoch 0 step 7300 loss 1.24195
INFO:name:epoch 0 step 7400 loss 1.18047
INFO:name:epoch 0 step 7500 loss 1.21722
INFO:name:epoch 0 step 7600 loss 1.1599
INFO:name:epoch 0 step 7700 loss 1.16643
INFO:name:epoch 0 step 7800 loss 1.14903
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0226
INFO:name:  ********************
INFO:name:  Best eval mrr:0.0226
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0132
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 1.0645
INFO:name:epoch 1 step 200 loss 0.92395
INFO:name:epoch 1 step 300 loss 0.92587
INFO:name:epoch 1 step 400 loss 0.94166
INFO:name:epoch 1 step 500 loss 0.89315
INFO:name:epoch 1 step 600 loss 0.92634
INFO:name:epoch 1 step 700 loss 0.89328
INFO:name:epoch 1 step 800 loss 0.87521
INFO:name:epoch 1 step 900 loss 0.86795
INFO:name:epoch 1 step 1000 loss 0.8409
INFO:name:epoch 1 step 1100 loss 0.81932
INFO:name:epoch 1 step 1200 loss 0.83312
INFO:name:epoch 1 step 1300 loss 0.78656
INFO:name:epoch 1 step 1400 loss 0.80559
INFO:name:epoch 1 step 1500 loss 0.79744
INFO:name:epoch 1 step 1600 loss 0.81639
INFO:name:epoch 1 step 1700 loss 0.77554
INFO:name:epoch 1 step 1800 loss 0.78549
INFO:name:epoch 1 step 1900 loss 0.76485
INFO:name:epoch 1 step 2000 loss 0.80833
INFO:name:epoch 1 step 2100 loss 0.73232
INFO:name:epoch 1 step 2200 loss 0.74178
INFO:name:epoch 1 step 2300 loss 0.74856
INFO:name:epoch 1 step 2400 loss 0.73096
INFO:name:epoch 1 step 2500 loss 0.72491
INFO:name:epoch 1 step 2600 loss 0.70378
INFO:name:epoch 1 step 2700 loss 0.72313
INFO:name:epoch 1 step 2800 loss 0.69205
INFO:name:epoch 1 step 2900 loss 0.66612
INFO:name:epoch 1 step 3000 loss 0.7041
INFO:name:epoch 1 step 3100 loss 0.62238
INFO:name:epoch 1 step 3200 loss 0.69918
INFO:name:epoch 1 step 3300 loss 0.67402
INFO:name:epoch 1 step 3400 loss 0.66798
INFO:name:epoch 1 step 3500 loss 0.6771
INFO:name:epoch 1 step 3600 loss 0.63453
INFO:name:epoch 1 step 3700 loss 0.64636
INFO:name:epoch 1 step 3800 loss 0.5982
INFO:name:epoch 1 step 3900 loss 0.60485
INFO:name:epoch 1 step 4000 loss 0.62121
INFO:name:epoch 1 step 4100 loss 0.60577
INFO:name:epoch 1 step 4200 loss 0.64794
INFO:name:epoch 1 step 4300 loss 0.63436
INFO:name:epoch 1 step 4400 loss 0.61378
INFO:name:epoch 1 step 4500 loss 0.5686
INFO:name:epoch 1 step 4600 loss 0.58636
INFO:name:epoch 1 step 4700 loss 0.61018
INFO:name:epoch 1 step 4800 loss 0.57316
INFO:name:epoch 1 step 4900 loss 0.61396
INFO:name:epoch 1 step 5000 loss 0.58214
INFO:name:epoch 1 step 5100 loss 0.59301
INFO:name:epoch 1 step 5200 loss 0.55999
INFO:name:epoch 1 step 5300 loss 0.55929
INFO:name:epoch 1 step 5400 loss 0.56908
INFO:name:epoch 1 step 5500 loss 0.56582
INFO:name:epoch 1 step 5600 loss 0.5427
INFO:name:epoch 1 step 5700 loss 0.59026
INFO:name:epoch 1 step 5800 loss 0.55053
INFO:name:epoch 1 step 5900 loss 0.5477
INFO:name:epoch 1 step 6000 loss 0.55104
INFO:name:epoch 1 step 6100 loss 0.51834
INFO:name:epoch 1 step 6200 loss 0.52868
INFO:name:epoch 1 step 6300 loss 0.52676
INFO:name:epoch 1 step 6400 loss 0.51021
INFO:name:epoch 1 step 6500 loss 0.48761
INFO:name:epoch 1 step 6600 loss 0.51668
INFO:name:epoch 1 step 6700 loss 0.54609
INFO:name:epoch 1 step 6800 loss 0.49744
INFO:name:epoch 1 step 6900 loss 0.5059
INFO:name:epoch 1 step 7000 loss 0.51911
INFO:name:epoch 1 step 7100 loss 0.47869
INFO:name:epoch 1 step 7200 loss 0.50317
INFO:name:epoch 1 step 7300 loss 0.46691
INFO:name:epoch 1 step 7400 loss 0.48259
INFO:name:epoch 1 step 7500 loss 0.48061
INFO:name:epoch 1 step 7600 loss 0.50285
INFO:name:epoch 1 step 7700 loss 0.46546
INFO:name:epoch 1 step 7800 loss 0.48772
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0488
INFO:name:  ********************
INFO:name:  Best eval mrr:0.0488
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0321
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 2 step 100 loss 0.46642
INFO:name:epoch 2 step 200 loss 0.42057
INFO:name:epoch 2 step 300 loss 0.47514
INFO:name:epoch 2 step 400 loss 0.40989
INFO:name:epoch 2 step 500 loss 0.40785
INFO:name:epoch 2 step 600 loss 0.39546
INFO:name:epoch 2 step 700 loss 0.42201
INFO:name:epoch 2 step 800 loss 0.45125
INFO:name:epoch 2 step 900 loss 0.42957
INFO:name:epoch 2 step 1000 loss 0.47727
INFO:name:epoch 2 step 1100 loss 0.42295
INFO:name:epoch 2 step 1200 loss 0.406
INFO:name:epoch 2 step 1300 loss 0.41318
INFO:name:epoch 2 step 1400 loss 0.39928
INFO:name:epoch 2 step 1500 loss 0.44485
INFO:name:epoch 2 step 1600 loss 0.39681
INFO:name:epoch 2 step 1700 loss 0.42524
INFO:name:epoch 2 step 1800 loss 0.40535
INFO:name:epoch 2 step 1900 loss 0.39147
INFO:name:epoch 2 step 2000 loss 0.43891
INFO:name:epoch 2 step 2100 loss 0.43006
INFO:name:epoch 2 step 2200 loss 0.42221
INFO:name:epoch 2 step 2300 loss 0.41914
INFO:name:epoch 2 step 2400 loss 0.45288
INFO:name:epoch 2 step 2500 loss 0.42262
INFO:name:epoch 2 step 2600 loss 0.42079
INFO:name:epoch 2 step 2700 loss 0.41567
INFO:name:epoch 2 step 2800 loss 0.41902
INFO:name:epoch 2 step 2900 loss 0.40287
INFO:name:epoch 2 step 3000 loss 0.4204
INFO:name:epoch 2 step 3100 loss 0.37448
INFO:name:epoch 2 step 3200 loss 0.43117
INFO:name:epoch 2 step 3300 loss 0.38672
INFO:name:epoch 2 step 3400 loss 0.3884
INFO:name:epoch 2 step 3500 loss 0.41156
INFO:name:epoch 2 step 3600 loss 0.3833
INFO:name:epoch 2 step 3700 loss 0.40153
INFO:name:epoch 2 step 3800 loss 0.37958
INFO:name:epoch 2 step 3900 loss 0.41972
INFO:name:epoch 2 step 4000 loss 0.37794
INFO:name:epoch 2 step 4100 loss 0.40772
INFO:name:epoch 2 step 4200 loss 0.3569
INFO:name:epoch 2 step 4300 loss 0.35783
INFO:name:epoch 2 step 4400 loss 0.3553
INFO:name:epoch 2 step 4500 loss 0.36632
INFO:name:epoch 2 step 4600 loss 0.38687
INFO:name:epoch 2 step 4700 loss 0.42409
INFO:name:epoch 2 step 4800 loss 0.35
INFO:name:epoch 2 step 4900 loss 0.38464
INFO:name:epoch 2 step 5000 loss 0.36811
INFO:name:epoch 2 step 5100 loss 0.37917
INFO:name:epoch 2 step 5200 loss 0.40703
INFO:name:epoch 2 step 5300 loss 0.37076
INFO:name:epoch 2 step 5400 loss 0.38454
INFO:name:epoch 2 step 5500 loss 0.34053
INFO:name:epoch 2 step 5600 loss 0.37255
INFO:name:epoch 2 step 5700 loss 0.35851
INFO:name:epoch 2 step 5800 loss 0.34034
INFO:name:epoch 2 step 5900 loss 0.34922
INFO:name:epoch 2 step 6000 loss 0.32465
INFO:name:epoch 2 step 6100 loss 0.35725
INFO:name:epoch 2 step 6200 loss 0.35906
INFO:name:epoch 2 step 6300 loss 0.32517
INFO:name:epoch 2 step 6400 loss 0.35391
INFO:name:epoch 2 step 6500 loss 0.36432
INFO:name:epoch 2 step 6600 loss 0.33329
INFO:name:epoch 2 step 6700 loss 0.34157
INFO:name:epoch 2 step 6800 loss 0.34479
INFO:name:epoch 2 step 6900 loss 0.33826
INFO:name:epoch 2 step 7000 loss 0.34447
INFO:name:epoch 2 step 7100 loss 0.32971
INFO:name:epoch 2 step 7200 loss 0.36057
INFO:name:epoch 2 step 7300 loss 0.33127
INFO:name:epoch 2 step 7400 loss 0.35861
INFO:name:epoch 2 step 7500 loss 0.34088
INFO:name:epoch 2 step 7600 loss 0.30942
INFO:name:epoch 2 step 7700 loss 0.33718
INFO:name:epoch 2 step 7800 loss 0.31257
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0807
INFO:name:  ********************
INFO:name:  Best eval mrr:0.0807
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0576
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.33157
INFO:name:epoch 3 step 200 loss 0.31973
INFO:name:epoch 3 step 300 loss 0.29991
INFO:name:epoch 3 step 400 loss 0.29573
INFO:name:epoch 3 step 500 loss 0.31101
INFO:name:epoch 3 step 600 loss 0.28518
INFO:name:epoch 3 step 700 loss 0.30547
INFO:name:epoch 3 step 800 loss 0.31385
INFO:name:epoch 3 step 900 loss 0.31701
INFO:name:epoch 3 step 1000 loss 0.27915
INFO:name:epoch 3 step 1100 loss 0.29124
INFO:name:epoch 3 step 1200 loss 0.29466
INFO:name:epoch 3 step 1300 loss 0.31616
INFO:name:epoch 3 step 1400 loss 0.30079
INFO:name:epoch 3 step 1500 loss 0.30867
INFO:name:epoch 3 step 1600 loss 0.2956
INFO:name:epoch 3 step 1700 loss 0.27111
INFO:name:epoch 3 step 1800 loss 0.28221
INFO:name:epoch 3 step 1900 loss 0.31283
INFO:name:epoch 3 step 2000 loss 0.30436
INFO:name:epoch 3 step 2100 loss 0.27812
INFO:name:epoch 3 step 2200 loss 0.30509
INFO:name:epoch 3 step 2300 loss 0.29461
INFO:name:epoch 3 step 2400 loss 0.32826
INFO:name:epoch 3 step 2500 loss 0.28896
INFO:name:epoch 3 step 2600 loss 0.26832
INFO:name:epoch 3 step 2700 loss 0.27328
INFO:name:epoch 3 step 2800 loss 0.30629
INFO:name:epoch 3 step 2900 loss 0.2839
INFO:name:epoch 3 step 3000 loss 0.27201
INFO:name:epoch 3 step 3100 loss 0.29889
INFO:name:epoch 3 step 3200 loss 0.28808
INFO:name:epoch 3 step 3300 loss 0.2897
INFO:name:epoch 3 step 3400 loss 0.30133
INFO:name:epoch 3 step 3500 loss 0.28554
INFO:name:epoch 3 step 3600 loss 0.28287
INFO:name:epoch 3 step 3700 loss 0.28763
INFO:name:epoch 3 step 3800 loss 0.2359
INFO:name:epoch 3 step 3900 loss 0.29014
INFO:name:epoch 3 step 4000 loss 0.28705
INFO:name:epoch 3 step 4100 loss 0.28256
INFO:name:epoch 3 step 4200 loss 0.27763
INFO:name:epoch 3 step 4300 loss 0.27612
INFO:name:epoch 3 step 4400 loss 0.29195
INFO:name:epoch 3 step 4500 loss 0.28974
INFO:name:epoch 3 step 4600 loss 0.28857
INFO:name:epoch 3 step 4700 loss 0.27694
INFO:name:epoch 3 step 4800 loss 0.29018
INFO:name:epoch 3 step 4900 loss 0.27313
INFO:name:epoch 3 step 5000 loss 0.26968
INFO:name:epoch 3 step 5100 loss 0.28271
INFO:name:epoch 3 step 5200 loss 0.2845
INFO:name:epoch 3 step 5300 loss 0.28195
INFO:name:epoch 3 step 5400 loss 0.27304
INFO:name:epoch 3 step 5500 loss 0.28574
INFO:name:epoch 3 step 5600 loss 0.28251
INFO:name:epoch 3 step 5700 loss 0.27463
INFO:name:epoch 3 step 5800 loss 0.26161
INFO:name:epoch 3 step 5900 loss 0.27768
INFO:name:epoch 3 step 6000 loss 0.27358
INFO:name:epoch 3 step 6100 loss 0.25571
INFO:name:epoch 3 step 6200 loss 0.27604
INFO:name:epoch 3 step 6300 loss 0.27153
INFO:name:epoch 3 step 6400 loss 0.28626
INFO:name:epoch 3 step 6500 loss 0.26607
INFO:name:epoch 3 step 6600 loss 0.2638
INFO:name:epoch 3 step 6700 loss 0.25844
INFO:name:epoch 3 step 6800 loss 0.27159
INFO:name:epoch 3 step 6900 loss 0.26914
INFO:name:epoch 3 step 7000 loss 0.26445
INFO:name:epoch 3 step 7100 loss 0.27437
INFO:name:epoch 3 step 7200 loss 0.2694
INFO:name:epoch 3 step 7300 loss 0.26985
INFO:name:epoch 3 step 7400 loss 0.26134
INFO:name:epoch 3 step 7500 loss 0.27314
INFO:name:epoch 3 step 7600 loss 0.27052
INFO:name:epoch 3 step 7700 loss 0.26179
INFO:name:epoch 3 step 7800 loss 0.26991
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1001
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1001
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0753
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 4 step 100 loss 0.2521
INFO:name:epoch 4 step 200 loss 0.24052
INFO:name:epoch 4 step 300 loss 0.23569
INFO:name:epoch 4 step 400 loss 0.25057
INFO:name:epoch 4 step 500 loss 0.22596
INFO:name:epoch 4 step 600 loss 0.22611
INFO:name:epoch 4 step 700 loss 0.22721
INFO:name:epoch 4 step 800 loss 0.22619
INFO:name:epoch 4 step 900 loss 0.2202
INFO:name:epoch 4 step 1000 loss 0.25008
INFO:name:epoch 4 step 1100 loss 0.24512
INFO:name:epoch 4 step 1200 loss 0.26232
INFO:name:epoch 4 step 1300 loss 0.23306
INFO:name:epoch 4 step 1400 loss 0.25244
INFO:name:epoch 4 step 1500 loss 0.23829
INFO:name:epoch 4 step 1600 loss 0.23743
INFO:name:epoch 4 step 1700 loss 0.24516
INFO:name:epoch 4 step 1800 loss 0.24043
INFO:name:epoch 4 step 1900 loss 0.22131
INFO:name:epoch 4 step 2000 loss 0.24173
INFO:name:epoch 4 step 2100 loss 0.23626
INFO:name:epoch 4 step 2200 loss 0.23236
INFO:name:epoch 4 step 2300 loss 0.22896
INFO:name:epoch 4 step 2400 loss 0.23042
INFO:name:epoch 4 step 2500 loss 0.24576
INFO:name:epoch 4 step 2600 loss 0.22021
INFO:name:epoch 4 step 2700 loss 0.25652
INFO:name:epoch 4 step 2800 loss 0.22026
INFO:name:epoch 4 step 2900 loss 0.23495
INFO:name:epoch 4 step 3000 loss 0.25243
INFO:name:epoch 4 step 3100 loss 0.23839
INFO:name:epoch 4 step 3200 loss 0.22332
INFO:name:epoch 4 step 3300 loss 0.25
INFO:name:epoch 4 step 3400 loss 0.22676
INFO:name:epoch 4 step 3500 loss 0.2341
INFO:name:epoch 4 step 3600 loss 0.2391
INFO:name:epoch 4 step 3700 loss 0.21834
INFO:name:epoch 4 step 3800 loss 0.23755
INFO:name:epoch 4 step 3900 loss 0.20635
INFO:name:epoch 4 step 4000 loss 0.27271
INFO:name:epoch 4 step 4100 loss 0.22644
INFO:name:epoch 4 step 4200 loss 0.23533
INFO:name:epoch 4 step 4300 loss 0.22535
INFO:name:epoch 4 step 4400 loss 0.25269
INFO:name:epoch 4 step 4500 loss 0.24408
INFO:name:epoch 4 step 4600 loss 0.23853
INFO:name:epoch 4 step 4700 loss 0.24047
INFO:name:epoch 4 step 4800 loss 0.22352
INFO:name:epoch 4 step 4900 loss 0.20277
INFO:name:epoch 4 step 5000 loss 0.23239
INFO:name:epoch 4 step 5100 loss 0.2127
INFO:name:epoch 4 step 5200 loss 0.22026
INFO:name:epoch 4 step 5300 loss 0.23406
INFO:name:epoch 4 step 5400 loss 0.22437
INFO:name:epoch 4 step 5500 loss 0.22312
INFO:name:epoch 4 step 5600 loss 0.21311
INFO:name:epoch 4 step 5700 loss 0.23064
INFO:name:epoch 4 step 5800 loss 0.21699
INFO:name:epoch 4 step 5900 loss 0.20312
INFO:name:epoch 4 step 6000 loss 0.21062
INFO:name:epoch 4 step 6100 loss 0.22692
INFO:name:epoch 4 step 6200 loss 0.21928
INFO:name:epoch 4 step 6300 loss 0.21397
INFO:name:epoch 4 step 6400 loss 0.22637
INFO:name:epoch 4 step 6500 loss 0.20914
INFO:name:epoch 4 step 6600 loss 0.23313
INFO:name:epoch 4 step 6700 loss 0.2341
INFO:name:epoch 4 step 6800 loss 0.20061
INFO:name:epoch 4 step 6900 loss 0.22014
INFO:name:epoch 4 step 7000 loss 0.20741
INFO:name:epoch 4 step 7100 loss 0.21013
INFO:name:epoch 4 step 7200 loss 0.22776
INFO:name:epoch 4 step 7300 loss 0.22009
INFO:name:epoch 4 step 7400 loss 0.20081
INFO:name:epoch 4 step 7500 loss 0.21849
INFO:name:epoch 4 step 7600 loss 0.21824
INFO:name:epoch 4 step 7700 loss 0.22378
INFO:name:epoch 4 step 7800 loss 0.22274
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1243
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1243
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.0926
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 5 step 100 loss 0.21441
INFO:name:epoch 5 step 200 loss 0.19554
INFO:name:epoch 5 step 300 loss 0.1729
INFO:name:epoch 5 step 400 loss 0.21221
INFO:name:epoch 5 step 500 loss 0.18794
INFO:name:epoch 5 step 600 loss 0.19786
INFO:name:epoch 5 step 700 loss 0.21963
INFO:name:epoch 5 step 800 loss 0.18174
INFO:name:epoch 5 step 900 loss 0.17912
INFO:name:epoch 5 step 1000 loss 0.20359
INFO:name:epoch 5 step 1100 loss 0.193
INFO:name:epoch 5 step 1200 loss 0.20856
INFO:name:epoch 5 step 1300 loss 0.18268
INFO:name:epoch 5 step 1400 loss 0.22442
INFO:name:epoch 5 step 1500 loss 0.18709
INFO:name:epoch 5 step 1600 loss 0.17366
INFO:name:epoch 5 step 1700 loss 0.20859
INFO:name:epoch 5 step 1800 loss 0.19179
INFO:name:epoch 5 step 1900 loss 0.20286
INFO:name:epoch 5 step 2000 loss 0.19837
INFO:name:epoch 5 step 2100 loss 0.19339
INFO:name:epoch 5 step 2200 loss 0.18328
INFO:name:epoch 5 step 2300 loss 0.18766
INFO:name:epoch 5 step 2400 loss 0.19928
INFO:name:epoch 5 step 2500 loss 0.19162
INFO:name:epoch 5 step 2600 loss 0.21557
INFO:name:epoch 5 step 2700 loss 0.20327
INFO:name:epoch 5 step 2800 loss 0.21101
INFO:name:epoch 5 step 2900 loss 0.2299
INFO:name:epoch 5 step 3000 loss 0.19029
INFO:name:epoch 5 step 3100 loss 0.20572
INFO:name:epoch 5 step 3200 loss 0.18271
INFO:name:epoch 5 step 3300 loss 0.18517
INFO:name:epoch 5 step 3400 loss 0.18274
INFO:name:epoch 5 step 3500 loss 0.19238
INFO:name:epoch 5 step 3600 loss 0.18435
INFO:name:epoch 5 step 3700 loss 0.20555
INFO:name:epoch 5 step 3800 loss 0.18718
INFO:name:epoch 5 step 3900 loss 0.18715
INFO:name:epoch 5 step 4000 loss 0.22602
INFO:name:epoch 5 step 4100 loss 0.14499
INFO:name:epoch 5 step 4200 loss 0.18384
INFO:name:epoch 5 step 4300 loss 0.19253
INFO:name:epoch 5 step 4400 loss 0.19436
INFO:name:epoch 5 step 4500 loss 0.20118
INFO:name:epoch 5 step 4600 loss 0.19655
INFO:name:epoch 5 step 4700 loss 0.19258
INFO:name:epoch 5 step 4800 loss 0.19288
INFO:name:epoch 5 step 4900 loss 0.15938
INFO:name:epoch 5 step 5000 loss 0.18648
INFO:name:epoch 5 step 5100 loss 0.21829
INFO:name:epoch 5 step 5200 loss 0.17827
INFO:name:epoch 5 step 5300 loss 0.20213
INFO:name:epoch 5 step 5400 loss 0.19015
INFO:name:epoch 5 step 5500 loss 0.18335
INFO:name:epoch 5 step 5600 loss 0.19815
INFO:name:epoch 5 step 5700 loss 0.18437
INFO:name:epoch 5 step 5800 loss 0.19023
INFO:name:epoch 5 step 5900 loss 0.18358
INFO:name:epoch 5 step 6000 loss 0.19599
INFO:name:epoch 5 step 6100 loss 0.17998
INFO:name:epoch 5 step 6200 loss 0.19682
INFO:name:epoch 5 step 6300 loss 0.19551
INFO:name:epoch 5 step 6400 loss 0.18683
INFO:name:epoch 5 step 6500 loss 0.17515
INFO:name:epoch 5 step 6600 loss 0.18551
INFO:name:epoch 5 step 6700 loss 0.18575
INFO:name:epoch 5 step 6800 loss 0.1863
INFO:name:epoch 5 step 6900 loss 0.22452
INFO:name:epoch 5 step 7000 loss 0.17439
INFO:name:epoch 5 step 7100 loss 0.17409
INFO:name:epoch 5 step 7200 loss 0.17446
INFO:name:epoch 5 step 7300 loss 0.18717
INFO:name:epoch 5 step 7400 loss 0.1891
INFO:name:epoch 5 step 7500 loss 0.16653
INFO:name:epoch 5 step 7600 loss 0.18977
INFO:name:epoch 5 step 7700 loss 0.16527
INFO:name:epoch 5 step 7800 loss 0.21873
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.136
INFO:name:  ********************
INFO:name:  Best eval mrr:0.136
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1014
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 6 step 100 loss 0.17225
INFO:name:epoch 6 step 200 loss 0.17153
INFO:name:epoch 6 step 300 loss 0.1549
INFO:name:epoch 6 step 400 loss 0.15318
INFO:name:epoch 6 step 500 loss 0.15808
INFO:name:epoch 6 step 600 loss 0.15907
INFO:name:epoch 6 step 700 loss 0.18563
INFO:name:epoch 6 step 800 loss 0.16465
INFO:name:epoch 6 step 900 loss 0.16528
INFO:name:epoch 6 step 1000 loss 0.13976
INFO:name:epoch 6 step 1100 loss 0.19168
INFO:name:epoch 6 step 1200 loss 0.1715
INFO:name:epoch 6 step 1300 loss 0.16324
INFO:name:epoch 6 step 1400 loss 0.14731
INFO:name:epoch 6 step 1500 loss 0.17416
INFO:name:epoch 6 step 1600 loss 0.18184
INFO:name:epoch 6 step 1700 loss 0.18066
INFO:name:epoch 6 step 1800 loss 0.17148
INFO:name:epoch 6 step 1900 loss 0.16313
INFO:name:epoch 6 step 2000 loss 0.16464
INFO:name:epoch 6 step 2100 loss 0.15505
INFO:name:epoch 6 step 2200 loss 0.17021
INFO:name:epoch 6 step 2300 loss 0.15603
INFO:name:epoch 6 step 2400 loss 0.17127
INFO:name:epoch 6 step 2500 loss 0.16031
INFO:name:epoch 6 step 2600 loss 0.17448
INFO:name:epoch 6 step 2700 loss 0.16772
INFO:name:epoch 6 step 2800 loss 0.14965
INFO:name:epoch 6 step 2900 loss 0.16279
INFO:name:epoch 6 step 3000 loss 0.15655
INFO:name:epoch 6 step 3100 loss 0.17525
INFO:name:epoch 6 step 3200 loss 0.16525
INFO:name:epoch 6 step 3300 loss 0.18092
INFO:name:epoch 6 step 3400 loss 0.17065
INFO:name:epoch 6 step 3500 loss 0.16617
INFO:name:epoch 6 step 3600 loss 0.17095
INFO:name:epoch 6 step 3700 loss 0.15694
INFO:name:epoch 6 step 3800 loss 0.16083
INFO:name:epoch 6 step 3900 loss 0.14598
INFO:name:epoch 6 step 4000 loss 0.16072
INFO:name:epoch 6 step 4100 loss 0.16345
INFO:name:epoch 6 step 4200 loss 0.15787
INFO:name:epoch 6 step 4300 loss 0.1684
INFO:name:epoch 6 step 4400 loss 0.18088
INFO:name:epoch 6 step 4500 loss 0.15377
INFO:name:epoch 6 step 4600 loss 0.16341
INFO:name:epoch 6 step 4700 loss 0.18546
INFO:name:epoch 6 step 4800 loss 0.1545
INFO:name:epoch 6 step 4900 loss 0.17427
INFO:name:epoch 6 step 5000 loss 0.16834
INFO:name:epoch 6 step 5100 loss 0.16411
INFO:name:epoch 6 step 5200 loss 0.18236
INFO:name:epoch 6 step 5300 loss 0.16785
INFO:name:epoch 6 step 5400 loss 0.18446
INFO:name:epoch 6 step 5500 loss 0.1758
INFO:name:epoch 6 step 5600 loss 0.16332
INFO:name:epoch 6 step 5700 loss 0.17299
INFO:name:epoch 6 step 5800 loss 0.16279
INFO:name:epoch 6 step 5900 loss 0.16296
INFO:name:epoch 6 step 6000 loss 0.15745
INFO:name:epoch 6 step 6100 loss 0.15733
INFO:name:epoch 6 step 6200 loss 0.1764
INFO:name:epoch 6 step 6300 loss 0.19389
INFO:name:epoch 6 step 6400 loss 0.14944
INFO:name:epoch 6 step 6500 loss 0.16526
INFO:name:epoch 6 step 6600 loss 0.1707
INFO:name:epoch 6 step 6700 loss 0.16727
INFO:name:epoch 6 step 6800 loss 0.16923
INFO:name:epoch 6 step 6900 loss 0.17928
INFO:name:epoch 6 step 7000 loss 0.16042
INFO:name:epoch 6 step 7100 loss 0.172
INFO:name:epoch 6 step 7200 loss 0.16688
INFO:name:epoch 6 step 7300 loss 0.16061
INFO:name:epoch 6 step 7400 loss 0.17393
INFO:name:epoch 6 step 7500 loss 0.17009
INFO:name:epoch 6 step 7600 loss 0.16797
INFO:name:epoch 6 step 7700 loss 0.16426
INFO:name:epoch 6 step 7800 loss 0.17558
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.142
INFO:name:  ********************
INFO:name:  Best eval mrr:0.142
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1057
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 7 step 100 loss 0.16163
INFO:name:epoch 7 step 200 loss 0.15824
INFO:name:epoch 7 step 300 loss 0.15164
INFO:name:epoch 7 step 400 loss 0.1312
INFO:name:epoch 7 step 500 loss 0.1403
INFO:name:epoch 7 step 600 loss 0.14817
INFO:name:epoch 7 step 700 loss 0.17229
INFO:name:epoch 7 step 800 loss 0.14933
INFO:name:epoch 7 step 900 loss 0.14446
INFO:name:epoch 7 step 1000 loss 0.13307
INFO:name:epoch 7 step 1100 loss 0.14738
INFO:name:epoch 7 step 1200 loss 0.15872
INFO:name:epoch 7 step 1300 loss 0.13749
INFO:name:epoch 7 step 1400 loss 0.13617
INFO:name:epoch 7 step 1500 loss 0.15699
INFO:name:epoch 7 step 1600 loss 0.13049
INFO:name:epoch 7 step 1700 loss 0.16327
INFO:name:epoch 7 step 1800 loss 0.16796
INFO:name:epoch 7 step 1900 loss 0.14653
INFO:name:epoch 7 step 2000 loss 0.14581
INFO:name:epoch 7 step 2100 loss 0.14849
INFO:name:epoch 7 step 2200 loss 0.15608
INFO:name:epoch 7 step 2300 loss 0.15781
INFO:name:epoch 7 step 2400 loss 0.14012
INFO:name:epoch 7 step 2500 loss 0.13408
INFO:name:epoch 7 step 2600 loss 0.14851
INFO:name:epoch 7 step 2700 loss 0.152
INFO:name:epoch 7 step 2800 loss 0.14634
INFO:name:epoch 7 step 2900 loss 0.14834
INFO:name:epoch 7 step 3000 loss 0.13798
INFO:name:epoch 7 step 3100 loss 0.13815
INFO:name:epoch 7 step 3200 loss 0.13888
INFO:name:epoch 7 step 3300 loss 0.12198
INFO:name:epoch 7 step 3400 loss 0.14764
INFO:name:epoch 7 step 3500 loss 0.15203
INFO:name:epoch 7 step 3600 loss 0.13625
INFO:name:epoch 7 step 3700 loss 0.14907
INFO:name:epoch 7 step 3800 loss 0.16348
INFO:name:epoch 7 step 3900 loss 0.16078
INFO:name:epoch 7 step 4000 loss 0.1489
INFO:name:epoch 7 step 4100 loss 0.14547
INFO:name:epoch 7 step 4200 loss 0.16045
INFO:name:epoch 7 step 4300 loss 0.15842
INFO:name:epoch 7 step 4400 loss 0.13203
INFO:name:epoch 7 step 4500 loss 0.14374
INFO:name:epoch 7 step 4600 loss 0.14447
INFO:name:epoch 7 step 4700 loss 0.13479
INFO:name:epoch 7 step 4800 loss 0.15042
INFO:name:epoch 7 step 4900 loss 0.15119
INFO:name:epoch 7 step 5000 loss 0.15148
INFO:name:epoch 7 step 5100 loss 0.15969
INFO:name:epoch 7 step 5200 loss 0.15558
INFO:name:epoch 7 step 5300 loss 0.14159
INFO:name:epoch 7 step 5400 loss 0.15773
INFO:name:epoch 7 step 5500 loss 0.14311
INFO:name:epoch 7 step 5600 loss 0.12731
INFO:name:epoch 7 step 5700 loss 0.15552
INFO:name:epoch 7 step 5800 loss 0.1376
INFO:name:epoch 7 step 5900 loss 0.14947
INFO:name:epoch 7 step 6000 loss 0.13802
INFO:name:epoch 7 step 6100 loss 0.14495
INFO:name:epoch 7 step 6200 loss 0.1553
INFO:name:epoch 7 step 6300 loss 0.14123
INFO:name:epoch 7 step 6400 loss 0.1471
INFO:name:epoch 7 step 6500 loss 0.12114
INFO:name:epoch 7 step 6600 loss 0.14949
INFO:name:epoch 7 step 6700 loss 0.1279
INFO:name:epoch 7 step 6800 loss 0.14394
INFO:name:epoch 7 step 6900 loss 0.15765
INFO:name:epoch 7 step 7000 loss 0.14665
INFO:name:epoch 7 step 7100 loss 0.15174
INFO:name:epoch 7 step 7200 loss 0.13991
INFO:name:epoch 7 step 7300 loss 0.14605
INFO:name:epoch 7 step 7400 loss 0.13573
INFO:name:epoch 7 step 7500 loss 0.15498
INFO:name:epoch 7 step 7600 loss 0.14191
INFO:name:epoch 7 step 7700 loss 0.14999
INFO:name:epoch 7 step 7800 loss 0.13833
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1523
INFO:name:  ********************
INFO:name:  Best eval mrr:0.1523
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1146
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 8 step 100 loss 0.13044
INFO:name:epoch 8 step 200 loss 0.13722
INFO:name:epoch 8 step 300 loss 0.14216
INFO:name:epoch 8 step 400 loss 0.12695
INFO:name:epoch 8 step 500 loss 0.12714
INFO:name:epoch 8 step 600 loss 0.14363
INFO:name:epoch 8 step 700 loss 0.14214
INFO:name:epoch 8 step 800 loss 0.11271
INFO:name:epoch 8 step 900 loss 0.12488
INFO:name:epoch 8 step 1000 loss 0.14226
INFO:name:epoch 8 step 1100 loss 0.13807
INFO:name:epoch 8 step 1200 loss 0.13201
INFO:name:epoch 8 step 1300 loss 0.14366
INFO:name:epoch 8 step 1400 loss 0.14288
INFO:name:epoch 8 step 1500 loss 0.12881
INFO:name:epoch 8 step 1600 loss 0.135
INFO:name:epoch 8 step 1700 loss 0.1197
INFO:name:epoch 8 step 1800 loss 0.13279
INFO:name:epoch 8 step 1900 loss 0.13792
INFO:name:epoch 8 step 2000 loss 0.14003
INFO:name:epoch 8 step 2100 loss 0.15015
INFO:name:epoch 8 step 2200 loss 0.128
INFO:name:epoch 8 step 2300 loss 0.12574
INFO:name:epoch 8 step 2400 loss 0.14208
INFO:name:epoch 8 step 2500 loss 0.13553
INFO:name:epoch 8 step 2600 loss 0.12636
INFO:name:epoch 8 step 2700 loss 0.14947
INFO:name:epoch 8 step 2800 loss 0.14942
INFO:name:epoch 8 step 2900 loss 0.13547
INFO:name:epoch 8 step 3000 loss 0.13887
INFO:name:epoch 8 step 3100 loss 0.14787
INFO:name:epoch 8 step 3200 loss 0.13699
INFO:name:epoch 8 step 3300 loss 0.15924
INFO:name:epoch 8 step 3400 loss 0.13072
INFO:name:epoch 8 step 3500 loss 0.14044
INFO:name:epoch 8 step 3600 loss 0.13731
INFO:name:epoch 8 step 3700 loss 0.12471
INFO:name:epoch 8 step 3800 loss 0.11609
INFO:name:epoch 8 step 3900 loss 0.13537
INFO:name:epoch 8 step 4000 loss 0.13701
INFO:name:epoch 8 step 4100 loss 0.14991
INFO:name:epoch 8 step 4200 loss 0.1332
INFO:name:epoch 8 step 4300 loss 0.14045
INFO:name:epoch 8 step 4400 loss 0.1257
INFO:name:epoch 8 step 4500 loss 0.13703
INFO:name:epoch 8 step 4600 loss 0.1472
INFO:name:epoch 8 step 4700 loss 0.13914
INFO:name:epoch 8 step 4800 loss 0.13036
INFO:name:epoch 8 step 4900 loss 0.13973
INFO:name:epoch 8 step 5000 loss 0.14319
INFO:name:epoch 8 step 5100 loss 0.13062
INFO:name:epoch 8 step 5200 loss 0.14184
INFO:name:epoch 8 step 5300 loss 0.13782
INFO:name:epoch 8 step 5400 loss 0.13807
INFO:name:epoch 8 step 5500 loss 0.12491
INFO:name:epoch 8 step 5600 loss 0.13432
INFO:name:epoch 8 step 5700 loss 0.13721
INFO:name:epoch 8 step 5800 loss 0.13762
INFO:name:epoch 8 step 5900 loss 0.13699
INFO:name:epoch 8 step 6000 loss 0.13143
INFO:name:epoch 8 step 6100 loss 0.15536
INFO:name:epoch 8 step 6200 loss 0.11788
INFO:name:epoch 8 step 6300 loss 0.12781
INFO:name:epoch 8 step 6400 loss 0.13353
INFO:name:epoch 8 step 6500 loss 0.15067
INFO:name:epoch 8 step 6600 loss 0.139
INFO:name:epoch 8 step 6700 loss 0.13486
INFO:name:epoch 8 step 6800 loss 0.12937
INFO:name:epoch 8 step 6900 loss 0.15018
INFO:name:epoch 8 step 7000 loss 0.1572
INFO:name:epoch 8 step 7100 loss 0.12813
INFO:name:epoch 8 step 7200 loss 0.12746
INFO:name:epoch 8 step 7300 loss 0.12563
INFO:name:epoch 8 step 7400 loss 0.12549
INFO:name:epoch 8 step 7500 loss 0.13774
INFO:name:epoch 8 step 7600 loss 0.15306
INFO:name:epoch 8 step 7700 loss 0.12598
INFO:name:epoch 8 step 7800 loss 0.13306
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1504
INFO:name:epoch 9 step 100 loss 0.13054
INFO:name:epoch 9 step 200 loss 0.11707
INFO:name:epoch 9 step 300 loss 0.12902
INFO:name:epoch 9 step 400 loss 0.12775
INFO:name:epoch 9 step 500 loss 0.11875
INFO:name:epoch 9 step 600 loss 0.13599
INFO:name:epoch 9 step 700 loss 0.10338
INFO:name:epoch 9 step 800 loss 0.12736
INFO:name:epoch 9 step 900 loss 0.14707
INFO:name:epoch 9 step 1000 loss 0.12094
INFO:name:epoch 9 step 1100 loss 0.1105
INFO:name:epoch 9 step 1200 loss 0.11695
INFO:name:epoch 9 step 1300 loss 0.11791
INFO:name:epoch 9 step 1400 loss 0.1229
INFO:name:epoch 9 step 1500 loss 0.13612
INFO:name:epoch 9 step 1600 loss 0.13944
INFO:name:epoch 9 step 1700 loss 0.12192
INFO:name:epoch 9 step 1800 loss 0.11484
INFO:name:epoch 9 step 1900 loss 0.1178
INFO:name:epoch 9 step 2000 loss 0.15535
INFO:name:epoch 9 step 2100 loss 0.12411
INFO:name:epoch 9 step 2200 loss 0.13142
INFO:name:epoch 9 step 2300 loss 0.11697
INFO:name:epoch 9 step 2400 loss 0.12477
INFO:name:epoch 9 step 2500 loss 0.11579
INFO:name:epoch 9 step 2600 loss 0.12456
INFO:name:epoch 9 step 2700 loss 0.13962
INFO:name:epoch 9 step 2800 loss 0.12568
INFO:name:epoch 9 step 2900 loss 0.12351
INFO:name:epoch 9 step 3000 loss 0.12577
INFO:name:epoch 9 step 3100 loss 0.12445
INFO:name:epoch 9 step 3200 loss 0.13281
INFO:name:epoch 9 step 3300 loss 0.11736
INFO:name:epoch 9 step 3400 loss 0.12147
INFO:name:epoch 9 step 3500 loss 0.12238
INFO:name:epoch 9 step 3600 loss 0.13902
INFO:name:epoch 9 step 3700 loss 0.10189
INFO:name:epoch 9 step 3800 loss 0.12731
INFO:name:epoch 9 step 3900 loss 0.1241
INFO:name:epoch 9 step 4000 loss 0.11444
INFO:name:epoch 9 step 4100 loss 0.1271
INFO:name:epoch 9 step 4200 loss 0.12557
INFO:name:epoch 9 step 4300 loss 0.12015
INFO:name:epoch 9 step 4400 loss 0.11677
INFO:name:epoch 9 step 4500 loss 0.11629
INFO:name:epoch 9 step 4600 loss 0.12388
INFO:name:epoch 9 step 4700 loss 0.1117
INFO:name:epoch 9 step 4800 loss 0.12744
INFO:name:epoch 9 step 4900 loss 0.13952
INFO:name:epoch 9 step 5000 loss 0.1167
INFO:name:epoch 9 step 5100 loss 0.11914
INFO:name:epoch 9 step 5200 loss 0.12039
INFO:name:epoch 9 step 5300 loss 0.12912
INFO:name:epoch 9 step 5400 loss 0.11999
INFO:name:epoch 9 step 5500 loss 0.14201
INFO:name:epoch 9 step 5600 loss 0.13435
INFO:name:epoch 9 step 5700 loss 0.12835
INFO:name:epoch 9 step 5800 loss 0.1335
INFO:name:epoch 9 step 5900 loss 0.12069
INFO:name:epoch 9 step 6000 loss 0.13326
INFO:name:epoch 9 step 6100 loss 0.11758
INFO:name:epoch 9 step 6200 loss 0.1226
INFO:name:epoch 9 step 6300 loss 0.12546
INFO:name:epoch 9 step 6400 loss 0.13299
INFO:name:epoch 9 step 6500 loss 0.13051
INFO:name:epoch 9 step 6600 loss 0.12845
INFO:name:epoch 9 step 6700 loss 0.12552
INFO:name:epoch 9 step 6800 loss 0.11915
INFO:name:epoch 9 step 6900 loss 0.125
INFO:name:epoch 9 step 7000 loss 0.11369
INFO:name:epoch 9 step 7100 loss 0.1158
INFO:name:epoch 9 step 7200 loss 0.1195
INFO:name:epoch 9 step 7300 loss 0.14368
INFO:name:epoch 9 step 7400 loss 0.11147
INFO:name:epoch 9 step 7500 loss 0.11769
INFO:name:epoch 9 step 7600 loss 0.13301
INFO:name:epoch 9 step 7700 loss 0.12606
INFO:name:epoch 9 step 7800 loss 0.13372
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.1516
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
train results ([1.9975486375476688, 0.6573307274569777, 0.38689575248943425, 0.28459382398690664, 0.229264225409836, 0.19218917721385442, 0.16678881968423048, 0.1465615936112087, 0.1358870689847697, 0.12496019797665901], [0.022604371280081102, 0.0487610205524922, 0.08068398984723787, 0.10010865850047562, 0.12427983244857946, 0.1360043317912048, 0.1419626855252135, 0.15231483338900123, 0.15038462840087863, 0.15163672688752244])
