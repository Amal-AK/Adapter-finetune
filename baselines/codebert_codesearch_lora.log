/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
INFO:name:device: cuda:0, n_gpu: 1
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/codebert-base/resolve/main/config.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/codebert-base/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

INFO:name:Preparing the code search Dataset...

root
├── embeddings (RobertaEmbeddings)
│   ├── word_embeddings (Embedding) weight:[50265, 768]
│   ├── position_embeddings (Embedding) weight:[514, 768]
│   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
├── encoder (RobertaEncoder)
│   └── layer (ModuleList)
│       └── 0-11(RobertaLayer)
│           ├── attention (RobertaAttention)
│           │   ├── self (RobertaSelfAttention)
│           │   │   ├── query,value(Linear) weight:[768, 768] bias:[768]
│           │   │   │   └── lora (LowRankLinear) lora_A:[8, 768] lora_B:[768, 8]
│           │   │   └── key (Linear) weight:[768, 768] bias:[768]
│           │   └── output (RobertaSelfOutput)
│           │       ├── dense (Linear) weight:[768, 768] bias:[768]
│           │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│           ├── intermediate (RobertaIntermediate)
│           │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│           └── output (RobertaOutput)
│               ├── dense (Linear) weight:[768, 3072] bias:[768]
│               └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── pooler (RobertaPooler)
    └── dense (Linear) weight:[768, 768] bias:[768]
[INFO|(OpenDelta)basemodel:700]2025-01-22 01:55:19,236 >> Trainable Ratio: 294912/124940544=0.236042%
[INFO|(OpenDelta)basemodel:702]2025-01-22 01:55:19,236 >> Delta Parameter Ratio: 294912/124940544=0.236042%
[INFO|(OpenDelta)basemodel:704]2025-01-22 01:55:19,236 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/home/aakli/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:name:***** Running training *****
INFO:name:  Num examples = 251820
INFO:name:  Num Epochs = 10
INFO:name:  Instantaneous batch size per GPU = 32
INFO:name:  Total train batch size  = 32
INFO:name:  Total optimization steps = 78700
INFO:name:epoch 0 step 100 loss 3.64102
INFO:name:epoch 0 step 200 loss 1.37346
INFO:name:epoch 0 step 300 loss 0.4546
INFO:name:epoch 0 step 400 loss 0.35015
INFO:name:epoch 0 step 500 loss 0.28295
INFO:name:epoch 0 step 600 loss 0.2642
INFO:name:epoch 0 step 700 loss 0.26196
INFO:name:epoch 0 step 800 loss 0.26352
INFO:name:epoch 0 step 900 loss 0.21824
INFO:name:epoch 0 step 1000 loss 0.23087
INFO:name:epoch 0 step 1100 loss 0.21944
INFO:name:epoch 0 step 1200 loss 0.21684
INFO:name:epoch 0 step 1300 loss 0.20188
INFO:name:epoch 0 step 1400 loss 0.20829
INFO:name:epoch 0 step 1500 loss 0.2057
INFO:name:epoch 0 step 1600 loss 0.20008
INFO:name:epoch 0 step 1700 loss 0.18179
INFO:name:epoch 0 step 1800 loss 0.17753
INFO:name:epoch 0 step 1900 loss 0.17231
INFO:name:epoch 0 step 2000 loss 0.18311
INFO:name:epoch 0 step 2100 loss 0.18973
INFO:name:epoch 0 step 2200 loss 0.17263
INFO:name:epoch 0 step 2300 loss 0.18397
INFO:name:epoch 0 step 2400 loss 0.18121
INFO:name:epoch 0 step 2500 loss 0.18288
INFO:name:epoch 0 step 2600 loss 0.1877
INFO:name:epoch 0 step 2700 loss 0.17294
INFO:name:epoch 0 step 2800 loss 0.1693
INFO:name:epoch 0 step 2900 loss 0.17729
INFO:name:epoch 0 step 3000 loss 0.16207
INFO:name:epoch 0 step 3100 loss 0.16921
INFO:name:epoch 0 step 3200 loss 0.16237
INFO:name:epoch 0 step 3300 loss 0.15348
INFO:name:epoch 0 step 3400 loss 0.17159
INFO:name:epoch 0 step 3500 loss 0.1529
INFO:name:epoch 0 step 3600 loss 0.16436
INFO:name:epoch 0 step 3700 loss 0.15727
INFO:name:epoch 0 step 3800 loss 0.16182
INFO:name:epoch 0 step 3900 loss 0.15926
INFO:name:epoch 0 step 4000 loss 0.17025
INFO:name:epoch 0 step 4100 loss 0.16746
INFO:name:epoch 0 step 4200 loss 0.15814
INFO:name:epoch 0 step 4300 loss 0.15595
INFO:name:epoch 0 step 4400 loss 0.16156
INFO:name:epoch 0 step 4500 loss 0.14547
INFO:name:epoch 0 step 4600 loss 0.17746
INFO:name:epoch 0 step 4700 loss 0.17091
INFO:name:epoch 0 step 4800 loss 0.15926
INFO:name:epoch 0 step 4900 loss 0.15389
INFO:name:epoch 0 step 5000 loss 0.17956
INFO:name:epoch 0 step 5100 loss 0.14219
INFO:name:epoch 0 step 5200 loss 0.16845
INFO:name:epoch 0 step 5300 loss 0.16407
INFO:name:epoch 0 step 5400 loss 0.17617
INFO:name:epoch 0 step 5500 loss 0.13642
INFO:name:epoch 0 step 5600 loss 0.1623
INFO:name:epoch 0 step 5700 loss 0.15103
INFO:name:epoch 0 step 5800 loss 0.16245
INFO:name:epoch 0 step 5900 loss 0.17472
INFO:name:epoch 0 step 6000 loss 0.15354
INFO:name:epoch 0 step 6100 loss 0.14615
INFO:name:epoch 0 step 6200 loss 0.15165
INFO:name:epoch 0 step 6300 loss 0.16483
INFO:name:epoch 0 step 6400 loss 0.16004
INFO:name:epoch 0 step 6500 loss 0.15978
INFO:name:epoch 0 step 6600 loss 0.16321
INFO:name:epoch 0 step 6700 loss 0.14228
INFO:name:epoch 0 step 6800 loss 0.15097
INFO:name:epoch 0 step 6900 loss 0.16094
INFO:name:epoch 0 step 7000 loss 0.1292
INFO:name:epoch 0 step 7100 loss 0.16894
INFO:name:epoch 0 step 7200 loss 0.13639
INFO:name:epoch 0 step 7300 loss 0.1506
INFO:name:epoch 0 step 7400 loss 0.14154
INFO:name:epoch 0 step 7500 loss 0.14916
INFO:name:epoch 0 step 7600 loss 0.13553
INFO:name:epoch 0 step 7700 loss 0.13104
INFO:name:epoch 0 step 7800 loss 0.14832
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3401
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3401
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.2773
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 1 step 100 loss 0.13199
INFO:name:epoch 1 step 200 loss 0.0828
INFO:name:epoch 1 step 300 loss 0.07988
INFO:name:epoch 1 step 400 loss 0.10388
INFO:name:epoch 1 step 500 loss 0.09708
INFO:name:epoch 1 step 600 loss 0.08889
INFO:name:epoch 1 step 700 loss 0.09573
INFO:name:epoch 1 step 800 loss 0.08092
INFO:name:epoch 1 step 900 loss 0.0895
INFO:name:epoch 1 step 1000 loss 0.09836
INFO:name:epoch 1 step 1100 loss 0.08087
INFO:name:epoch 1 step 1200 loss 0.08562
INFO:name:epoch 1 step 1300 loss 0.08962
INFO:name:epoch 1 step 1400 loss 0.09098
INFO:name:epoch 1 step 1500 loss 0.08758
INFO:name:epoch 1 step 1600 loss 0.08939
INFO:name:epoch 1 step 1700 loss 0.08534
INFO:name:epoch 1 step 1800 loss 0.08207
INFO:name:epoch 1 step 1900 loss 0.07312
INFO:name:epoch 1 step 2000 loss 0.07351
INFO:name:epoch 1 step 2100 loss 0.08494
INFO:name:epoch 1 step 2200 loss 0.08739
INFO:name:epoch 1 step 2300 loss 0.08767
INFO:name:epoch 1 step 2400 loss 0.09887
INFO:name:epoch 1 step 2500 loss 0.08937
INFO:name:epoch 1 step 2600 loss 0.0783
INFO:name:epoch 1 step 2700 loss 0.0764
INFO:name:epoch 1 step 2800 loss 0.07978
INFO:name:epoch 1 step 2900 loss 0.09116
INFO:name:epoch 1 step 3000 loss 0.07134
INFO:name:epoch 1 step 3100 loss 0.08055
INFO:name:epoch 1 step 3200 loss 0.07227
INFO:name:epoch 1 step 3300 loss 0.09067
INFO:name:epoch 1 step 3400 loss 0.08516
INFO:name:epoch 1 step 3500 loss 0.08457
INFO:name:epoch 1 step 3600 loss 0.07561
INFO:name:epoch 1 step 3700 loss 0.07483
INFO:name:epoch 1 step 3800 loss 0.08279
INFO:name:epoch 1 step 3900 loss 0.06569
INFO:name:epoch 1 step 4000 loss 0.06962
INFO:name:epoch 1 step 4100 loss 0.07266
INFO:name:epoch 1 step 4200 loss 0.0673
INFO:name:epoch 1 step 4300 loss 0.07811
INFO:name:epoch 1 step 4400 loss 0.09056
INFO:name:epoch 1 step 4500 loss 0.09278
INFO:name:epoch 1 step 4600 loss 0.08337
INFO:name:epoch 1 step 4700 loss 0.08634
INFO:name:epoch 1 step 4800 loss 0.07356
INFO:name:epoch 1 step 4900 loss 0.08667
INFO:name:epoch 1 step 5000 loss 0.07629
INFO:name:epoch 1 step 5100 loss 0.08173
INFO:name:epoch 1 step 5200 loss 0.08393
INFO:name:epoch 1 step 5300 loss 0.07211
INFO:name:epoch 1 step 5400 loss 0.07977
INFO:name:epoch 1 step 5500 loss 0.08259
INFO:name:epoch 1 step 5600 loss 0.0824
INFO:name:epoch 1 step 5700 loss 0.06369
INFO:name:epoch 1 step 5800 loss 0.08342
INFO:name:epoch 1 step 5900 loss 0.0829
INFO:name:epoch 1 step 6000 loss 0.08246
INFO:name:epoch 1 step 6100 loss 0.07525
INFO:name:epoch 1 step 6200 loss 0.07911
INFO:name:epoch 1 step 6300 loss 0.07703
INFO:name:epoch 1 step 6400 loss 0.08137
INFO:name:epoch 1 step 6500 loss 0.08551
INFO:name:epoch 1 step 6600 loss 0.0827
INFO:name:epoch 1 step 6700 loss 0.07587
INFO:name:epoch 1 step 6800 loss 0.07565
INFO:name:epoch 1 step 6900 loss 0.06984
INFO:name:epoch 1 step 7000 loss 0.07976
INFO:name:epoch 1 step 7100 loss 0.07903
INFO:name:epoch 1 step 7200 loss 0.07328
INFO:name:epoch 1 step 7300 loss 0.07194
INFO:name:epoch 1 step 7400 loss 0.07236
INFO:name:epoch 1 step 7500 loss 0.07272
INFO:name:epoch 1 step 7600 loss 0.06598
INFO:name:epoch 1 step 7700 loss 0.07242
INFO:name:epoch 1 step 7800 loss 0.07956
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3677
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3677
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3046
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 2 step 100 loss 0.07349
INFO:name:epoch 2 step 200 loss 0.0755
INFO:name:epoch 2 step 300 loss 0.07074
INFO:name:epoch 2 step 400 loss 0.07416
INFO:name:epoch 2 step 500 loss 0.07227
INFO:name:epoch 2 step 600 loss 0.07814
INFO:name:epoch 2 step 700 loss 0.07047
INFO:name:epoch 2 step 800 loss 0.06846
INFO:name:epoch 2 step 900 loss 0.0688
INFO:name:epoch 2 step 1000 loss 0.07093
INFO:name:epoch 2 step 1100 loss 0.06496
INFO:name:epoch 2 step 1200 loss 0.08007
INFO:name:epoch 2 step 1300 loss 0.07478
INFO:name:epoch 2 step 1400 loss 0.07842
INFO:name:epoch 2 step 1500 loss 0.06583
INFO:name:epoch 2 step 1600 loss 0.06638
INFO:name:epoch 2 step 1700 loss 0.07122
INFO:name:epoch 2 step 1800 loss 0.06044
INFO:name:epoch 2 step 1900 loss 0.06753
INFO:name:epoch 2 step 2000 loss 0.06705
INFO:name:epoch 2 step 2100 loss 0.07022
INFO:name:epoch 2 step 2200 loss 0.08567
INFO:name:epoch 2 step 2300 loss 0.07104
INFO:name:epoch 2 step 2400 loss 0.05557
INFO:name:epoch 2 step 2500 loss 0.0652
INFO:name:epoch 2 step 2600 loss 0.08145
INFO:name:epoch 2 step 2700 loss 0.06476
INFO:name:epoch 2 step 2800 loss 0.08358
INFO:name:epoch 2 step 2900 loss 0.07501
INFO:name:epoch 2 step 3000 loss 0.07672
INFO:name:epoch 2 step 3100 loss 0.07099
INFO:name:epoch 2 step 3200 loss 0.06697
INFO:name:epoch 2 step 3300 loss 0.07022
INFO:name:epoch 2 step 3400 loss 0.07042
INFO:name:epoch 2 step 3500 loss 0.06777
INFO:name:epoch 2 step 3600 loss 0.06858
INFO:name:epoch 2 step 3700 loss 0.07133
INFO:name:epoch 2 step 3800 loss 0.07924
INFO:name:epoch 2 step 3900 loss 0.06538
INFO:name:epoch 2 step 4000 loss 0.06762
INFO:name:epoch 2 step 4100 loss 0.07331
INFO:name:epoch 2 step 4200 loss 0.0718
INFO:name:epoch 2 step 4300 loss 0.07101
INFO:name:epoch 2 step 4400 loss 0.07605
INFO:name:epoch 2 step 4500 loss 0.06926
INFO:name:epoch 2 step 4600 loss 0.07878
INFO:name:epoch 2 step 4700 loss 0.07704
INFO:name:epoch 2 step 4800 loss 0.07229
INFO:name:epoch 2 step 4900 loss 0.06485
INFO:name:epoch 2 step 5000 loss 0.06647
INFO:name:epoch 2 step 5100 loss 0.06529
INFO:name:epoch 2 step 5200 loss 0.05681
INFO:name:epoch 2 step 5300 loss 0.07111
INFO:name:epoch 2 step 5400 loss 0.06626
INFO:name:epoch 2 step 5500 loss 0.08085
INFO:name:epoch 2 step 5600 loss 0.05966
INFO:name:epoch 2 step 5700 loss 0.0668
INFO:name:epoch 2 step 5800 loss 0.07191
INFO:name:epoch 2 step 5900 loss 0.06074
INFO:name:epoch 2 step 6000 loss 0.07762
INFO:name:epoch 2 step 6100 loss 0.07288
INFO:name:epoch 2 step 6200 loss 0.06703
INFO:name:epoch 2 step 6300 loss 0.06197
INFO:name:epoch 2 step 6400 loss 0.07621
INFO:name:epoch 2 step 6500 loss 0.07607
INFO:name:epoch 2 step 6600 loss 0.07265
INFO:name:epoch 2 step 6700 loss 0.06581
INFO:name:epoch 2 step 6800 loss 0.07531
INFO:name:epoch 2 step 6900 loss 0.05689
INFO:name:epoch 2 step 7000 loss 0.06594
INFO:name:epoch 2 step 7100 loss 0.06077
INFO:name:epoch 2 step 7200 loss 0.07083
INFO:name:epoch 2 step 7300 loss 0.08338
INFO:name:epoch 2 step 7400 loss 0.07021
INFO:name:epoch 2 step 7500 loss 0.06599
INFO:name:epoch 2 step 7600 loss 0.06761
INFO:name:epoch 2 step 7700 loss 0.07191
INFO:name:epoch 2 step 7800 loss 0.05707
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3714
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3714
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3092
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 3 step 100 loss 0.0645
INFO:name:epoch 3 step 200 loss 0.05399
INFO:name:epoch 3 step 300 loss 0.06615
INFO:name:epoch 3 step 400 loss 0.06573
INFO:name:epoch 3 step 500 loss 0.06101
INFO:name:epoch 3 step 600 loss 0.06789
INFO:name:epoch 3 step 700 loss 0.07304
INFO:name:epoch 3 step 800 loss 0.06047
INFO:name:epoch 3 step 900 loss 0.06078
INFO:name:epoch 3 step 1000 loss 0.0723
INFO:name:epoch 3 step 1100 loss 0.05866
INFO:name:epoch 3 step 1200 loss 0.07224
INFO:name:epoch 3 step 1300 loss 0.06668
INFO:name:epoch 3 step 1400 loss 0.05998
INFO:name:epoch 3 step 1500 loss 0.06485
INFO:name:epoch 3 step 1600 loss 0.05539
INFO:name:epoch 3 step 1700 loss 0.06603
INFO:name:epoch 3 step 1800 loss 0.06994
INFO:name:epoch 3 step 1900 loss 0.06886
INFO:name:epoch 3 step 2000 loss 0.06095
INFO:name:epoch 3 step 2100 loss 0.05334
INFO:name:epoch 3 step 2200 loss 0.06865
INFO:name:epoch 3 step 2300 loss 0.06414
INFO:name:epoch 3 step 2400 loss 0.05688
INFO:name:epoch 3 step 2500 loss 0.06007
INFO:name:epoch 3 step 2600 loss 0.07387
INFO:name:epoch 3 step 2700 loss 0.06455
INFO:name:epoch 3 step 2800 loss 0.07065
INFO:name:epoch 3 step 2900 loss 0.06461
INFO:name:epoch 3 step 3000 loss 0.06595
INFO:name:epoch 3 step 3100 loss 0.06587
INFO:name:epoch 3 step 3200 loss 0.05695
INFO:name:epoch 3 step 3300 loss 0.06458
INFO:name:epoch 3 step 3400 loss 0.06084
INFO:name:epoch 3 step 3500 loss 0.06237
INFO:name:epoch 3 step 3600 loss 0.05846
INFO:name:epoch 3 step 3700 loss 0.06826
INFO:name:epoch 3 step 3800 loss 0.06551
INFO:name:epoch 3 step 3900 loss 0.0605
INFO:name:epoch 3 step 4000 loss 0.06077
INFO:name:epoch 3 step 4100 loss 0.06464
INFO:name:epoch 3 step 4200 loss 0.06787
INFO:name:epoch 3 step 4300 loss 0.08535
INFO:name:epoch 3 step 4400 loss 0.05258
INFO:name:epoch 3 step 4500 loss 0.06544
INFO:name:epoch 3 step 4600 loss 0.05719
INFO:name:epoch 3 step 4700 loss 0.07567
INFO:name:epoch 3 step 4800 loss 0.05508
INFO:name:epoch 3 step 4900 loss 0.06352
INFO:name:epoch 3 step 5000 loss 0.05805
INFO:name:epoch 3 step 5100 loss 0.06778
INFO:name:epoch 3 step 5200 loss 0.07534
INFO:name:epoch 3 step 5300 loss 0.06895
INFO:name:epoch 3 step 5400 loss 0.06584
INFO:name:epoch 3 step 5500 loss 0.05354
INFO:name:epoch 3 step 5600 loss 0.06042
INFO:name:epoch 3 step 5700 loss 0.0572
INFO:name:epoch 3 step 5800 loss 0.06202
INFO:name:epoch 3 step 5900 loss 0.06845
INFO:name:epoch 3 step 6000 loss 0.0679
INFO:name:epoch 3 step 6100 loss 0.0582
INFO:name:epoch 3 step 6200 loss 0.06005
INFO:name:epoch 3 step 6300 loss 0.06187
INFO:name:epoch 3 step 6400 loss 0.05957
INFO:name:epoch 3 step 6500 loss 0.05859
INFO:name:epoch 3 step 6600 loss 0.07451
INFO:name:epoch 3 step 6700 loss 0.06418
INFO:name:epoch 3 step 6800 loss 0.06377
INFO:name:epoch 3 step 6900 loss 0.06568
INFO:name:epoch 3 step 7000 loss 0.06484
INFO:name:epoch 3 step 7100 loss 0.054
INFO:name:epoch 3 step 7200 loss 0.06707
INFO:name:epoch 3 step 7300 loss 0.07901
INFO:name:epoch 3 step 7400 loss 0.06618
INFO:name:epoch 3 step 7500 loss 0.06096
INFO:name:epoch 3 step 7600 loss 0.07584
INFO:name:epoch 3 step 7700 loss 0.07166
INFO:name:epoch 3 step 7800 loss 0.05586
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3666
INFO:name:epoch 4 step 100 loss 0.05548
INFO:name:epoch 4 step 200 loss 0.05222
INFO:name:epoch 4 step 300 loss 0.07378
INFO:name:epoch 4 step 400 loss 0.06047
INFO:name:epoch 4 step 500 loss 0.04739
INFO:name:epoch 4 step 600 loss 0.0612
INFO:name:epoch 4 step 700 loss 0.07323
INFO:name:epoch 4 step 800 loss 0.06438
INFO:name:epoch 4 step 900 loss 0.05637
INFO:name:epoch 4 step 1000 loss 0.06335
INFO:name:epoch 4 step 1100 loss 0.0549
INFO:name:epoch 4 step 1200 loss 0.05907
INFO:name:epoch 4 step 1300 loss 0.06335
INFO:name:epoch 4 step 1400 loss 0.05442
INFO:name:epoch 4 step 1500 loss 0.05174
INFO:name:epoch 4 step 1600 loss 0.05494
INFO:name:epoch 4 step 1700 loss 0.06065
INFO:name:epoch 4 step 1800 loss 0.06302
INFO:name:epoch 4 step 1900 loss 0.0585
INFO:name:epoch 4 step 2000 loss 0.06671
INFO:name:epoch 4 step 2100 loss 0.05854
INFO:name:epoch 4 step 2200 loss 0.05542
INFO:name:epoch 4 step 2300 loss 0.04975
INFO:name:epoch 4 step 2400 loss 0.06625
INFO:name:epoch 4 step 2500 loss 0.06162
INFO:name:epoch 4 step 2600 loss 0.06021
INFO:name:epoch 4 step 2700 loss 0.06477
INFO:name:epoch 4 step 2800 loss 0.056
INFO:name:epoch 4 step 2900 loss 0.05851
INFO:name:epoch 4 step 3000 loss 0.06308
INFO:name:epoch 4 step 3100 loss 0.06431
INFO:name:epoch 4 step 3200 loss 0.05323
INFO:name:epoch 4 step 3300 loss 0.05294
INFO:name:epoch 4 step 3400 loss 0.05331
INFO:name:epoch 4 step 3500 loss 0.05709
INFO:name:epoch 4 step 3600 loss 0.0635
INFO:name:epoch 4 step 3700 loss 0.0582
INFO:name:epoch 4 step 3800 loss 0.06028
INFO:name:epoch 4 step 3900 loss 0.05129
INFO:name:epoch 4 step 4000 loss 0.05437
INFO:name:epoch 4 step 4100 loss 0.04603
INFO:name:epoch 4 step 4200 loss 0.06046
INFO:name:epoch 4 step 4300 loss 0.05699
INFO:name:epoch 4 step 4400 loss 0.05035
INFO:name:epoch 4 step 4500 loss 0.0606
INFO:name:epoch 4 step 4600 loss 0.05475
INFO:name:epoch 4 step 4700 loss 0.06182
INFO:name:epoch 4 step 4800 loss 0.06682
INFO:name:epoch 4 step 4900 loss 0.06593
INFO:name:epoch 4 step 5000 loss 0.06531
INFO:name:epoch 4 step 5100 loss 0.0444
INFO:name:epoch 4 step 5200 loss 0.05671
INFO:name:epoch 4 step 5300 loss 0.05161
INFO:name:epoch 4 step 5400 loss 0.05639
INFO:name:epoch 4 step 5500 loss 0.05999
INFO:name:epoch 4 step 5600 loss 0.06255
INFO:name:epoch 4 step 5700 loss 0.04858
INFO:name:epoch 4 step 5800 loss 0.0699
INFO:name:epoch 4 step 5900 loss 0.0677
INFO:name:epoch 4 step 6000 loss 0.0558
INFO:name:epoch 4 step 6100 loss 0.05787
INFO:name:epoch 4 step 6200 loss 0.05022
INFO:name:epoch 4 step 6300 loss 0.06081
INFO:name:epoch 4 step 6400 loss 0.06569
INFO:name:epoch 4 step 6500 loss 0.06399
INFO:name:epoch 4 step 6600 loss 0.05844
INFO:name:epoch 4 step 6700 loss 0.06103
INFO:name:epoch 4 step 6800 loss 0.05734
INFO:name:epoch 4 step 6900 loss 0.05299
INFO:name:epoch 4 step 7000 loss 0.06406
INFO:name:epoch 4 step 7100 loss 0.05246
INFO:name:epoch 4 step 7200 loss 0.06622
INFO:name:epoch 4 step 7300 loss 0.06841
INFO:name:epoch 4 step 7400 loss 0.06842
INFO:name:epoch 4 step 7500 loss 0.05262
INFO:name:epoch 4 step 7600 loss 0.06294
INFO:name:epoch 4 step 7700 loss 0.06679
INFO:name:epoch 4 step 7800 loss 0.0517
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.368
INFO:name:epoch 5 step 100 loss 0.05432
INFO:name:epoch 5 step 200 loss 0.04905
INFO:name:epoch 5 step 300 loss 0.04695
INFO:name:epoch 5 step 400 loss 0.05508
INFO:name:epoch 5 step 500 loss 0.04982
INFO:name:epoch 5 step 600 loss 0.05375
INFO:name:epoch 5 step 700 loss 0.06131
INFO:name:epoch 5 step 800 loss 0.06662
INFO:name:epoch 5 step 900 loss 0.05517
INFO:name:epoch 5 step 1000 loss 0.05385
INFO:name:epoch 5 step 1100 loss 0.05029
INFO:name:epoch 5 step 1200 loss 0.05826
INFO:name:epoch 5 step 1300 loss 0.05709
INFO:name:epoch 5 step 1400 loss 0.05701
INFO:name:epoch 5 step 1500 loss 0.06804
INFO:name:epoch 5 step 1600 loss 0.05824
INFO:name:epoch 5 step 1700 loss 0.06498
INFO:name:epoch 5 step 1800 loss 0.05198
INFO:name:epoch 5 step 1900 loss 0.05134
INFO:name:epoch 5 step 2000 loss 0.05335
INFO:name:epoch 5 step 2100 loss 0.04789
INFO:name:epoch 5 step 2200 loss 0.05583
INFO:name:epoch 5 step 2300 loss 0.06226
INFO:name:epoch 5 step 2400 loss 0.0637
INFO:name:epoch 5 step 2500 loss 0.06708
INFO:name:epoch 5 step 2600 loss 0.057
INFO:name:epoch 5 step 2700 loss 0.05525
INFO:name:epoch 5 step 2800 loss 0.05923
INFO:name:epoch 5 step 2900 loss 0.05979
INFO:name:epoch 5 step 3000 loss 0.05824
INFO:name:epoch 5 step 3100 loss 0.05362
INFO:name:epoch 5 step 3200 loss 0.06205
INFO:name:epoch 5 step 3300 loss 0.05639
INFO:name:epoch 5 step 3400 loss 0.04846
INFO:name:epoch 5 step 3500 loss 0.06119
INFO:name:epoch 5 step 3600 loss 0.06166
INFO:name:epoch 5 step 3700 loss 0.0654
INFO:name:epoch 5 step 3800 loss 0.05655
INFO:name:epoch 5 step 3900 loss 0.05582
INFO:name:epoch 5 step 4000 loss 0.06056
INFO:name:epoch 5 step 4100 loss 0.0535
INFO:name:epoch 5 step 4200 loss 0.05373
INFO:name:epoch 5 step 4300 loss 0.05064
INFO:name:epoch 5 step 4400 loss 0.04756
INFO:name:epoch 5 step 4500 loss 0.04021
INFO:name:epoch 5 step 4600 loss 0.0585
INFO:name:epoch 5 step 4700 loss 0.04753
INFO:name:epoch 5 step 4800 loss 0.06999
INFO:name:epoch 5 step 4900 loss 0.05731
INFO:name:epoch 5 step 5000 loss 0.05273
INFO:name:epoch 5 step 5100 loss 0.05098
INFO:name:epoch 5 step 5200 loss 0.05371
INFO:name:epoch 5 step 5300 loss 0.04851
INFO:name:epoch 5 step 5400 loss 0.04777
INFO:name:epoch 5 step 5500 loss 0.0577
INFO:name:epoch 5 step 5600 loss 0.06647
INFO:name:epoch 5 step 5700 loss 0.05902
INFO:name:epoch 5 step 5800 loss 0.06173
INFO:name:epoch 5 step 5900 loss 0.04783
INFO:name:epoch 5 step 6000 loss 0.05518
INFO:name:epoch 5 step 6100 loss 0.05946
INFO:name:epoch 5 step 6200 loss 0.05606
INFO:name:epoch 5 step 6300 loss 0.06975
INFO:name:epoch 5 step 6400 loss 0.05293
INFO:name:epoch 5 step 6500 loss 0.06309
INFO:name:epoch 5 step 6600 loss 0.04772
INFO:name:epoch 5 step 6700 loss 0.0509
INFO:name:epoch 5 step 6800 loss 0.04487
INFO:name:epoch 5 step 6900 loss 0.0542
INFO:name:epoch 5 step 7000 loss 0.05773
INFO:name:epoch 5 step 7100 loss 0.05166
INFO:name:epoch 5 step 7200 loss 0.05632
INFO:name:epoch 5 step 7300 loss 0.05421
INFO:name:epoch 5 step 7400 loss 0.06284
INFO:name:epoch 5 step 7500 loss 0.04762
INFO:name:epoch 5 step 7600 loss 0.05686
INFO:name:epoch 5 step 7700 loss 0.05063
INFO:name:epoch 5 step 7800 loss 0.06297
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3744
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3744
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3113
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 6 step 100 loss 0.05454
INFO:name:epoch 6 step 200 loss 0.05256
INFO:name:epoch 6 step 300 loss 0.05977
INFO:name:epoch 6 step 400 loss 0.0531
INFO:name:epoch 6 step 500 loss 0.05364
INFO:name:epoch 6 step 600 loss 0.04335
INFO:name:epoch 6 step 700 loss 0.05262
INFO:name:epoch 6 step 800 loss 0.06608
INFO:name:epoch 6 step 900 loss 0.05717
INFO:name:epoch 6 step 1000 loss 0.06842
INFO:name:epoch 6 step 1100 loss 0.04841
INFO:name:epoch 6 step 1200 loss 0.0446
INFO:name:epoch 6 step 1300 loss 0.04812
INFO:name:epoch 6 step 1400 loss 0.04708
INFO:name:epoch 6 step 1500 loss 0.06349
INFO:name:epoch 6 step 1600 loss 0.04861
INFO:name:epoch 6 step 1700 loss 0.05305
INFO:name:epoch 6 step 1800 loss 0.04852
INFO:name:epoch 6 step 1900 loss 0.05337
INFO:name:epoch 6 step 2000 loss 0.04476
INFO:name:epoch 6 step 2100 loss 0.05628
INFO:name:epoch 6 step 2200 loss 0.0546
INFO:name:epoch 6 step 2300 loss 0.05459
INFO:name:epoch 6 step 2400 loss 0.04123
INFO:name:epoch 6 step 2500 loss 0.04617
INFO:name:epoch 6 step 2600 loss 0.05552
INFO:name:epoch 6 step 2700 loss 0.05159
INFO:name:epoch 6 step 2800 loss 0.04818
INFO:name:epoch 6 step 2900 loss 0.05317
INFO:name:epoch 6 step 3000 loss 0.05674
INFO:name:epoch 6 step 3100 loss 0.05332
INFO:name:epoch 6 step 3200 loss 0.04889
INFO:name:epoch 6 step 3300 loss 0.05739
INFO:name:epoch 6 step 3400 loss 0.05016
INFO:name:epoch 6 step 3500 loss 0.05777
INFO:name:epoch 6 step 3600 loss 0.05694
INFO:name:epoch 6 step 3700 loss 0.05158
INFO:name:epoch 6 step 3800 loss 0.05077
INFO:name:epoch 6 step 3900 loss 0.04889
INFO:name:epoch 6 step 4000 loss 0.04508
INFO:name:epoch 6 step 4100 loss 0.05971
INFO:name:epoch 6 step 4200 loss 0.05124
INFO:name:epoch 6 step 4300 loss 0.05066
INFO:name:epoch 6 step 4400 loss 0.04687
INFO:name:epoch 6 step 4500 loss 0.0542
INFO:name:epoch 6 step 4600 loss 0.05607
INFO:name:epoch 6 step 4700 loss 0.0449
INFO:name:epoch 6 step 4800 loss 0.05444
INFO:name:epoch 6 step 4900 loss 0.04766
INFO:name:epoch 6 step 5000 loss 0.05436
INFO:name:epoch 6 step 5100 loss 0.0556
INFO:name:epoch 6 step 5200 loss 0.05469
INFO:name:epoch 6 step 5300 loss 0.04978
INFO:name:epoch 6 step 5400 loss 0.0547
INFO:name:epoch 6 step 5500 loss 0.05495
INFO:name:epoch 6 step 5600 loss 0.05201
INFO:name:epoch 6 step 5700 loss 0.05252
INFO:name:epoch 6 step 5800 loss 0.06713
INFO:name:epoch 6 step 5900 loss 0.0475
INFO:name:epoch 6 step 6000 loss 0.04214
INFO:name:epoch 6 step 6100 loss 0.05153
INFO:name:epoch 6 step 6200 loss 0.0603
INFO:name:epoch 6 step 6300 loss 0.04516
INFO:name:epoch 6 step 6400 loss 0.05117
INFO:name:epoch 6 step 6500 loss 0.05422
INFO:name:epoch 6 step 6600 loss 0.05529
INFO:name:epoch 6 step 6700 loss 0.04322
INFO:name:epoch 6 step 6800 loss 0.0484
INFO:name:epoch 6 step 6900 loss 0.05373
INFO:name:epoch 6 step 7000 loss 0.04693
INFO:name:epoch 6 step 7100 loss 0.06153
INFO:name:epoch 6 step 7200 loss 0.05554
INFO:name:epoch 6 step 7300 loss 0.0585
INFO:name:epoch 6 step 7400 loss 0.05459
INFO:name:epoch 6 step 7500 loss 0.05621
INFO:name:epoch 6 step 7600 loss 0.04648
INFO:name:epoch 6 step 7700 loss 0.05273
INFO:name:epoch 6 step 7800 loss 0.05503
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3736
INFO:name:epoch 7 step 100 loss 0.05532
INFO:name:epoch 7 step 200 loss 0.04677
INFO:name:epoch 7 step 300 loss 0.04256
INFO:name:epoch 7 step 400 loss 0.05295
INFO:name:epoch 7 step 500 loss 0.05349
INFO:name:epoch 7 step 600 loss 0.04775
INFO:name:epoch 7 step 700 loss 0.05788
INFO:name:epoch 7 step 800 loss 0.04989
INFO:name:epoch 7 step 900 loss 0.04528
INFO:name:epoch 7 step 1000 loss 0.04821
INFO:name:epoch 7 step 1100 loss 0.05074
INFO:name:epoch 7 step 1200 loss 0.04485
INFO:name:epoch 7 step 1300 loss 0.04353
INFO:name:epoch 7 step 1400 loss 0.04395
INFO:name:epoch 7 step 1500 loss 0.05351
INFO:name:epoch 7 step 1600 loss 0.0589
INFO:name:epoch 7 step 1700 loss 0.06298
INFO:name:epoch 7 step 1800 loss 0.04685
INFO:name:epoch 7 step 1900 loss 0.04146
INFO:name:epoch 7 step 2000 loss 0.04866
INFO:name:epoch 7 step 2100 loss 0.06336
INFO:name:epoch 7 step 2200 loss 0.04379
INFO:name:epoch 7 step 2300 loss 0.04908
INFO:name:epoch 7 step 2400 loss 0.03859
INFO:name:epoch 7 step 2500 loss 0.04763
INFO:name:epoch 7 step 2600 loss 0.05458
INFO:name:epoch 7 step 2700 loss 0.06039
INFO:name:epoch 7 step 2800 loss 0.04702
INFO:name:epoch 7 step 2900 loss 0.06124
INFO:name:epoch 7 step 3000 loss 0.05142
INFO:name:epoch 7 step 3100 loss 0.04577
INFO:name:epoch 7 step 3200 loss 0.05391
INFO:name:epoch 7 step 3300 loss 0.03789
INFO:name:epoch 7 step 3400 loss 0.05222
INFO:name:epoch 7 step 3500 loss 0.04524
INFO:name:epoch 7 step 3600 loss 0.04181
INFO:name:epoch 7 step 3700 loss 0.04739
INFO:name:epoch 7 step 3800 loss 0.05102
INFO:name:epoch 7 step 3900 loss 0.04753
INFO:name:epoch 7 step 4000 loss 0.04651
INFO:name:epoch 7 step 4100 loss 0.05151
INFO:name:epoch 7 step 4200 loss 0.05388
INFO:name:epoch 7 step 4300 loss 0.04929
INFO:name:epoch 7 step 4400 loss 0.05408
INFO:name:epoch 7 step 4500 loss 0.03933
INFO:name:epoch 7 step 4600 loss 0.05616
INFO:name:epoch 7 step 4700 loss 0.04274
INFO:name:epoch 7 step 4800 loss 0.04477
INFO:name:epoch 7 step 4900 loss 0.05044
INFO:name:epoch 7 step 5000 loss 0.04751
INFO:name:epoch 7 step 5100 loss 0.04607
INFO:name:epoch 7 step 5200 loss 0.05983
INFO:name:epoch 7 step 5300 loss 0.05603
INFO:name:epoch 7 step 5400 loss 0.05251
INFO:name:epoch 7 step 5500 loss 0.05512
INFO:name:epoch 7 step 5600 loss 0.05154
INFO:name:epoch 7 step 5700 loss 0.04866
INFO:name:epoch 7 step 5800 loss 0.05144
INFO:name:epoch 7 step 5900 loss 0.04677
INFO:name:epoch 7 step 6000 loss 0.0438
INFO:name:epoch 7 step 6100 loss 0.05301
INFO:name:epoch 7 step 6200 loss 0.05447
INFO:name:epoch 7 step 6300 loss 0.05732
INFO:name:epoch 7 step 6400 loss 0.05356
INFO:name:epoch 7 step 6500 loss 0.05522
INFO:name:epoch 7 step 6600 loss 0.05397
INFO:name:epoch 7 step 6700 loss 0.05004
INFO:name:epoch 7 step 6800 loss 0.05036
INFO:name:epoch 7 step 6900 loss 0.04733
INFO:name:epoch 7 step 7000 loss 0.03918
INFO:name:epoch 7 step 7100 loss 0.05554
INFO:name:epoch 7 step 7200 loss 0.04805
INFO:name:epoch 7 step 7300 loss 0.0431
INFO:name:epoch 7 step 7400 loss 0.05008
INFO:name:epoch 7 step 7500 loss 0.05863
INFO:name:epoch 7 step 7600 loss 0.06255
INFO:name:epoch 7 step 7700 loss 0.05115
INFO:name:epoch 7 step 7800 loss 0.04557
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3727
INFO:name:epoch 8 step 100 loss 0.04924
INFO:name:epoch 8 step 200 loss 0.03453
INFO:name:epoch 8 step 300 loss 0.04896
INFO:name:epoch 8 step 400 loss 0.0514
INFO:name:epoch 8 step 500 loss 0.04553
INFO:name:epoch 8 step 600 loss 0.04115
INFO:name:epoch 8 step 700 loss 0.05421
INFO:name:epoch 8 step 800 loss 0.05109
INFO:name:epoch 8 step 900 loss 0.05026
INFO:name:epoch 8 step 1000 loss 0.05126
INFO:name:epoch 8 step 1100 loss 0.05512
INFO:name:epoch 8 step 1200 loss 0.04449
INFO:name:epoch 8 step 1300 loss 0.05085
INFO:name:epoch 8 step 1400 loss 0.04719
INFO:name:epoch 8 step 1500 loss 0.04236
INFO:name:epoch 8 step 1600 loss 0.04598
INFO:name:epoch 8 step 1700 loss 0.0491
INFO:name:epoch 8 step 1800 loss 0.04902
INFO:name:epoch 8 step 1900 loss 0.05678
INFO:name:epoch 8 step 2000 loss 0.03398
INFO:name:epoch 8 step 2100 loss 0.05161
INFO:name:epoch 8 step 2200 loss 0.04215
INFO:name:epoch 8 step 2300 loss 0.05102
INFO:name:epoch 8 step 2400 loss 0.05933
INFO:name:epoch 8 step 2500 loss 0.05062
INFO:name:epoch 8 step 2600 loss 0.05302
INFO:name:epoch 8 step 2700 loss 0.05234
INFO:name:epoch 8 step 2800 loss 0.03862
INFO:name:epoch 8 step 2900 loss 0.05864
INFO:name:epoch 8 step 3000 loss 0.05021
INFO:name:epoch 8 step 3100 loss 0.05695
INFO:name:epoch 8 step 3200 loss 0.04563
INFO:name:epoch 8 step 3300 loss 0.04388
INFO:name:epoch 8 step 3400 loss 0.04912
INFO:name:epoch 8 step 3500 loss 0.04277
INFO:name:epoch 8 step 3600 loss 0.04899
INFO:name:epoch 8 step 3700 loss 0.04153
INFO:name:epoch 8 step 3800 loss 0.05485
INFO:name:epoch 8 step 3900 loss 0.05368
INFO:name:epoch 8 step 4000 loss 0.05089
INFO:name:epoch 8 step 4100 loss 0.04316
INFO:name:epoch 8 step 4200 loss 0.05033
INFO:name:epoch 8 step 4300 loss 0.0488
INFO:name:epoch 8 step 4400 loss 0.04431
INFO:name:epoch 8 step 4500 loss 0.04772
INFO:name:epoch 8 step 4600 loss 0.04178
INFO:name:epoch 8 step 4700 loss 0.03666
INFO:name:epoch 8 step 4800 loss 0.05236
INFO:name:epoch 8 step 4900 loss 0.04361
INFO:name:epoch 8 step 5000 loss 0.03494
INFO:name:epoch 8 step 5100 loss 0.04396
INFO:name:epoch 8 step 5200 loss 0.0377
INFO:name:epoch 8 step 5300 loss 0.05257
INFO:name:epoch 8 step 5400 loss 0.05128
INFO:name:epoch 8 step 5500 loss 0.03703
INFO:name:epoch 8 step 5600 loss 0.05763
INFO:name:epoch 8 step 5700 loss 0.04562
INFO:name:epoch 8 step 5800 loss 0.05044
INFO:name:epoch 8 step 5900 loss 0.04764
INFO:name:epoch 8 step 6000 loss 0.05258
INFO:name:epoch 8 step 6100 loss 0.0506
INFO:name:epoch 8 step 6200 loss 0.04605
INFO:name:epoch 8 step 6300 loss 0.04528
INFO:name:epoch 8 step 6400 loss 0.04141
INFO:name:epoch 8 step 6500 loss 0.0482
INFO:name:epoch 8 step 6600 loss 0.04756
INFO:name:epoch 8 step 6700 loss 0.04734
INFO:name:epoch 8 step 6800 loss 0.04479
INFO:name:epoch 8 step 6900 loss 0.04927
INFO:name:epoch 8 step 7000 loss 0.04671
INFO:name:epoch 8 step 7100 loss 0.06094
INFO:name:epoch 8 step 7200 loss 0.0487
INFO:name:epoch 8 step 7300 loss 0.03885
INFO:name:epoch 8 step 7400 loss 0.0573
INFO:name:epoch 8 step 7500 loss 0.04606
INFO:name:epoch 8 step 7600 loss 0.055
INFO:name:epoch 8 step 7700 loss 0.05915
INFO:name:epoch 8 step 7800 loss 0.03873
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3749
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3749
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3129
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:epoch 9 step 100 loss 0.04597
INFO:name:epoch 9 step 200 loss 0.04337
INFO:name:epoch 9 step 300 loss 0.03862
INFO:name:epoch 9 step 400 loss 0.04269
INFO:name:epoch 9 step 500 loss 0.0461
INFO:name:epoch 9 step 600 loss 0.04079
INFO:name:epoch 9 step 700 loss 0.04951
INFO:name:epoch 9 step 800 loss 0.05179
INFO:name:epoch 9 step 900 loss 0.04198
INFO:name:epoch 9 step 1000 loss 0.05707
INFO:name:epoch 9 step 1100 loss 0.0406
INFO:name:epoch 9 step 1200 loss 0.05207
INFO:name:epoch 9 step 1300 loss 0.04936
INFO:name:epoch 9 step 1400 loss 0.06234
INFO:name:epoch 9 step 1500 loss 0.04741
INFO:name:epoch 9 step 1600 loss 0.04067
INFO:name:epoch 9 step 1700 loss 0.04785
INFO:name:epoch 9 step 1800 loss 0.03968
INFO:name:epoch 9 step 1900 loss 0.04585
INFO:name:epoch 9 step 2000 loss 0.04521
INFO:name:epoch 9 step 2100 loss 0.04214
INFO:name:epoch 9 step 2200 loss 0.04193
INFO:name:epoch 9 step 2300 loss 0.05321
INFO:name:epoch 9 step 2400 loss 0.04055
INFO:name:epoch 9 step 2500 loss 0.04904
INFO:name:epoch 9 step 2600 loss 0.03544
INFO:name:epoch 9 step 2700 loss 0.04013
INFO:name:epoch 9 step 2800 loss 0.04828
INFO:name:epoch 9 step 2900 loss 0.04183
INFO:name:epoch 9 step 3000 loss 0.04909
INFO:name:epoch 9 step 3100 loss 0.04514
INFO:name:epoch 9 step 3200 loss 0.04431
INFO:name:epoch 9 step 3300 loss 0.04868
INFO:name:epoch 9 step 3400 loss 0.05064
INFO:name:epoch 9 step 3500 loss 0.04264
INFO:name:epoch 9 step 3600 loss 0.04592
INFO:name:epoch 9 step 3700 loss 0.0459
INFO:name:epoch 9 step 3800 loss 0.0437
INFO:name:epoch 9 step 3900 loss 0.05575
INFO:name:epoch 9 step 4000 loss 0.04174
INFO:name:epoch 9 step 4100 loss 0.04315
INFO:name:epoch 9 step 4200 loss 0.04301
INFO:name:epoch 9 step 4300 loss 0.04286
INFO:name:epoch 9 step 4400 loss 0.04678
INFO:name:epoch 9 step 4500 loss 0.05464
INFO:name:epoch 9 step 4600 loss 0.04752
INFO:name:epoch 9 step 4700 loss 0.03928
INFO:name:epoch 9 step 4800 loss 0.04916
INFO:name:epoch 9 step 4900 loss 0.04706
INFO:name:epoch 9 step 5000 loss 0.04963
INFO:name:epoch 9 step 5100 loss 0.04769
INFO:name:epoch 9 step 5200 loss 0.04307
INFO:name:epoch 9 step 5300 loss 0.0398
INFO:name:epoch 9 step 5400 loss 0.046
INFO:name:epoch 9 step 5500 loss 0.04542
INFO:name:epoch 9 step 5600 loss 0.05683
INFO:name:epoch 9 step 5700 loss 0.04684
INFO:name:epoch 9 step 5800 loss 0.04505
INFO:name:epoch 9 step 5900 loss 0.04789
INFO:name:epoch 9 step 6000 loss 0.0465
INFO:name:epoch 9 step 6100 loss 0.04211
INFO:name:epoch 9 step 6200 loss 0.05168
INFO:name:epoch 9 step 6300 loss 0.04477
INFO:name:epoch 9 step 6400 loss 0.04092
INFO:name:epoch 9 step 6500 loss 0.04944
INFO:name:epoch 9 step 6600 loss 0.05827
INFO:name:epoch 9 step 6700 loss 0.05674
INFO:name:epoch 9 step 6800 loss 0.05203
INFO:name:epoch 9 step 6900 loss 0.04655
INFO:name:epoch 9 step 7000 loss 0.04366
INFO:name:epoch 9 step 7100 loss 0.04262
INFO:name:epoch 9 step 7200 loss 0.04183
INFO:name:epoch 9 step 7300 loss 0.04637
INFO:name:epoch 9 step 7400 loss 0.04068
INFO:name:epoch 9 step 7500 loss 0.04099
INFO:name:epoch 9 step 7600 loss 0.04894
INFO:name:epoch 9 step 7700 loss 0.04398
INFO:name:epoch 9 step 7800 loss 0.04562
INFO:name:***** Running evaluation *****
INFO:name:  Num queries = 9604
INFO:name:  Num codes = 9604
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.3752
INFO:name:  ********************
INFO:name:  Best eval mrr:0.3752
INFO:name:  ********************
INFO:name:***** Running Test *****
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
INFO:name:  eval_mrr = 0.313
INFO:name:Saving model checkpoint to ./models/best_model_codeSearch/model.bin
INFO:name:Saving model checkpoint to ./models/final_model_codeSearch/model.bin
INFO:name:  Num queries = 19210
INFO:name:  Num codes = 19210
INFO:name:  Batch size = 32
train results ([0.2382019702598377, 0.0814722989205084, 0.0701117301779356, 0.06419881596138258, 0.05896034432139326, 0.055981188308510614, 0.052656394681018175, 0.05009973562442966, 0.04791360699536611, 0.046201134794669786], [0.34012747346887284, 0.36772930801235965, 0.3713874710398657, 0.3665909447862103, 0.3679844411050984, 0.37440205946206456, 0.3735840434561323, 0.37270945375420317, 0.37491052914858497, 0.37520855939392195])
